#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å«æ˜Ÿæ¨ç†æœåŠ¡å™¨
ä¸“é—¨ä¸ºé‡æ„åçš„å«æ˜Ÿç³»ç»Ÿè®¾è®¡çš„æ¨ç†æœåŠ¡å™¨
"""

import torch
import numpy as np
import time
import json
import socket
import threading
import logging
import pickle
import struct
from typing import Dict, Any, Optional
from dataclasses import dataclass
import argparse
import sys
import os

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))

# æ£€æŸ¥å¿…è¦çš„ä¾èµ–
try:
    import segmentation_models_pytorch as smp
except ImportError:
    print("âŒ ç¼ºå°‘ä¾èµ–: segmentation_models_pytorch")
    print("è¯·è¿è¡Œ: pip install segmentation-models-pytorch")
    sys.exit(1)

try:
    from train_model import EnhancedDeepLab
except ImportError as e:
    print(f"âŒ å¯¼å…¥EnhancedDeepLabå¤±è´¥: {e}")
    print("è¯·ç¡®ä¿train_model.pyæ–‡ä»¶å­˜åœ¨ä¸”åŒ…å«EnhancedDeepLabç±»")
    sys.exit(1)

@dataclass
class InferenceRequest:
    """æ¨ç†è¯·æ±‚æ•°æ®ç±»"""
    task_id: str
    image_data: np.ndarray
    sim_features: Optional[np.ndarray]
    priority: int
    timestamp: float

@dataclass
class InferenceResponse:
    """æ¨ç†å“åº”æ•°æ®ç±»"""
    task_id: str
    prediction: np.ndarray
    processing_time: float
    status: str
    error_message: Optional[str] = None

class SatelliteInferenceServer:
    """å«æ˜Ÿç«¯æ¨ç†æœåŠ¡å™¨"""
    
    def __init__(self, host: str = "0.0.0.0", port: int = 8080, 
                 model_path: str = "models/best_multimodal_patch_model.pth",
                 satellite_id: str = "sat_001"):
        self.host = host
        self.port = port
        self.model_path = model_path
        self.satellite_id = satellite_id
        
        # åˆå§‹åŒ–æ¨¡å‹
        self.model = self._load_model()
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        
        # æœåŠ¡å™¨çŠ¶æ€
        self.running = False
        self.current_load = 0.0
        self.processed_tasks = 0
        self.failed_tasks = 0
        
        # é…ç½®æ—¥å¿—
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
        )
        self.logger = logging.getLogger(__name__)
        
        # å¯åŠ¨ç›‘æ§çº¿ç¨‹
        self._start_monitoring()
    
    def _load_model(self) -> Optional[EnhancedDeepLab]:
        """åŠ è½½æ¨ç†æ¨¡å‹"""
        try:
            # æ£€æŸ¥æ¨¡å‹æ–‡ä»¶æ˜¯å¦å­˜åœ¨
            if not os.path.exists(self.model_path):
                self.logger.error(f"æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {self.model_path}")
                self.logger.info("è¯·ç¡®ä¿æ¨¡å‹æ–‡ä»¶å­˜åœ¨ï¼Œæˆ–ä½¿ç”¨ --model å‚æ•°æŒ‡å®šæ­£ç¡®çš„æ¨¡å‹è·¯å¾„")
                return None
            
            model = EnhancedDeepLab(in_channels=3, num_classes=1, sim_feat_dim=11)
            
            # åŠ è½½æ¨¡å‹æƒé‡
            checkpoint = torch.load(self.model_path, map_location='cpu')
            
            # æ£€æŸ¥checkpointæ ¼å¼
            if 'model_state_dict' in checkpoint:
                model.load_state_dict(checkpoint['model_state_dict'])
            else:
                # å¦‚æœcheckpointç›´æ¥æ˜¯state_dict
                model.load_state_dict(checkpoint)
            
            model.eval()
            
            # ç§»åŠ¨åˆ°è®¾å¤‡
            device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
            model = model.to(device)
            
            self.logger.info(f"æ¨¡å‹åŠ è½½æˆåŠŸ: {self.model_path}")
            self.logger.info(f"ä½¿ç”¨è®¾å¤‡: {device}")
            
            return model
            
        except Exception as e:
            self.logger.error(f"æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            self.logger.info("è¯·æ£€æŸ¥æ¨¡å‹æ–‡ä»¶æ ¼å¼æ˜¯å¦æ­£ç¡®")
            return None
    
    def _start_monitoring(self):
        """å¯åŠ¨ç›‘æ§çº¿ç¨‹"""
        def monitor_loop():
            while self.running:
                try:
                    # æ›´æ–°è´Ÿè½½ä¿¡æ¯
                    self._update_load_info()
                    time.sleep(30)  # æ¯30ç§’æ›´æ–°ä¸€æ¬¡
                except Exception as e:
                    self.logger.error(f"ç›‘æ§çº¿ç¨‹é”™è¯¯: {e}")
        
        monitor_thread = threading.Thread(target=monitor_loop, daemon=True)
        monitor_thread.start()
        self.logger.info("ç›‘æ§çº¿ç¨‹å·²å¯åŠ¨")
    
    def _update_load_info(self):
        """æ›´æ–°è´Ÿè½½ä¿¡æ¯"""
        try:
            import psutil
            self.current_load = psutil.cpu_percent() / 100.0
        except ImportError:
            # å¦‚æœæ²¡æœ‰psutilï¼Œä½¿ç”¨ç®€å•çš„è´Ÿè½½ä¼°ç®—
            self.current_load = min(1.0, self.processed_tasks / 1000.0)
    
    def start_server(self):
        """å¯åŠ¨æœåŠ¡å™¨"""
        if self.model is None:
            self.logger.error("æ¨¡å‹æœªåŠ è½½ï¼Œæ— æ³•å¯åŠ¨æœåŠ¡å™¨")
            self.logger.info("è¯·æ£€æŸ¥æ¨¡å‹æ–‡ä»¶è·¯å¾„æ˜¯å¦æ­£ç¡®")
            return
        
        self.running = True
        self.logger.info(f"å¯åŠ¨æ¨ç†æœåŠ¡å™¨: {self.host}:{self.port}")
        self.logger.info(f"å«æ˜ŸID: {self.satellite_id}")
        
        try:
            with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as server_socket:
                server_socket.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)
                server_socket.bind((self.host, self.port))
                server_socket.listen(5)
                
                self.logger.info("æœåŠ¡å™¨å¯åŠ¨æˆåŠŸï¼Œç­‰å¾…è¿æ¥...")
                
                while self.running:
                    try:
                        client_socket, address = server_socket.accept()
                        self.logger.info(f"æ¥å—è¿æ¥: {address}")
                        
                        # ä¸ºæ¯ä¸ªå®¢æˆ·ç«¯åˆ›å»ºå¤„ç†çº¿ç¨‹
                        client_thread = threading.Thread(
                            target=self._handle_client,
                            args=(client_socket, address)
                        )
                        client_thread.daemon = True
                        client_thread.start()
                        
                    except Exception as e:
                        self.logger.error(f"æ¥å—è¿æ¥é”™è¯¯: {e}")
                        
        except Exception as e:
            self.logger.error(f"æœåŠ¡å™¨å¯åŠ¨å¤±è´¥: {e}")
        finally:
            self.running = False
    
    def _handle_client(self, client_socket: socket.socket, address: tuple):
        """å¤„ç†å®¢æˆ·ç«¯è¿æ¥"""
        try:
            while self.running:
                # æ¥æ”¶æ•°æ®é•¿åº¦
                length_data = client_socket.recv(4)
                if not length_data:
                    break
                
                data_length = struct.unpack('!I', length_data)[0]
                
                # æ¥æ”¶æ•°æ®
                data = b''
                while len(data) < data_length:
                    chunk = client_socket.recv(data_length - len(data))
                    if not chunk:
                        break
                    data += chunk
                
                if len(data) != data_length:
                    self.logger.warning(f"æ•°æ®æ¥æ”¶ä¸å®Œæ•´: {len(data)}/{data_length}")
                    continue
                
                # è§£æè¯·æ±‚
                try:
                    request_data = pickle.loads(data)
                    
                    # å¤„ç†ä¸åŒç±»å‹çš„è¯·æ±‚
                    if request_data.get('type') == 'heartbeat':
                        response = self._handle_heartbeat(request_data)
                    elif request_data.get('type') == 'get_load':
                        response = self._handle_get_load(request_data)
                    elif request_data.get('type') == 'inference':
                        request = InferenceRequest(**request_data)
                        response = self._process_inference(request)
                    else:
                        response = InferenceResponse(
                            task_id="unknown",
                            prediction=np.array([]),
                            processing_time=0.0,
                            status="error",
                            error_message="Unknown request type"
                        )
                    
                    # å‘é€å“åº”
                    response_data = pickle.dumps(response.__dict__)
                    response_length = struct.pack('!I', len(response_data))
                    client_socket.send(response_length + response_data)
                    
                except Exception as e:
                    self.logger.error(f"å¤„ç†è¯·æ±‚é”™è¯¯: {e}")
                    # å‘é€é”™è¯¯å“åº”
                    error_response = InferenceResponse(
                        task_id="unknown",
                        prediction=np.array([]),
                        processing_time=0.0,
                        status="error",
                        error_message=str(e)
                    )
                    response_data = pickle.dumps(error_response.__dict__)
                    response_length = struct.pack('!I', len(response_data))
                    client_socket.send(response_length + response_data)
                    
        except Exception as e:
            self.logger.error(f"å®¢æˆ·ç«¯å¤„ç†é”™è¯¯ {address}: {e}")
        finally:
            client_socket.close()
            self.logger.info(f"å®¢æˆ·ç«¯è¿æ¥å…³é—­: {address}")
    
    def _handle_heartbeat(self, request_data: dict) -> dict:
        """å¤„ç†å¿ƒè·³è¯·æ±‚"""
        return {
            'type': 'heartbeat_response',
            'status': 'ok',
            'satellite_id': self.satellite_id,
            'timestamp': time.time(),
            'load': self.current_load
        }
    
    def _handle_get_load(self, request_data: dict) -> dict:
        """å¤„ç†è·å–è´Ÿè½½ä¿¡æ¯è¯·æ±‚"""
        return {
            'type': 'load_response',
            'status': 'ok',
            'satellite_id': self.satellite_id,
            'load': self.current_load,
            'processed_tasks': self.processed_tasks,
            'failed_tasks': self.failed_tasks,
            'training_data_count': 0,  # å¯ä»¥æ‰©å±•
            'last_training_time': 0.0,
            'model_hash': "model_hash_placeholder",
            'parameter_version': 1
        }
    
    def _process_inference(self, request: InferenceRequest) -> InferenceResponse:
        """å¤„ç†æ¨ç†è¯·æ±‚"""
        start_time = time.time()
        
        try:
            # é¢„å¤„ç†å›¾åƒæ•°æ®
            img_tensor = torch.tensor(request.image_data, dtype=torch.float32)
            if img_tensor.dim() == 3:
                img_tensor = img_tensor.unsqueeze(0)
            
            # ç§»åŠ¨åˆ°è®¾å¤‡
            img_tensor = img_tensor.to(self.device)
            
            # å‡†å¤‡ä»¿çœŸç‰¹å¾
            if request.sim_features is not None:
                sim_feats = torch.tensor(request.sim_features, dtype=torch.float32)
                if sim_feats.dim() == 1:
                    sim_feats = sim_feats.unsqueeze(0)
                sim_feats = sim_feats.to(self.device)
            else:
                sim_feats = torch.zeros(1, 11, dtype=torch.float32, device=self.device)
            
            # æ‰§è¡Œæ¨ç†
            with torch.no_grad():
                pred = self.model(img_tensor, sim_feats)
                pred_mask = torch.sigmoid(pred).cpu().numpy()[0, 0]
            
            processing_time = time.time() - start_time
            
            # æ›´æ–°ç»Ÿè®¡ä¿¡æ¯
            self.processed_tasks += 1
            
            self.logger.info(f"æ¨ç†å®Œæˆ: {request.task_id}, è€—æ—¶: {processing_time:.3f}s")
            
            return InferenceResponse(
                task_id=request.task_id,
                prediction=pred_mask,
                processing_time=processing_time,
                status="success"
            )
            
        except Exception as e:
            processing_time = time.time() - start_time
            self.failed_tasks += 1
            
            self.logger.error(f"æ¨ç†å¤±è´¥: {request.task_id}, é”™è¯¯: {e}")
            
            return InferenceResponse(
                task_id=request.task_id,
                prediction=np.array([]),
                processing_time=processing_time,
                status="error",
                error_message=str(e)
            )
    
    def get_status(self) -> Dict[str, Any]:
        """è·å–æœåŠ¡å™¨çŠ¶æ€"""
        return {
            "satellite_id": self.satellite_id,
            "running": self.running,
            "current_load": self.current_load,
            "processed_tasks": self.processed_tasks,
            "failed_tasks": self.failed_tasks,
            "success_rate": (self.processed_tasks - self.failed_tasks) / max(self.processed_tasks, 1),
            "device": str(self.device),
            "model_loaded": self.model is not None
        }
    
    def stop_server(self):
        """åœæ­¢æœåŠ¡å™¨"""
        self.running = False
        self.logger.info("æœåŠ¡å™¨åœæ­¢")


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description='å«æ˜Ÿæ¨ç†æœåŠ¡å™¨')
    parser.add_argument('--host', type=str, default='0.0.0.0', help='æœåŠ¡å™¨åœ°å€')
    parser.add_argument('--port', type=int, default=8080, help='æœåŠ¡å™¨ç«¯å£')
    parser.add_argument('--model', type=str, default='models/best_multimodal_patch_model.pth', 
                       help='æ¨¡å‹è·¯å¾„')
    parser.add_argument('--satellite_id', type=str, default='sat_001', help='å«æ˜ŸID')
    
    args = parser.parse_args()
    
    # åˆ›å»ºæœåŠ¡å™¨
    server = SatelliteInferenceServer(
        host=args.host,
        port=args.port,
        model_path=args.model,
        satellite_id=args.satellite_id
    )
    
    print(f"ğŸš€ å¯åŠ¨å«æ˜Ÿæ¨ç†æœåŠ¡å™¨")
    print(f"   å«æ˜ŸID: {args.satellite_id}")
    print(f"   åœ°å€: {args.host}:{args.port}")
    print(f"   æ¨¡å‹: {args.model}")
    print(f"   è®¾å¤‡: {server.device}")
    print(f"   æ¨¡å‹çŠ¶æ€: {'å·²åŠ è½½' if server.model else 'æœªåŠ è½½'}")
    
    try:
        # å¯åŠ¨æœåŠ¡å™¨
        server.start_server()
    except KeyboardInterrupt:
        print("\nğŸ›‘ æ”¶åˆ°åœæ­¢ä¿¡å·")
    except Exception as e:
        print(f"âŒ æœåŠ¡å™¨é”™è¯¯: {e}")
    finally:
        server.stop_server()
        print("âœ… æœåŠ¡å™¨å·²åœæ­¢")


if __name__ == "__main__":
    main() 