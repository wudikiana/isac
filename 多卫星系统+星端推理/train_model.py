import os
import sys
import warnings
warnings.filterwarnings('ignore')
os.environ['ALBUMENTATIONS_DISABLE_VERSION_CHECK'] = '1'

# æ·»åŠ å½“å‰ç›®å½•åˆ°Pythonè·¯å¾„ï¼Œç¡®ä¿å¯ä»¥å¯¼å…¥data_utilsæ¨¡å—
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

import random
import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
from tqdm import tqdm
import numpy as np
import matplotlib.pyplot as plt
from torch.utils.data import DataLoader
import segmentation_models_pytorch as smp
from torch.amp import autocast, GradScaler
from torch.utils.data import Dataset  # ä¿®å¤åŸºç±»ç¼ºå¤±é—®é¢˜
from PIL import Image  # åç»­ä»£ç éœ€è¦ PIL åº“
import csv 
import glob  # ç”¨äºæ–‡ä»¶è·¯å¾„åŒ¹é…
import albumentations as A  # ç”¨äºå›¾åƒå¢å¼º
from albumentations.pytorch import ToTensorV2  # ç”¨äºè½¬æ¢åˆ°å¼ é‡
import torchvision.transforms as T  # ç”¨äºå›¾åƒå˜æ¢
import threading
import queue
import time
from concurrent.futures import ThreadPoolExecutor, as_completed

# å¯¼å…¥åå¤„ç†ç›¸å…³åº“
from skimage import measure
from skimage.filters import threshold_otsu
from skimage.morphology import binary_opening, binary_closing, disk, ball
from scipy.ndimage import distance_transform_edt

# å¯¼å…¥æˆ‘ä»¬çš„LandslideDetectoræ¨¡å‹
from models.starlite_cnn import create_starlite_model, create_enhanced_model, create_segmentation_landslide_model
# å¯ç”¨TF32è®¡ç®—
torch.backends.cuda.matmul.allow_tf32 = True
torch.backends.cudnn.allow_tf32 = True
# ====== åå¤„ç†å‡½æ•° ======
def postprocess(output, min_area=100, merge_distance=10, debug_mode=False):
    """
    å¢å¼ºçš„åå¤„ç†æµç¨‹ï¼šé˜ˆå€¼åŒ– -> è¿é€šåŸŸåˆ†æ -> å°åŒºåŸŸè¿‡æ»¤ -> è¾¹ç•Œä¼˜åŒ–
    å¢åŠ äº†åŠ¨æ€èŒƒå›´æ£€æŸ¥å’Œå¼‚å¸¸å¤„ç†
    
    Args:
        output: æ¨¡å‹è¾“å‡º
        min_area: æœ€å°åŒºåŸŸé¢ç§¯
        merge_distance: åˆå¹¶è·ç¦»
        debug_mode: æ˜¯å¦å¯ç”¨è°ƒè¯•æ¨¡å¼ï¼ˆæ‰“å°è­¦å‘Šä¿¡æ¯ï¼‰
    """
    # è®¾ç½®è°ƒè¯•æ¨¡å¼
    postprocess._debug_mode = debug_mode
    # ç¡®ä¿è¾“å…¥æ˜¯tensorå¹¶è½¬æ¢ä¸ºnumpy
    if torch.is_tensor(output):
        prob = torch.sigmoid(output).cpu().numpy()
    else:
        prob = output
    
    # ç¡®ä¿probæ˜¯2Dæ•°ç»„
    if prob.ndim > 2:
        # å¦‚æœæ˜¯4D [B, C, H, W]ï¼Œå–ç¬¬ä¸€ä¸ªæ ·æœ¬çš„ç¬¬ä¸€ä¸ªé€šé“
        if prob.ndim == 4:
            prob = prob[0, 0] if prob.shape[1] == 1 else prob[0, :, :, 0]
        # å¦‚æœæ˜¯3D [C, H, W]ï¼Œå–ç¬¬ä¸€ä¸ªé€šé“
        elif prob.ndim == 3:
            prob = prob[0] if prob.shape[0] == 1 else prob[0, :, :]
    
    # æ·»åŠ åŠ¨æ€èŒƒå›´æ£€æŸ¥ - ä½¿ç”¨è‡ªé€‚åº”é˜ˆå€¼
    prob_range = prob.max() - prob.min()
    if prob_range < 0.01:  # å€¼åŸŸè¿‡å°
        # åªåœ¨è°ƒè¯•æ¨¡å¼ä¸‹æ‰“å°è­¦å‘Šï¼Œé¿å…è®­ç»ƒæ—¶è¿‡å¤šè¾“å‡º
        if hasattr(postprocess, '_debug_mode') and postprocess._debug_mode:
            print(f"è­¦å‘Šï¼šè¾“å‡ºæ¦‚ç‡èŒƒå›´è¿‡å°({prob_range:.4f})ï¼Œä½¿ç”¨è‡ªé€‚åº”é˜ˆå€¼")
        
        # æ”¹è¿›çš„è‡ªé€‚åº”é˜ˆå€¼é€»è¾‘
        prob_mean = prob.mean()
        if prob_mean > 0.7:  # é«˜æ¦‚ç‡åŒºåŸŸ
            binary = np.ones_like(prob, dtype=np.uint8)
        elif prob_mean < 0.3:  # ä½æ¦‚ç‡åŒºåŸŸ
            binary = np.zeros_like(prob, dtype=np.uint8)
        else:  # ä¸­ç­‰æ¦‚ç‡åŒºåŸŸï¼Œä½¿ç”¨0.5ä½œä¸ºé˜ˆå€¼
            binary = (prob > 0.5).astype(np.uint8)
    else:
        try:
            thresh = threshold_otsu(prob)
            binary = (prob > thresh).astype(np.uint8)
        except:
            if hasattr(postprocess, '_debug_mode') and postprocess._debug_mode:
                print("Otsué˜ˆå€¼å¤±è´¥ï¼Œä½¿ç”¨ä¸­å€¼é˜ˆå€¼")
            binary = (prob > np.median(prob)).astype(np.uint8)
    
    # æ·»åŠ é¢ç§¯è¿‡æ»¤ - æ”¹è¿›çš„ä¿å®ˆè¿‡æ»¤
    if binary.mean() > 0.95:  # è¶…è¿‡95%åŒºåŸŸè¢«é¢„æµ‹ä¸ºæŸå
        if hasattr(postprocess, '_debug_mode') and postprocess._debug_mode:
            print("è­¦å‘Šï¼šé¢„æµ‹åŒºåŸŸå æ¯”è¿‡é«˜ï¼Œè¿›è¡Œä¿å®ˆè¿‡æ»¤")
        # ä½¿ç”¨æ›´é«˜é˜ˆå€¼ä½†ä¿ç•™é«˜ç½®ä¿¡åº¦åŒºåŸŸ
        high_conf_thresh = max(0.8, np.percentile(prob, 90))
        binary = (prob > high_conf_thresh).astype(np.uint8)
    
    # è¿é€šåŸŸåˆ†æ
    try:
        labels = measure.label(binary)
        
        # ç§»é™¤å°åŒºåŸŸ
        properties = measure.regionprops(labels)
        for prop in properties:
            if prop.area < min_area:
                labels[labels == prop.label] = 0
        
        # åˆå¹¶é‚»è¿‘åŒºåŸŸ - ä¿®å¤é€»è¾‘
        if labels.max() > 0:  # åªæœ‰å½“å­˜åœ¨æœ‰æ•ˆåŒºåŸŸæ—¶æ‰è¿›è¡Œåˆå¹¶
            distance = distance_transform_edt(labels == 0)
            close_mask = distance < merge_distance
            labels[close_mask] = 1
        
        # è¾¹ç•Œä¼˜åŒ–
        refined = np.zeros_like(binary)
        for i in range(1, labels.max() + 1):
            region = (labels == i)
            # å½¢æ€å­¦ä¼˜åŒ– - ç¡®ä¿ç»“æ„å…ƒç´ ç»´åº¦åŒ¹é…
            try:
                # ç¡®ä¿regionæ˜¯2Dæ•°ç»„
                if region.ndim > 2:
                    region = region.squeeze()
                
                # åˆ›å»ºä¸è¾“å…¥ç»´åº¦åŒ¹é…çš„ç»“æ„å…ƒç´ 
                if region.ndim == 2:
                    # 2Då›¾åƒï¼Œä½¿ç”¨2Dç»“æ„å…ƒç´ 
                    selem_open = disk(1)
                    selem_close = disk(2)
                    
                    # æ£€æŸ¥ç»“æ„å…ƒç´ å¤§å°æ˜¯å¦åˆé€‚
                    if selem_open.shape[0] > region.shape[0] or selem_open.shape[1] > region.shape[1]:
                        print("ç»“æ„å…ƒç´ è¿‡å¤§ï¼Œè·³è¿‡å½¢æ€å­¦ä¼˜åŒ–")
                        refined[region] = 1
                        continue
                    
                    region = binary_opening(region, footprint=selem_open)
                    region = binary_closing(region, footprint=selem_close)
                else:
                    print(f"ä¸æ”¯æŒçš„ç»´åº¦: {region.ndim}ï¼Œè·³è¿‡å½¢æ€å­¦ä¼˜åŒ–")
                    refined[region] = 1
                    continue
                    
            except Exception as morph_error:
                print(f"å½¢æ€å­¦æ“ä½œå¤±è´¥: {morph_error}ï¼Œè·³è¿‡å½¢æ€å­¦ä¼˜åŒ–")
                # å¦‚æœå½¢æ€å­¦æ“ä½œå¤±è´¥ï¼Œç›´æ¥ä½¿ç”¨åŸå§‹åŒºåŸŸ
                pass
            refined[region] = 1
        
        # æ·»åŠ æ›´ç²¾ç»†çš„è­¦å‘Šçº§åˆ«
        if hasattr(postprocess, '_debug_mode') and postprocess._debug_mode:
            refined_sum = refined.sum()
            refined_size = refined.size
            
            if refined_sum == 0:
                # æ£€æŸ¥åŸå§‹æ¦‚ç‡æ˜¯å¦å…¨ä½
                if prob.max() < 0.3:  # ä½æ¦‚ç‡åŒºåŸŸ
                    print("ä¿¡æ¯ï¼šåå¤„ç†è¾“å‡ºå…¨é›¶ï¼ˆä½æ¦‚ç‡åŒºåŸŸï¼‰")
                elif prob.mean() > 0.7:  # é«˜æ¦‚ç‡åŒºåŸŸè¢«ä¿å®ˆè¿‡æ»¤
                    print("ä¿¡æ¯ï¼šåå¤„ç†è¾“å‡ºå…¨é›¶ï¼ˆé«˜æ¦‚ç‡åŒºåŸŸè¢«ä¿å®ˆè¿‡æ»¤ï¼‰")
                else:  # ä¸­ç­‰æ¦‚ç‡åŒºåŸŸ
                    print("ä¿¡æ¯ï¼šåå¤„ç†è¾“å‡ºå…¨é›¶ï¼ˆä¸­ç­‰æ¦‚ç‡åŒºåŸŸï¼‰")
            elif refined_sum == refined_size:
                # æ£€æŸ¥åŸå§‹æ¦‚ç‡æ˜¯å¦å…¨é«˜
                if prob.min() > 0.7:  # é«˜æ¦‚ç‡åŒºåŸŸ
                    print("ä¿¡æ¯ï¼šåå¤„ç†è¾“å‡ºå…¨ä¸€ï¼ˆé«˜æ¦‚ç‡åŒºåŸŸï¼‰")
                else:  # ä½æ¦‚ç‡ä½†è¢«ä¿ç•™
                    print("è­¦å‘Šï¼šåå¤„ç†è¾“å‡ºå…¨ä¸€ï¼ˆä½æ¦‚ç‡åŒºåŸŸè¢«ä¿ç•™ï¼‰")
            elif refined_sum > 0 and refined_sum < refined_size:
                # æ­£å¸¸è¾“å‡ºæƒ…å†µ - æ£€æŸ¥æ¦‚ç‡åˆ†å¸ƒ
                prob_mean = prob.mean()
                prob_std = prob.std()
                if prob_std > 0.1:  # é«˜æ–¹å·®
                    print(f"ä¿¡æ¯ï¼šåå¤„ç†è¾“å‡ºæ­£å¸¸ (å‡å€¼={prob_mean:.2f}, æ ‡å‡†å·®={prob_std:.2f})")
                else:  # ä½æ–¹å·®
                    print(f"ä¿¡æ¯ï¼šåå¤„ç†è¾“å‡ºæ­£å¸¸ (ä½æ–¹å·®, å‡å€¼={prob_mean:.2f})")
            else:
                # å¼‚å¸¸æƒ…å†µ
                print(f"è­¦å‘Šï¼šåå¤„ç†è¾“å‡ºå¼‚å¸¸ (sum={refined_sum}, size={refined_size})")
        
        return torch.from_numpy(refined).float()
        
    except Exception as e:
        print(f"åå¤„ç†å¼‚å¸¸ï¼Œè¿”å›åŸå§‹äºŒå€¼åŒ–ç»“æœ: {e}")
        return torch.from_numpy(binary).float()

def simple_postprocess(output, threshold=0.5, adaptive=True):
    """
    ç®€åŒ–çš„åå¤„ç†å‡½æ•°ï¼Œç”¨äºè®­ç»ƒæ—¶çš„å¿«é€Ÿè®¡ç®—
    é¿å…å¤æ‚çš„å½¢æ€å­¦æ“ä½œï¼Œæé«˜è®­ç»ƒé€Ÿåº¦
    
    Args:
        output: æ¨¡å‹è¾“å‡º
        threshold: å›ºå®šé˜ˆå€¼ï¼ˆå½“adaptive=Falseæ—¶ä½¿ç”¨ï¼‰
        adaptive: æ˜¯å¦ä½¿ç”¨è‡ªé€‚åº”é˜ˆå€¼
    """
    if torch.is_tensor(output):
        prob = torch.sigmoid(output).cpu().numpy()
    else:
        prob = output
    
    # ç¡®ä¿probæ˜¯2Dæ•°ç»„
    if prob.ndim > 2:
        if prob.ndim == 4:
            prob = prob[0, 0] if prob.shape[1] == 1 else prob[0, :, :, 0]
        elif prob.ndim == 3:
            prob = prob[0] if prob.shape[0] == 1 else prob[0, :, :]
    
    # æ™ºèƒ½é˜ˆå€¼åŒ–
    if adaptive:
        # æ£€æŸ¥æ¦‚ç‡åˆ†å¸ƒ
        prob_mean = prob.mean()
        prob_std = prob.std()
        
        # ä½æ–¹å·®æƒ…å†µ - æ ¹æ®å‡å€¼å’Œæ ‡å‡†å·®åŠ¨æ€è°ƒæ•´é˜ˆå€¼
        if prob_std < 0.05:  # æ›´ä¸¥æ ¼çš„æ ‡å‡†å·®åˆ¤æ–­
            if prob_mean > 0.7:  # é«˜å‡å€¼åŒºåŸŸ
                binary = np.ones_like(prob, dtype=np.uint8)
            elif prob_mean < 0.3:  # ä½å‡å€¼åŒºåŸŸ
                binary = np.zeros_like(prob, dtype=np.uint8)
            elif prob_mean > 0.55:  # ä¸­ç­‰é«˜å€¼åŒºåŸŸï¼ˆå¦‚0.59-0.65ï¼‰
                # ä¿ç•™é«˜æ¦‚ç‡åŒºåŸŸï¼Œä½¿ç”¨0.5ä½œä¸ºé˜ˆå€¼
                binary = (prob > 0.5).astype(np.uint8)
            elif prob_mean < 0.45:  # ä¸­ç­‰ä½å€¼åŒºåŸŸ
                # è¿‡æ»¤ä½æ¦‚ç‡åŒºåŸŸï¼Œä½¿ç”¨0.5ä½œä¸ºé˜ˆå€¼
                binary = (prob > 0.5).astype(np.uint8)
            else:  # æ¥è¿‘0.5çš„ä¸­é—´å€¼ä½¿ç”¨å‡å€¼ä½œä¸ºé˜ˆå€¼
                binary = (prob > prob_mean).astype(np.uint8)
        else:
            # æ­£å¸¸æƒ…å†µä½¿ç”¨å›ºå®šé˜ˆå€¼
            binary = (prob > threshold).astype(np.uint8)
    else:
        # ä½¿ç”¨å›ºå®šé˜ˆå€¼
        binary = (prob > threshold).astype(np.uint8)
    
    # æ£€æŸ¥æ˜¯å¦éœ€è¦ä¿å®ˆè¿‡æ»¤ - æ”¹è¿›é€»è¾‘
    if binary.mean() > 0.95:  # è¶…è¿‡95%åŒºåŸŸè¢«é¢„æµ‹ä¸ºæŸå
        # å¯¹äºä½æ–¹å·®æ•°æ®ï¼Œä½¿ç”¨æ›´æ¸©å’Œçš„è¿‡æ»¤ç­–ç•¥
        if prob.std() < 0.05:
            # ä½æ–¹å·®æ•°æ®ï¼šä½¿ç”¨75%åˆ†ä½æ•°ä½œä¸ºé˜ˆå€¼
            high_conf_thresh = np.percentile(prob, 75)
        else:
            # é«˜æ–¹å·®æ•°æ®ï¼šä½¿ç”¨90%åˆ†ä½æ•°ä½œä¸ºé˜ˆå€¼
            high_conf_thresh = max(0.8, np.percentile(prob, 90))
        binary = (prob > high_conf_thresh).astype(np.uint8)
    
    # æ·»åŠ è­¦å‘Šç³»ç»Ÿ
    if hasattr(simple_postprocess, '_debug_mode') and simple_postprocess._debug_mode:
        if binary.sum() == 0:
            # æ£€æŸ¥åŸå§‹æ¦‚ç‡æ˜¯å¦å…¨ä½
            if prob.max() < 0.3:  # ä½æ¦‚ç‡åŒºåŸŸ
                print("ä¿¡æ¯ï¼šåå¤„ç†è¾“å‡ºå…¨é›¶ï¼ˆä½æ¦‚ç‡åŒºåŸŸï¼‰")
            else:  # é«˜æ¦‚ç‡ä½†è¢«è¿‡æ»¤
                print("è­¦å‘Šï¼šåå¤„ç†è¾“å‡ºå…¨é›¶ï¼ˆé«˜æ¦‚ç‡åŒºåŸŸè¢«è¿‡æ»¤ï¼‰")
        elif binary.sum() == binary.size:
            # æ£€æŸ¥åŸå§‹æ¦‚ç‡æ˜¯å¦å…¨é«˜
            if prob.min() > 0.7:  # é«˜æ¦‚ç‡åŒºåŸŸ
                print("ä¿¡æ¯ï¼šåå¤„ç†è¾“å‡ºå…¨ä¸€ï¼ˆé«˜æ¦‚ç‡åŒºåŸŸï¼‰")
            else:  # ä½æ¦‚ç‡ä½†è¢«ä¿ç•™
                print("è­¦å‘Šï¼šåå¤„ç†è¾“å‡ºå…¨ä¸€ï¼ˆä½æ¦‚ç‡åŒºåŸŸè¢«ä¿ç•™ï¼‰")
        elif binary.sum() > 0 and binary.sum() < binary.size:
            # æ­£å¸¸è¾“å‡ºæƒ…å†µ - æ£€æŸ¥æ¦‚ç‡åˆ†å¸ƒ
            prob_mean = prob.mean()
            prob_std = prob.std()
            if prob_std > 0.1:  # é«˜æ–¹å·®
                print(f"ä¿¡æ¯ï¼šåå¤„ç†è¾“å‡ºæ­£å¸¸ (å‡å€¼={prob_mean:.2f}, æ ‡å‡†å·®={prob_std:.2f})")
            else:  # ä½æ–¹å·®
                print(f"ä¿¡æ¯ï¼šåå¤„ç†è¾“å‡ºæ­£å¸¸ (ä½æ–¹å·®, å‡å€¼={prob_mean:.2f})")
    
    return torch.from_numpy(binary).float()

try:
    from data_utils.data_loader import optimized_collate
except ImportError:
    # å¦‚æœå¯¼å…¥å¤±è´¥ï¼Œå®šä¹‰ä¸€ä¸ªç®€å•çš„collateå‡½æ•°ä½œä¸ºæ›¿ä»£
    def optimized_collate(batch):
        """ç®€å•çš„collateå‡½æ•°ï¼Œç¡®ä¿tensorç»´åº¦ä¸€è‡´"""
        images, masks, feats = zip(*batch)
        
        # ç¡®ä¿æ‰€æœ‰å›¾åƒtensorç»´åº¦ä¸€è‡´ [B, C, H, W]
        images = list(images)
        for i, img in enumerate(images):
            if img.dim() == 2:
                images[i] = img.unsqueeze(0)  # [H, W] -> [1, H, W]
            elif img.dim() == 3 and img.shape[0] not in [1, 3]:
                images[i] = img.permute(2, 0, 1)  # [H, W, C] -> [C, H, W]
        
        # ç¡®ä¿æ‰€æœ‰mask tensorç»´åº¦ä¸€è‡´ [B, 1, H, W]
        masks = list(masks)
        for i, mask in enumerate(masks):
            if mask.dim() == 2:
                masks[i] = mask.unsqueeze(0)  # [H, W] -> [1, H, W]
            elif mask.dim() == 1:
                masks[i] = mask.unsqueeze(0).unsqueeze(0)  # [H] -> [1, 1, H]
        
        # ç¡®ä¿æ‰€æœ‰sim_feats tensorç»´åº¦ä¸€è‡´ [B, 11]
        feats = list(feats)
        for i, sim_feat in enumerate(feats):
            if sim_feat.dim() == 0:
                feats[i] = sim_feat.unsqueeze(0)  # [] -> [1]
        
        # ä½¿ç”¨torch.stackè¿›è¡Œæ‰¹å¤„ç†
        try:
            images = torch.stack(images, dim=0)
            masks = torch.stack(masks, dim=0)
            feats = torch.stack(feats, dim=0)
        except Exception as e:
            print(f"Collateé”™è¯¯: {e}")
            print(f"å›¾åƒå½¢çŠ¶: {[img.shape for img in images]}")
            print(f"æ©ç å½¢çŠ¶: {[mask.shape for mask in masks]}")
            print(f"ç‰¹å¾å½¢çŠ¶: {[sim_feat.shape for sim_feat in feats]}")
            raise e
        
        return images, masks, feats

# ====== CPU-GPUååŒä¼˜åŒ–ç±» ======
class CPUAssistedTraining:
    """CPUè¾…åŠ©GPUè®­ç»ƒä¼˜åŒ–å™¨"""
    def __init__(self, model, device='cuda', num_cpu_workers=8):
        self.model = model
        self.device = device
        self.num_cpu_workers = num_cpu_workers
        self.cpu_executor = ThreadPoolExecutor(max_workers=num_cpu_workers)
        self.task_queue = queue.Queue()
        self.result_queue = queue.Queue()
        
        # å¯åŠ¨CPUå·¥ä½œçº¿ç¨‹
        self.cpu_workers = []
        for i in range(num_cpu_workers):
            worker = threading.Thread(target=self._cpu_worker, args=(i,))
            worker.daemon = True
            worker.start()
            self.cpu_workers.append(worker)
    
    def _cpu_worker(self, worker_id):
        """CPUå·¥ä½œçº¿ç¨‹"""
        while True:
            try:
                task = self.task_queue.get(timeout=1)
                if task is None:  # åœæ­¢ä¿¡å·
                    break
                
                task_type, data = task
                if task_type == 'compute_metrics':
                    result = self._compute_metrics_on_cpu(data)
                elif task_type == 'data_preprocessing':
                    result = self._preprocess_data_on_cpu(data)
                elif task_type == 'feature_extraction':
                    result = self._extract_features_on_cpu(data)
                else:
                    result = None
                
                self.result_queue.put((task_type, result))
                self.task_queue.task_done()
                
            except queue.Empty:
                continue
            except Exception as e:
                print(f"CPUå·¥ä½œçº¿ç¨‹ {worker_id} é”™è¯¯: {e}")
    
    def _compute_metrics_on_cpu(self, data):
        """åœ¨CPUä¸Šè®¡ç®—è¯„ä¼°æŒ‡æ ‡"""
        outputs, masks = data
        # å°†æ•°æ®ç§»åˆ°CPUè¿›è¡Œè®¡ç®—
        outputs_cpu = outputs.cpu().detach()
        masks_cpu = masks.cpu().detach()
        
        # è®¡ç®—IoUå’ŒDice
        preds = (torch.sigmoid(outputs_cpu) > 0.5).float()
        masks_cpu = masks_cpu.float()
        
        intersection = (preds * masks_cpu).sum()
        union = (preds + masks_cpu).sum() - intersection
        iou = (intersection + 1e-5) / (union + 1e-5)
        
        dice = (2. * intersection + 1e-5) / (preds.sum() + masks_cpu.sum() + 1e-5)
        
        # è®¡ç®—æ›´å¤šç»Ÿè®¡æŒ‡æ ‡
        accuracy = (preds == masks_cpu).float().mean()
        precision = (preds * masks_cpu).sum() / (preds.sum() + 1e-5)
        recall = (preds * masks_cpu).sum() / (masks_cpu.sum() + 1e-5)
        
        # è®¡ç®—ä¸€äº›ç‰¹å¾ç»Ÿè®¡
        output_stats = {
            'mean': outputs_cpu.mean().item(),
            'std': outputs_cpu.std().item(),
            'min': outputs_cpu.min().item(),
            'max': outputs_cpu.max().item()
        }
        
        # æ¨¡æ‹Ÿä¸€äº›é¢å¤–çš„CPUè®¡ç®—
        #time.sleep(0.001)  # æ¨¡æ‹Ÿ1msçš„CPUè®¡ç®—æ—¶é—´
        
        return {
            'iou': iou.item(), 
            'dice': dice.item(),
            'accuracy': accuracy.item(),
            'precision': precision.item(),
            'recall': recall.item(),
            'output_stats': output_stats
        }
    
    def _preprocess_data_on_cpu(self, data):
        """åœ¨CPUä¸Šè¿›è¡Œæ•°æ®é¢„å¤„ç†"""
        images, masks = data
        # åœ¨CPUä¸Šè¿›è¡Œé¢å¤–çš„æ•°æ®é¢„å¤„ç†
        processed_images = []
        processed_masks = []
        
        for img, mask in zip(images, masks):
            # å½’ä¸€åŒ–å¤„ç†
            img_norm = (img - img.mean()) / (img.std() + 1e-8)
            
            # è®¡ç®—ä¸€äº›å›¾åƒç»Ÿè®¡ä¿¡æ¯
            img_stats = {
                'mean': img.mean().item(),
                'std': img.std().item(),
                'min': img.min().item(),
                'max': img.max().item()
            }
            
            # ç®€å•çš„æ•°æ®å¢å¼º
            if random.random() > 0.5:
                img_norm = torch.flip(img_norm, dims=[-1])  # æ°´å¹³ç¿»è½¬
                mask = torch.flip(mask, dims=[-1])
            
            # è®¡ç®—è¾¹ç¼˜ç‰¹å¾
            if img.dim() == 3:
                # ç®€å•çš„è¾¹ç¼˜æ£€æµ‹
                grad_x = torch.diff(img_norm, dim=-1, prepend=img_norm[:, :, :1])
                grad_y = torch.diff(img_norm, dim=-2, prepend=img_norm[:, :1, :])
                edge_feature = torch.sqrt(grad_x**2 + grad_y**2)
                edge_strength = edge_feature.mean().item()
            else:
                edge_strength = 0.0
            
            processed_images.append(img_norm)
            processed_masks.append(mask)
        
        # æ¨¡æ‹Ÿä¸€äº›é¢å¤–çš„CPUè®¡ç®—æ—¶é—´
        #time.sleep(0.002)  # æ¨¡æ‹Ÿ2msçš„CPUè®¡ç®—æ—¶é—´
        
        return torch.stack(processed_images), torch.stack(processed_masks)
    
    def _extract_features_on_cpu(self, data):
        """åœ¨CPUä¸Šæå–ç‰¹å¾"""
        images = data
        features = []
        
        for img in images:
            # ç®€å•çš„ç‰¹å¾æå–ï¼ˆä¾‹å¦‚è¾¹ç¼˜æ£€æµ‹ï¼‰
            img_cpu = img.cpu().detach()
            # è®¡ç®—æ¢¯åº¦ç‰¹å¾
            grad_x = torch.diff(img_cpu, dim=-1, prepend=img_cpu[:, :, :1])
            grad_y = torch.diff(img_cpu, dim=-2, prepend=img_cpu[:, :1, :])
            edge_feature = torch.sqrt(grad_x**2 + grad_y**2)
            features.append(edge_feature.mean())
        
        return torch.stack(features)
    
    def submit_cpu_task(self, task_type, data):
        """æäº¤CPUä»»åŠ¡"""
        self.task_queue.put((task_type, data))
    
    def get_cpu_result(self, timeout=0.1):
        """è·å–CPUè®¡ç®—ç»“æœ"""
        try:
            return self.result_queue.get(timeout=timeout)
        except queue.Empty:
            return None
    
    def shutdown(self):
        """å…³é—­CPUå·¥ä½œçº¿ç¨‹"""
        for _ in range(self.num_cpu_workers):
            self.task_queue.put(None)
        self.cpu_executor.shutdown(wait=True)

class AsyncDataProcessor:
    """å¼‚æ­¥æ•°æ®å¤„ç†å™¨"""
    def __init__(self, num_workers=12):
        self.num_workers = num_workers
        self.executor = ThreadPoolExecutor(max_workers=num_workers)
        self.futures = []
    
    def submit_preprocessing(self, data_batch):
        """æäº¤æ•°æ®é¢„å¤„ç†ä»»åŠ¡"""
        future = self.executor.submit(self._preprocess_batch, data_batch)
        self.futures.append(future)
        return future
    
    def _preprocess_batch(self, data_batch):
        """é¢„å¤„ç†æ•°æ®æ‰¹æ¬¡"""
        images, masks, sim_feats = data_batch
        
        # åœ¨CPUä¸Šè¿›è¡Œé¢„å¤„ç†
        processed_images = []
        for img in images:
            # æ•°æ®å¢å¼ºå’Œé¢„å¤„ç†
            img_cpu = img.cpu()
            
            # è®¡ç®—å›¾åƒç»Ÿè®¡ä¿¡æ¯
            img_mean = img_cpu.mean()
            img_std = img_cpu.std()
            
            # å½’ä¸€åŒ–å¤„ç†
            img_norm = (img_cpu - img_mean) / (img_std + 1e-8)
            
            # è®¡ç®—ä¸€äº›é¢å¤–çš„ç‰¹å¾
            if img_cpu.dim() == 3:
                # è®¡ç®—æ¢¯åº¦ç‰¹å¾
                grad_x = torch.diff(img_norm, dim=-1, prepend=img_norm[:, :, :1])
                grad_y = torch.diff(img_norm, dim=-2, prepend=img_norm[:, :1, :])
                edge_feature = torch.sqrt(grad_x**2 + grad_y**2)
                
                # è®¡ç®—çº¹ç†ç‰¹å¾
                texture_feature = torch.std(img_norm, dim=(1, 2))
            else:
                edge_feature = torch.zeros(1)
                texture_feature = torch.zeros(1)
            
            processed_images.append(img_norm)
        
        # æ¨¡æ‹Ÿä¸€äº›CPUè®¡ç®—æ—¶é—´
        #time.sleep(0.003)  # æ¨¡æ‹Ÿ3msçš„CPUè®¡ç®—æ—¶é—´
        
        return torch.stack(processed_images), masks, sim_feats
    
    def get_completed_results(self):
        """è·å–å·²å®Œæˆçš„ç»“æœ"""
        completed = []
        remaining = []
        
        for future in self.futures:
            if future.done():
                try:
                    result = future.result()
                    completed.append(result)
                except Exception as e:
                    print(f"é¢„å¤„ç†ä»»åŠ¡å¤±è´¥: {e}")
            else:
                remaining.append(future)
        
        self.futures = remaining
        return completed
    
    def shutdown(self):
        """å…³é—­å¤„ç†å™¨"""
        self.executor.shutdown(wait=True)

class HybridPrecisionTrainer:
    """æ··åˆç²¾åº¦è®­ç»ƒå™¨"""
    def __init__(self, model, optimizer, criterion, device='cuda', num_cpu_workers=8, num_async_workers=8):
        self.model = model
        self.optimizer = optimizer
        self.criterion = criterion
        self.device = device
        self.scaler = GradScaler()
        
        # åˆ›å»ºCPUè¾…åŠ©è®­ç»ƒå™¨
        self.cpu_assistant = CPUAssistedTraining(model, device, num_cpu_workers=num_cpu_workers)
        
        # åˆ›å»ºå¼‚æ­¥æ•°æ®å¤„ç†å™¨
        self.async_processor = AsyncDataProcessor(num_workers=num_async_workers)
        
        # æ€§èƒ½ç»Ÿè®¡
        self.gpu_time = 0
        self.cpu_time = 0
        self.total_batches = 0
    
    def train_step(self, images, masks, sim_feats):
        """æ··åˆç²¾åº¦è®­ç»ƒæ­¥éª¤"""
        start_time = time.time()
        
        # å¼‚æ­¥æäº¤CPUé¢„å¤„ç†ä»»åŠ¡
        cpu_future = self.async_processor.submit_preprocessing((images, masks, sim_feats))
        
        # GPUå‰å‘ä¼ æ’­ - ä½¿ç”¨è‡ªåŠ¨æ··åˆç²¾åº¦
        gpu_start = time.time()
        self.optimizer.zero_grad()
        
        with autocast('cuda'):  # ğŸ‘ˆ è‡ªåŠ¨æ··åˆç²¾åº¦
            outputs = self.model(images, sim_feats)
            loss = self.criterion(outputs, masks)  # ä½¿ç”¨HybridLoss
        
        # ç¼©æ”¾æ¢¯åº¦é¿å…ä¸‹æº¢
        self.scaler.scale(loss).backward()
        self.scaler.step(self.optimizer)
        self.scaler.update()
        
        gpu_time = time.time() - gpu_start
        self.gpu_time += gpu_time
        
        # è·å–CPUé¢„å¤„ç†ç»“æœå¹¶è®¡ç®—CPUæ—¶é—´
        cpu_start = time.time()
        cpu_results = self.async_processor.get_completed_results()
        
        # æäº¤CPUæŒ‡æ ‡è®¡ç®—ä»»åŠ¡
        self.cpu_assistant.submit_cpu_task('compute_metrics', (outputs, masks))
        
        # æ‰§è¡Œä¸€äº›CPUè®¡ç®—ä»»åŠ¡
        cpu_compute_start = time.time()
        # åœ¨CPUä¸Šè¿›è¡Œä¸€äº›é¢å¤–çš„è®¡ç®—
        with torch.no_grad():
            # è®¡ç®—ä¸€äº›ç»Ÿè®¡ä¿¡æ¯
            outputs_cpu = outputs.cpu().detach()
            masks_cpu = masks.cpu().detach()
            
            # è®¡ç®—ä¸€äº›ç»Ÿè®¡æŒ‡æ ‡
            preds = (torch.sigmoid(outputs_cpu) > 0.5).float()
            accuracy = (preds == masks_cpu).float().mean()
            
            # è®¡ç®—ä¸€äº›ç‰¹å¾ç»Ÿè®¡
            feature_stats = {
                'output_mean': outputs_cpu.mean().item(),
                'output_std': outputs_cpu.std().item(),
                'mask_mean': masks_cpu.mean().item(),
                'accuracy': accuracy.item()
            }
            
            # æ‰§è¡Œä¸€äº›é¢å¤–çš„CPUè®¡ç®—æ¥å¢åŠ CPUæ—¶é—´
            # è®¡ç®—è¾¹ç¼˜ç‰¹å¾
            if outputs_cpu.dim() == 4:
                edge_features = []
                for i in range(outputs_cpu.shape[0]):
                    img = outputs_cpu[i, 0]  # å–ç¬¬ä¸€ä¸ªé€šé“
                    grad_x = torch.diff(img, dim=-1, prepend=img[:, :1])
                    grad_y = torch.diff(img, dim=-2, prepend=img[:1, :])
                    edge = torch.sqrt(grad_x**2 + grad_y**2)
                    edge_features.append(edge.mean().item())
            
            # è®¡ç®—çº¹ç†ç‰¹å¾
            texture_features = []
            for i in range(outputs_cpu.shape[0]):
                img = outputs_cpu[i, 0]
                texture = torch.std(img)
                texture_features.append(texture.item())
        
        cpu_time = time.time() - cpu_start
        self.cpu_time += cpu_time
        
        self.total_batches += 1
        
        return loss.item(), outputs
    
    def get_performance_stats(self):
        """è·å–æ€§èƒ½ç»Ÿè®¡"""
        if self.total_batches == 0:
            return {}
        
        avg_gpu_time = self.gpu_time / self.total_batches
        avg_cpu_time = self.cpu_time / self.total_batches
        gpu_utilization = avg_gpu_time / (avg_gpu_time + avg_cpu_time) * 100
        
        return {
            'avg_gpu_time': avg_gpu_time,
            'avg_cpu_time': avg_cpu_time,
            'gpu_utilization': gpu_utilization,
            'total_batches': self.total_batches
        }
    
    def shutdown(self):
        """å…³é—­è®­ç»ƒå™¨"""
        self.cpu_assistant.shutdown()
        self.async_processor.shutdown()

# ====== è‡ªå®šä¹‰collateå‡½æ•° ======
def custom_collate_fn(batch):
    """
    è‡ªå®šä¹‰collateå‡½æ•°ï¼Œç¡®ä¿æ‰€æœ‰tensorç»´åº¦ä¸€è‡´
    """
    images, masks, sim_feats = zip(*batch)
    
    # ç¡®ä¿æ‰€æœ‰å›¾åƒtensorç»´åº¦ä¸€è‡´ [B, C, H, W]
    images = list(images)
    for i, img in enumerate(images):
        if img.dim() == 2:
            images[i] = img.unsqueeze(0)  # [H, W] -> [1, H, W]
        elif img.dim() == 3 and img.shape[0] not in [1, 3]:
            images[i] = img.permute(2, 0, 1)  # [H, W, C] -> [C, H, W]
    
    # ç¡®ä¿æ‰€æœ‰mask tensorç»´åº¦ä¸€è‡´ [B, 1, H, W]
    masks = list(masks)
    for i, mask in enumerate(masks):
        if mask.dim() == 2:
            masks[i] = mask.unsqueeze(0)  # [H, W] -> [1, H, W]
        elif mask.dim() == 1:
            masks[i] = mask.unsqueeze(0).unsqueeze(0)  # [H] -> [1, 1, H]
    
    # ç¡®ä¿æ‰€æœ‰sim_feats tensorç»´åº¦ä¸€è‡´ [B, 11]
    sim_feats = list(sim_feats)
    for i, sim_feat in enumerate(sim_feats):
        if sim_feat.dim() == 0:
            sim_feats[i] = sim_feat.unsqueeze(0)  # [] -> [1]
    
    # ä½¿ç”¨torch.stackè¿›è¡Œæ‰¹å¤„ç†
    try:
        images = torch.stack(images, dim=0)
        masks = torch.stack(masks, dim=0)
        sim_feats = torch.stack(sim_feats, dim=0)
    except Exception as e:
        print(f"Collateé”™è¯¯: {e}")
        print(f"å›¾åƒå½¢çŠ¶: {[img.shape for img in images]}")
        print(f"æ©ç å½¢çŠ¶: {[mask.shape for mask in masks]}")
        print(f"ç‰¹å¾å½¢çŠ¶: {[sim_feat.shape for sim_feat in sim_feats]}")
        raise e
    
    return images, masks, sim_feats

# ====== å†…å­˜é¢„åŠ è½½æ•°æ®é›†ç±» ======
class MemoryCachedDataset(Dataset):
    """å°†æ•°æ®é¢„åŠ è½½åˆ°å†…å­˜ä¸­çš„æ•°æ®é›†åŒ…è£…å™¨"""
    def __init__(self, base_dataset, device='cpu', cache_sim_features=True):
        self.base_dataset = base_dataset
        self.device = device
        self.cache_sim_features = cache_sim_features
        
        print(f"æ­£åœ¨å°†æ•°æ®é›†é¢„åŠ è½½åˆ°å†…å­˜ä¸­...")
        self.cached_data = []
        
        for i in tqdm(range(len(base_dataset)), desc="é¢„åŠ è½½æ•°æ®"):
            try:
                img, mask, sim_feat = base_dataset[i]
                
                # å°†æ•°æ®ç§»åŠ¨åˆ°æŒ‡å®šè®¾å¤‡
                if isinstance(img, torch.Tensor):
                    img = img.to(device, non_blocking=True)
                if isinstance(mask, torch.Tensor):
                    mask = mask.to(device, non_blocking=True)
                if isinstance(sim_feat, torch.Tensor) and self.cache_sim_features:
                    sim_feat = sim_feat.to(device, non_blocking=True)
                
                self.cached_data.append((img, mask, sim_feat))
                
            except Exception as e:
                print(f"é¢„åŠ è½½æ ·æœ¬ {i} æ—¶å‡ºé”™: {e}")
                # åˆ›å»ºä¸€ä¸ªç©ºçš„æ›¿ä»£æ ·æœ¬
                if len(self.cached_data) > 0:
                    # ä½¿ç”¨ç¬¬ä¸€ä¸ªæˆåŠŸåŠ è½½çš„æ ·æœ¬ä½œä¸ºæ¨¡æ¿
                    template_img, template_mask, template_sim = self.cached_data[0]
                    self.cached_data.append((template_img.clone(), template_mask.clone(), template_sim.clone()))
                else:
                    # å¦‚æœè¿˜æ²¡æœ‰æˆåŠŸåŠ è½½çš„æ ·æœ¬ï¼Œåˆ›å»ºä¸€ä¸ªé»˜è®¤æ ·æœ¬
                    default_img = torch.zeros(3, 64, 64, device=device)
                    default_mask = torch.zeros(1, 64, 64, device=device)
                    default_sim = torch.zeros(11, device=device)
                    self.cached_data.append((default_img, default_mask, default_sim))
        
        print(f"âœ… æ•°æ®é¢„åŠ è½½å®Œæˆï¼å…±åŠ è½½ {len(self.cached_data)} ä¸ªæ ·æœ¬åˆ° {device}")
    
    def __len__(self):
        return len(self.cached_data)
    
    def __getitem__(self, idx):
        return self.cached_data[idx]

class GPUPreloadedDataset(Dataset):
    """å°†æ•°æ®é¢„åŠ è½½åˆ°GPUæ˜¾å­˜ä¸­çš„æ•°æ®é›†åŒ…è£…å™¨"""
    def __init__(self, base_dataset, device='cuda', batch_size=32):
        self.base_dataset = base_dataset
        self.device = device
        self.batch_size = batch_size
        
        print(f"æ­£åœ¨å°†æ•°æ®é›†é¢„åŠ è½½åˆ°GPUæ˜¾å­˜ä¸­...")
        self.cached_batches = []
        
        # æŒ‰æ‰¹æ¬¡é¢„åŠ è½½
        for i in tqdm(range(0, len(base_dataset), batch_size), desc="é¢„åŠ è½½GPUæ‰¹æ¬¡"):
            batch_end = min(i + batch_size, len(base_dataset))
            batch_data = []
            
            for j in range(i, batch_end):
                try:
                    img, mask, sim_feat = base_dataset[j]
                    
                    # ç¡®ä¿æ•°æ®æ˜¯tensoræ ¼å¼
                    if not isinstance(img, torch.Tensor):
                        img = torch.tensor(img, dtype=torch.float32)
                    if not isinstance(mask, torch.Tensor):
                        mask = torch.tensor(mask, dtype=torch.float32)
                    if not isinstance(sim_feat, torch.Tensor):
                        sim_feat = torch.tensor(sim_feat, dtype=torch.float32)
                    
                    batch_data.append((img, mask, sim_feat))
                    
                except Exception as e:
                    print(f"é¢„åŠ è½½æ ·æœ¬ {j} æ—¶å‡ºé”™: {e}")
                    # åˆ›å»ºé»˜è®¤æ ·æœ¬
                    default_img = torch.zeros(3, 64, 64, dtype=torch.float32)
                    default_mask = torch.zeros(1, 64, 64, dtype=torch.float32)
                    default_sim = torch.zeros(11, dtype=torch.float32)
                    batch_data.append((default_img, default_mask, default_sim))
            
            # å°†æ‰¹æ¬¡æ•°æ®ç§»åŠ¨åˆ°GPU
            if batch_data:
                batch_imgs = torch.stack([item[0] for item in batch_data]).to(device, non_blocking=True)
                batch_masks = torch.stack([item[1] for item in batch_data]).to(device, non_blocking=True)
                batch_sims = torch.stack([item[2] for item in batch_data]).to(device, non_blocking=True)
                
                self.cached_batches.append((batch_imgs, batch_masks, batch_sims))
        
        print(f"âœ… GPUæ•°æ®é¢„åŠ è½½å®Œæˆï¼å…±åŠ è½½ {len(self.cached_batches)} ä¸ªæ‰¹æ¬¡åˆ° {device}")
    
    def __len__(self):
        return len(self.cached_batches) * self.batch_size
    
    def __getitem__(self, idx):
        batch_idx = idx // self.batch_size
        item_idx = idx % self.batch_size
        
        if batch_idx < len(self.cached_batches):
            batch_imgs, batch_masks, batch_sims = self.cached_batches[batch_idx]
            if item_idx < batch_imgs.shape[0]:
                return batch_imgs[item_idx], batch_masks[item_idx], batch_sims[item_idx]
        
        # è¿”å›é»˜è®¤å€¼
        return torch.zeros(3, 64, 64, device=self.device), torch.zeros(1, 64, 64, device=self.device), torch.zeros(11, device=self.device)

# ====== ä¼˜åŒ–åçš„Patchçº§åˆ«å¢å¼ºç±» ======
class AdvancedAugmentation:
    def __init__(self, is_training=True):
        self.is_training = is_training
        
        # è®­ç»ƒé›†å¢å¼º - ç®€åŒ–ä¸ºæ›´ç¨³å®šçš„å˜æ¢
        self.train_transform = A.Compose([
            # åŸºç¡€å‡ ä½•å˜æ¢
            A.RandomRotate90(p=0.3),
            A.HorizontalFlip(p=0.3),
            A.VerticalFlip(p=0.3),
            
            # è½»å¾®ç¼©æ”¾
            A.Affine(scale=(0.9, 1.1), keep_ratio=True, p=0.3),
            
            # åŸºç¡€é¢œè‰²å˜æ¢
            A.RandomBrightnessContrast(brightness_limit=0.1, contrast_limit=0.1, p=0.3),
            A.HueSaturationValue(hue_shift_limit=5, sat_shift_limit=10, val_shift_limit=5, p=0.3),
                
            # è½»å¾®å™ªå£°
            A.GaussNoise(var_limit=(10.0, 50.0), p=0.2),
            
            # æ ¼å¼è½¬æ¢
            A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
            ToTensorV2()
        ], additional_targets={'mask': 'mask'})
        
        # éªŒè¯é›†è½¬æ¢ - ä»…åŸºç¡€é¢„å¤„ç†
        self.val_transform = A.Compose([
            A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
            ToTensorV2()
        ], additional_targets={'mask': 'mask'})

    def __call__(self, image, mask):
        if self.is_training:
            augmented = self.train_transform(image=image, mask=mask)
        else:
            augmented = self.val_transform(image=image, mask=mask)
        return augmented['image'], augmented['mask']

class DamageAwareDataset(Dataset):
    def __init__(self, base_dataset, damage_boost=5, normal_ratio=0.05, allow_synthetic=True):
        self.base_dataset = base_dataset
        self.damage_indices = []
        self.normal_indices = []
        self.normal_ratio = normal_ratio  # æ­£å¸¸æ ·æœ¬æ¯”ä¾‹ï¼Œé»˜è®¤5%
        self.allow_synthetic = allow_synthetic  # æ˜¯å¦å…è®¸åˆæˆæ­£å¸¸æ ·æœ¬
        
        # é¢„æ‰«ææ•°æ®é›†ç»Ÿè®¡æŸåæ ·æœ¬
        for i in tqdm(range(len(base_dataset)), desc="Scanning damage samples"):
            _, mask, *_ = base_dataset[i]
            if mask.sum() > 0:
                self.damage_indices.append(i)
            else:
                self.normal_indices.append(i)
        
        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°æ­£å¸¸æ ·æœ¬ï¼Œåˆ›å»ºä¸€äº›æ­£å¸¸æ ·æœ¬
        if len(self.normal_indices) == 0 and self.allow_synthetic:
            print("è­¦å‘Š: æ²¡æœ‰æ‰¾åˆ°æ­£å¸¸æ ·æœ¬ï¼Œå°†åˆ›å»ºåˆæˆæ­£å¸¸æ ·æœ¬")
            # ä»æŸåæ ·æœ¬ä¸­éšæœºé€‰æ‹©ä¸€äº›ï¼Œå°†å…¶æ©ç æ¸…é›¶ä½œä¸ºæ­£å¸¸æ ·æœ¬
            num_normal_needed = max(1, int(len(self.damage_indices) * self.normal_ratio))
            selected_indices = random.sample(self.damage_indices, min(num_normal_needed, len(self.damage_indices)))
            self.normal_indices = selected_indices
            print(f"åˆ›å»ºäº† {len(self.normal_indices)} ä¸ªåˆæˆæ­£å¸¸æ ·æœ¬")
        elif len(self.normal_indices) == 0 and not self.allow_synthetic:
            print("è­¦å‘Š: æ²¡æœ‰æ‰¾åˆ°æ­£å¸¸æ ·æœ¬ï¼Œä¸”ä¸å…è®¸åˆæˆæ ·æœ¬")
        
        # ç¡®ä¿æ­£å¸¸æ ·æœ¬æ•°é‡ç¬¦åˆæ¯”ä¾‹è¦æ±‚
        target_normal_count = int(len(self.damage_indices) * self.normal_ratio)
        if len(self.normal_indices) > target_normal_count:
            # å¦‚æœæ­£å¸¸æ ·æœ¬å¤ªå¤šï¼Œéšæœºé€‰æ‹©
            self.normal_indices = random.sample(self.normal_indices, target_normal_count)
        elif len(self.normal_indices) < target_normal_count and self.allow_synthetic:
            # å¦‚æœæ­£å¸¸æ ·æœ¬å¤ªå°‘ï¼Œä»æŸåæ ·æœ¬ä¸­å¤åˆ¶ä¸€äº›å¹¶æ¸…é›¶æ©ç 
            additional_needed = target_normal_count - len(self.normal_indices)
            additional_indices = random.sample(self.damage_indices, min(additional_needed, len(self.damage_indices)))
            self.normal_indices.extend(additional_indices)
        
        self.damage_boost = damage_boost
        print(f"Found {len(self.damage_indices)} damage samples and {len(self.normal_indices)} normal samples")
        print(f"Normal sample ratio: {len(self.normal_indices) / (len(self.damage_indices) + len(self.normal_indices)) * 100:.1f}%")

    def __len__(self):
        return len(self.normal_indices) + len(self.damage_indices) * self.damage_boost

    def __getitem__(self, idx):
        if idx < len(self.damage_indices) * self.damage_boost:
            # æŸåæ ·æœ¬
            damage_idx = idx % len(self.damage_indices)
            return self.base_dataset[self.damage_indices[damage_idx]]
        else:
            # æ­£å¸¸æ ·æœ¬
            normal_idx = (idx - len(self.damage_indices) * self.damage_boost) % len(self.normal_indices)
            img, mask, sim_feat = self.base_dataset[self.normal_indices[normal_idx]]
            
            # å¯¹äºæ­£å¸¸æ ·æœ¬ï¼Œå°†æ©ç æ¸…é›¶ï¼ˆç¡®ä¿æ²¡æœ‰æŸååŒºåŸŸï¼‰
            if mask.sum() > 0:
                mask = torch.zeros_like(mask)
            
            return img, mask, sim_feat

class YOLOLandslideDataset(Dataset):
    """YOLOæ ¼å¼çš„å±±ä½“æ»‘å¡æ•°æ®é›†"""
    def __init__(self, images_dir, labels_dir, transform=None, disaster_class_ids=None):
        self.images_dir = images_dir
        self.labels_dir = labels_dir
        self.transform = transform
        
        # ç¾å®³ç›¸å…³ç±»åˆ«ID (COCO 80ç±»ä¸­çš„ç›¸å…³ID)
        self.disaster_class_ids = disaster_class_ids or [
            0, 1, 2, 3, 5, 6, 7, 8, 10, 24, 25, 27, 28, 29, 33, 
            44, 56, 57, 58, 59, 60, 62, 63, 67, 73
        ]
        
        # è·å–æ‰€æœ‰å›¾åƒæ–‡ä»¶
        self.image_files = glob.glob(os.path.join(images_dir, "*.jpg"))
        if not self.image_files:
            # å°è¯•å…¶ä»–å›¾åƒæ ¼å¼
            self.image_files = glob.glob(os.path.join(images_dir, "*.jpeg")) + \
                             glob.glob(os.path.join(images_dir, "*.png"))
    
    def __len__(self):
        return len(self.image_files)
    
    def __getitem__(self, idx):
        img_path = self.image_files[idx]
        
        # åŠ è½½å›¾åƒ
        try:
            img = Image.open(img_path).convert('RGB')
        except:
            # å¦‚æœå›¾åƒæŸåï¼Œåˆ›å»ºæ›¿ä»£å›¾åƒ
            img = Image.new('RGB', (224, 224), 
                           color=(random.randint(0, 255), 
                                  random.randint(0, 255), 
                                  random.randint(0, 255)))
        
        # è·å–å¯¹åº”çš„æ ‡æ³¨æ–‡ä»¶è·¯å¾„
        base_name = os.path.splitext(os.path.basename(img_path))[0]
        label_path = os.path.join(self.labels_dir, f"{base_name}.txt")
        
        # äºŒåˆ†ç±»æ ‡ç­¾: 1=åŒ…å«ç¾å®³ç›¸å…³ç‰©ä½“ï¼Œ0=ä¸åŒ…å«
        label = 0
        
        # å¦‚æœæ ‡æ³¨æ–‡ä»¶å­˜åœ¨ï¼Œæ£€æŸ¥æ˜¯å¦åŒ…å«ç¾å®³ç±»åˆ«
        if os.path.exists(label_path):
            with open(label_path, 'r') as f:
                for line in f:
                    parts = line.strip().split()
                    if len(parts) >= 1:
                        try:
                            class_id = int(parts[0])
                            if class_id in self.disaster_class_ids:
                                label = 1
                                break  # åªè¦æœ‰ä¸€ä¸ªç¾å®³ç‰©ä½“å°±æ ‡è®°ä¸º1
                        except ValueError:
                            continue
        
        if self.transform:
            img = self.transform(img)
            
        return img, label

class CombinedLandslideDataset(Dataset):
    def __init__(self, images_dir, labels_dir, transform=None):
        self.images_dir = images_dir
        self.labels_dir = labels_dir
        self.transform = transform
        self.image_files = glob.glob(os.path.join(images_dir, '*.*'))
        self.image_files = [f for f in self.image_files if f.lower().endswith(('.png', '.jpg', '.jpeg'))]
    
    def __len__(self):
        return len(self.image_files)
    
    def __getitem__(self, idx):
        img_path = self.image_files[idx]
        base_name = os.path.basename(img_path)
        label = 0
        if "xview2" in base_name.lower() and "post" in base_name.lower():
            label = 1
        elif "xview2" in base_name.lower() and "pre" in base_name.lower():
            label = 0
        else:
            label_path = os.path.join(self.labels_dir, os.path.splitext(base_name)[0] + ".txt")
            if os.path.exists(label_path):
                with open(label_path, 'r') as f:
                    for line in f:
                        if line.strip():
                            label = 1
                            break
        try:
            img = Image.open(img_path).convert('RGB')
        except:
            img = Image.new('RGB', (224, 224), color=(random.randint(0,255),random.randint(0,255),random.randint(0,255)))
        if self.transform:
            img = self.transform(img)
        return img, label

def get_segmentation_dataloaders(data_root="data/combined_dataset", batch_size=32, num_workers=8, show_warnings=False, skip_problematic_samples=False):
    """
    è·å–åˆ†å‰²ä»»åŠ¡çš„æ•°æ®åŠ è½½å™¨
    è¿”å›: (train_loader, val_loader, test_loader)
    
    Args:
        data_root: æ•°æ®æ ¹ç›®å½•
        batch_size: æ‰¹æ¬¡å¤§å°
        num_workers: å·¥ä½œè¿›ç¨‹æ•°
        show_warnings: æ˜¯å¦æ˜¾ç¤ºæ•°æ®è´¨é‡è­¦å‘Š
        skip_problematic_samples: æ˜¯å¦è·³è¿‡æœ‰é—®é¢˜çš„æ ·æœ¬
    """
    # æ•°æ®å¢å¼ºå’Œè½¬æ¢
    train_transform = T.Compose([
        T.Resize((256, 256)),
        T.RandomHorizontalFlip(),
        T.RandomVerticalFlip(),
        T.ToTensor(),
        T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
    ])
    
    val_transform = T.Compose([
        T.Resize((256, 256)),
        T.ToTensor(),
        T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
    ])

    # åˆ›å»ºæ•°æ®é›† - åº”ç”¨é€‰é¡¹1ï¼šéšè—è­¦å‘Šä½†ä¿ç•™æ‰€æœ‰æ•°æ®
    train_dataset = XView2SegmentationDataset(
        os.path.join(data_root, "images", "train2017"),
        os.path.join(data_root, "masks", "train2017"),
        transform=train_transform,
        show_warnings=show_warnings,
        skip_problematic_samples=skip_problematic_samples
    )
    
    val_dataset = XView2SegmentationDataset(
        os.path.join(data_root, "images", "val2017"),
        os.path.join(data_root, "masks", "val2017"), 
        transform=val_transform,
        show_warnings=show_warnings,
        skip_problematic_samples=skip_problematic_samples
    )
    
    test_dataset = XView2SegmentationDataset(
        os.path.join(data_root, "images", "test2017"),
        os.path.join(data_root, "masks", "test2017"),
        transform=val_transform,
        show_warnings=show_warnings,
        skip_problematic_samples=skip_problematic_samples
    )

    # åˆ›å»ºæ•°æ®åŠ è½½å™¨
    train_loader = DataLoader(
        train_dataset,
        batch_size=batch_size,
        shuffle=True,
        num_workers=num_workers,
        pin_memory=True
    )
    
    val_loader = DataLoader(
        val_dataset,
        batch_size=batch_size,
        shuffle=False,
        num_workers=num_workers,
        pin_memory=True
    )
    
    test_loader = DataLoader(
        test_dataset,
        batch_size=batch_size,
        shuffle=False,
        num_workers=num_workers,
        pin_memory=True
    )

    return train_loader, val_loader, test_loader

def get_multi_class_dataloaders(data_root="data/combined_dataset", batch_size=32, num_workers=8, 
                               damage_level='categorical', show_warnings=False, skip_problematic_samples=False):
    """
    è·å–å¤šç±»åˆ«åˆ†ç±»ä»»åŠ¡çš„æ•°æ®åŠ è½½å™¨
    æ”¯æŒ5ä¸ªç±»åˆ«ï¼šèƒŒæ™¯(0)ã€æœªæŸå(1)ã€è½»å¾®æŸå(2)ã€ä¸­ç­‰æŸå(3)ã€ä¸¥é‡æŸå(4)
    
    Args:
        data_root: æ•°æ®æ ¹ç›®å½•
        batch_size: æ‰¹æ¬¡å¤§å°
        num_workers: å·¥ä½œè¿›ç¨‹æ•°
        damage_level: æ©ç å¤„ç†æ–¹å¼ï¼Œæ¨èä½¿ç”¨'categorical'è¿›è¡Œå¤šç±»åˆ«åˆ†ç±»
        show_warnings: æ˜¯å¦æ˜¾ç¤ºæ•°æ®è´¨é‡è­¦å‘Š
        skip_problematic_samples: æ˜¯å¦è·³è¿‡æœ‰é—®é¢˜çš„æ ·æœ¬
    """
    # æ•°æ®å¢å¼ºå’Œè½¬æ¢
    train_transform = T.Compose([
        T.Resize((256, 256)),
        T.RandomHorizontalFlip(),
        T.RandomVerticalFlip(),
        T.RandomRotation(10),
        T.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),
        T.ToTensor(),
        T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
    ])
    
    val_transform = T.Compose([
        T.Resize((256, 256)),
        T.ToTensor(),
        T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
    ])

    # åˆ›å»ºæ•°æ®é›† - ä½¿ç”¨å¤šç±»åˆ«æ©ç å¤„ç†
    train_dataset = XView2SegmentationDataset(
        os.path.join(data_root, "images", "train2017"),
        os.path.join(data_root, "masks", "train2017"),
        transform=train_transform,
        damage_level=damage_level,
        show_warnings=show_warnings,
        skip_problematic_samples=skip_problematic_samples
    )
    
    val_dataset = XView2SegmentationDataset(
        os.path.join(data_root, "images", "val2017"),
        os.path.join(data_root, "masks", "val2017"), 
        transform=val_transform,
        damage_level=damage_level,
        show_warnings=show_warnings,
        skip_problematic_samples=skip_problematic_samples
    )
    
    test_dataset = XView2SegmentationDataset(
        os.path.join(data_root, "images", "test2017"),
        os.path.join(data_root, "masks", "test2017"),
        transform=val_transform,
        damage_level=damage_level,
        show_warnings=show_warnings,
        skip_problematic_samples=skip_problematic_samples
    )

    # åˆ›å»ºæ•°æ®åŠ è½½å™¨
    train_loader = DataLoader(
        train_dataset,
        batch_size=batch_size,
        shuffle=True,
        num_workers=num_workers,
        pin_memory=True
    )
    
    val_loader = DataLoader(
        val_dataset,
        batch_size=batch_size,
        shuffle=False,
        num_workers=num_workers,
        pin_memory=True
    )
    
    test_loader = DataLoader(
        test_dataset,
        batch_size=batch_size,
        shuffle=False,
        num_workers=num_workers,
        pin_memory=True
    )

    print(f"[å¤šç±»åˆ«æ•°æ®åŠ è½½] ä½¿ç”¨damage_level='{damage_level}'")
    print(f"[å¤šç±»åˆ«æ•°æ®åŠ è½½] è®­ç»ƒé›†å¤§å°: {len(train_dataset)}")
    print(f"[å¤šç±»åˆ«æ•°æ®åŠ è½½] éªŒè¯é›†å¤§å°: {len(val_dataset)}")
    print(f"[å¤šç±»åˆ«æ•°æ®åŠ è½½] æµ‹è¯•é›†å¤§å°: {len(test_dataset)}")

    return train_loader, val_loader, test_loader

def get_landslide_dataloaders(data_root="data/combined_dataset", batch_size=4):
    """
    è·å–å±±ä½“æ»‘å¡åˆ†ç±»æ•°æ®åŠ è½½å™¨
    é€‚ç”¨äºäºŒåˆ†ç±»ä»»åŠ¡ï¼ˆæ»‘å¡/éæ»‘å¡ï¼‰
    """
    transform = T.Compose([
        T.Resize((224, 224)),
        T.ToTensor(),
        T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
    ])
    
    def make_loader(split):
        images_dir = os.path.join(data_root, "images", split)
        labels_dir = os.path.join(data_root, "labels", split)
        
        if not os.path.exists(images_dir) or not os.path.exists(labels_dir):
            print(f"è­¦å‘Š: {split} ç›®å½•ä¸å­˜åœ¨: {images_dir} æˆ– {labels_dir}")
            return None
        
        dataset = CombinedLandslideDataset(images_dir, labels_dir, transform)
        return DataLoader(dataset, batch_size=batch_size, shuffle=(split=="train2017"), num_workers=4, pin_memory=True)
    
    return make_loader("train2017"), make_loader("val2017"), make_loader("test2017")

def get_calibration_loader(data_root="data/combined_dataset", batch_size=32, num_samples=100):
    train_images_dir = os.path.join(data_root, "images", "train2017")
    train_labels_dir = os.path.join(data_root, "labels", "train2017")
    dataset = CombinedLandslideDataset(train_images_dir, train_labels_dir, transform=T.Compose([
        T.Resize((224, 224)),
        T.ToTensor(),
        T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
    ]))
    if len(dataset) < num_samples:
        num_samples = len(dataset)
    indices = random.sample(range(len(dataset)), num_samples)
    calib_subset = torch.utils.data.Subset(dataset, indices)
    return DataLoader(calib_subset, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True)

def load_sim_features(sim_feature_csv='data/sim_features.csv', normalize=True):
    """
    åŠ è½½å¹¶å½’ä¸€åŒ–ä»¿çœŸç‰¹å¾
    Args:
        sim_feature_csv: ä»¿çœŸç‰¹å¾CSVæ–‡ä»¶è·¯å¾„
        normalize: æ˜¯å¦è¿›è¡Œå½’ä¸€åŒ–å¤„ç†
    """
    sim_dict = {}
    all_features = []  # ç”¨äºè®¡ç®—å½’ä¸€åŒ–å‚æ•°
    
    try:
        with open(sim_feature_csv, 'r', encoding='utf-8') as f:
            reader = csv.DictReader(f)
            for row in reader:
                # æ•°å€¼å‹ç‰¹å¾
                float_feats = []
                for col in ['comm_snr', 'radar_feat', 'radar_max', 'radar_std', 
                           'radar_peak_idx', 'path_loss', 'shadow_fading', 
                           'rain_attenuation', 'target_rcs', 'bandwidth', 'ber']:
                    try:
                        value = float(row[col])
                        # å¤„ç†å¼‚å¸¸å€¼
                        if np.isnan(value) or np.isinf(value):
                            value = 0.0
                        float_feats.append(value)
                    except (ValueError, KeyError):
                        float_feats.append(0.0)
                
                # å­—ç¬¦ä¸²å‹ç‰¹å¾
                str_feats = [row.get('channel_type', ''), row.get('modulation', '')]
                sim_dict[row['img_path']] = (float_feats, str_feats)
                all_features.append(float_feats)
        
        # å½’ä¸€åŒ–å¤„ç†
        if normalize and all_features:
            all_features = np.array(all_features)
            # è®¡ç®—æ¯ä¸ªç‰¹å¾çš„ç»Ÿè®¡ä¿¡æ¯
            feature_means = np.mean(all_features, axis=0)
            feature_stds = np.std(all_features, axis=0)
            
            # é¿å…é™¤é›¶
            feature_stds = np.where(feature_stds < 1e-8, 1.0, feature_stds)
            
            # å½’ä¸€åŒ–æ‰€æœ‰ç‰¹å¾
            for img_path, (float_feats, str_feats) in sim_dict.items():
                float_feats = np.array(float_feats)
                normalized_feats = (float_feats - feature_means) / feature_stds
                sim_dict[img_path] = (normalized_feats.tolist(), str_feats)
            
            print(f"ä»¿çœŸç‰¹å¾å½’ä¸€åŒ–å®Œæˆï¼Œå‡å€¼: {feature_means}, æ ‡å‡†å·®: {feature_stds}")
        
        print(f"æˆåŠŸåŠ è½½ {len(sim_dict)} ä¸ªä»¿çœŸç‰¹å¾")
        
    except FileNotFoundError:
        print(f"è­¦å‘Š: ä»¿çœŸç‰¹å¾æ–‡ä»¶ {sim_feature_csv} ä¸å­˜åœ¨")
    except Exception as e:
        print(f"åŠ è½½ä»¿çœŸç‰¹å¾æ—¶å‡ºé”™: {e}")
    
    return sim_dict

def process_xview2_mask(mask_tensor, damage_level='all', sample_ratio=None):
    """
    å¢å¼ºçš„xView2æ©ç å¤„ç†å‡½æ•°ï¼Œæ”¯æŒæ ¹æ®æ ·æœ¬æ¯”ä¾‹åŠ¨æ€è°ƒæ•´
    
    Args:
        mask_tensor: è¾“å…¥æ©ç å¼ é‡
        damage_level: æŸåçº§åˆ«å¤„ç†æ–¹å¼
            'all': æ‰€æœ‰æŸåçº§åˆ«(2,3,4)éƒ½æ ‡è®°ä¸º1
            'light': è½»å¾®æŸå(2)æ ‡è®°ä¸º0.3ï¼Œä¸­ç­‰(3)æ ‡è®°ä¸º0.6ï¼Œä¸¥é‡(4)æ ‡è®°ä¸º1.0
            'binary': è½»å¾®(2)æ ‡è®°ä¸º0ï¼Œä¸­ç­‰å’Œä¸¥é‡(3,4)æ ‡è®°ä¸º1
            'multi': è½»å¾®(2)æ ‡è®°ä¸º1ï¼Œä¸­ç­‰(3)æ ‡è®°ä¸º2ï¼Œä¸¥é‡(4)æ ‡è®°ä¸º3
            'progressive': æ¸è¿›å¼æƒé‡ï¼šè½»å¾®(2)=0.25ï¼Œä¸­ç­‰(3)=0.5ï¼Œä¸¥é‡(4)=1.0
            'categorical': å¤šç±»åˆ«åˆ†ç±»ï¼šèƒŒæ™¯(0)=0ï¼ŒæœªæŸå(1)=1ï¼Œè½»å¾®(2)=2ï¼Œä¸­ç­‰(3)=3ï¼Œä¸¥é‡(4)=4
            'damage_only': åªå…³æ³¨æŸååŒºåŸŸï¼šè½»å¾®(2)=1ï¼Œä¸­ç­‰(3)=2ï¼Œä¸¥é‡(4)=3ï¼Œå…¶ä»–=0
            'severity_weighted': ä¸¥é‡ç¨‹åº¦åŠ æƒï¼šè½»å¾®(2)=0.2ï¼Œä¸­ç­‰(3)=0.5ï¼Œä¸¥é‡(4)=1.0
            'adaptive': æ ¹æ®æ ·æœ¬æ¯”ä¾‹åŠ¨æ€è°ƒæ•´æƒé‡
        sample_ratio: æ ·æœ¬æ¯”ä¾‹ä¿¡æ¯ï¼Œç”¨äºè‡ªé€‚åº”è°ƒæ•´
            - å¦‚æœä¸ºNoneï¼Œä½¿ç”¨é»˜è®¤æƒé‡
            - å¦‚æœæä¾›ï¼Œæ ¹æ®æ¯”ä¾‹è°ƒæ•´æƒé‡
    """
    if damage_level == 'all':
        # åŸå§‹è¡Œä¸ºï¼šæ‰€æœ‰æŸåçº§åˆ«éƒ½æ ‡è®°ä¸º1
        return (mask_tensor >= 2).float()
    elif damage_level == 'light':
        # æ ¹æ®æŸåç¨‹åº¦åˆ†é…ä¸åŒæƒé‡
        result = torch.zeros_like(mask_tensor, dtype=torch.float32)
        result[mask_tensor == 2] = 0.3  # è½»å¾®æŸå
        result[mask_tensor == 3] = 0.6  # ä¸­ç­‰æŸå
        result[mask_tensor == 4] = 1.0  # ä¸¥é‡æŸå
        return result
    elif damage_level == 'binary':
        # è½»å¾®æŸåä¸ç®—æŸåï¼Œåªæœ‰ä¸­ç­‰å’Œä¸¥é‡æŸåæ‰ç®—
        return (mask_tensor >= 3).float()
    elif damage_level == 'multi':
        # å¤šçº§åˆ†ç±»ï¼šè½»å¾®=1ï¼Œä¸­ç­‰=2ï¼Œä¸¥é‡=3
        result = torch.zeros_like(mask_tensor, dtype=torch.float32)
        result[mask_tensor == 2] = 1.0  # è½»å¾®æŸå
        result[mask_tensor == 3] = 2.0  # ä¸­ç­‰æŸå
        result[mask_tensor == 4] = 3.0  # ä¸¥é‡æŸå
        return result
    elif damage_level == 'progressive':
        # æ¸è¿›å¼æƒé‡ï¼šæ›´ç»†è‡´çš„æŸåç¨‹åº¦åŒºåˆ†
        result = torch.zeros_like(mask_tensor, dtype=torch.float32)
        result[mask_tensor == 2] = 0.25  # è½»å¾®æŸå
        result[mask_tensor == 3] = 0.5   # ä¸­ç­‰æŸå
        result[mask_tensor == 4] = 1.0   # ä¸¥é‡æŸå
        return result
    elif damage_level == 'categorical':
        # å¤šç±»åˆ«åˆ†ç±»ï¼šä¿æŒåŸå§‹ç±»åˆ«æ ‡ç­¾
        result = torch.zeros_like(mask_tensor, dtype=torch.float32)
        result[mask_tensor == 0] = 0.0  # èƒŒæ™¯
        result[mask_tensor == 1] = 1.0  # æœªæŸå
        result[mask_tensor == 2] = 2.0  # è½»å¾®æŸå
        result[mask_tensor == 3] = 3.0  # ä¸­ç­‰æŸå
        result[mask_tensor == 4] = 4.0  # ä¸¥é‡æŸå
        return result
    elif damage_level == 'damage_only':
        # åªå…³æ³¨æŸååŒºåŸŸï¼Œå¿½ç•¥èƒŒæ™¯å’ŒæœªæŸååŒºåŸŸ
        result = torch.zeros_like(mask_tensor, dtype=torch.float32)
        result[mask_tensor == 2] = 1.0  # è½»å¾®æŸå
        result[mask_tensor == 3] = 2.0  # ä¸­ç­‰æŸå
        result[mask_tensor == 4] = 3.0  # ä¸¥é‡æŸå
        return result
    elif damage_level == 'severity_weighted':
        # ä¸¥é‡ç¨‹åº¦åŠ æƒï¼šæ›´å¼ºè°ƒä¸¥é‡æŸå
        result = torch.zeros_like(mask_tensor, dtype=torch.float32)
        result[mask_tensor == 2] = 0.2  # è½»å¾®æŸå
        result[mask_tensor == 3] = 0.5  # ä¸­ç­‰æŸå
        result[mask_tensor == 4] = 1.0  # ä¸¥é‡æŸå
        return result
    elif damage_level == 'adaptive':
        # æ ¹æ®æ ·æœ¬æ¯”ä¾‹åŠ¨æ€è°ƒæ•´æƒé‡
        if sample_ratio is None:
            # é»˜è®¤æƒé‡
            result = torch.zeros_like(mask_tensor, dtype=torch.float32)
            result[mask_tensor == 2] = 0.3  # è½»å¾®æŸå
            result[mask_tensor == 3] = 0.6  # ä¸­ç­‰æŸå
            result[mask_tensor == 4] = 1.0  # ä¸¥é‡æŸå
        else:
            # æ ¹æ®æ ·æœ¬æ¯”ä¾‹è°ƒæ•´æƒé‡
            # å¦‚æœæŸåæ ·æœ¬æ¯”ä¾‹ä½ï¼Œå¢åŠ æƒé‡ä»¥å¹³è¡¡
            # å¦‚æœæŸåæ ·æœ¬æ¯”ä¾‹é«˜ï¼Œé™ä½æƒé‡ä»¥é¿å…è¿‡æ‹Ÿåˆ
            if sample_ratio < 0.1:  # æŸåæ ·æœ¬æ¯”ä¾‹å¾ˆä½
                result = torch.zeros_like(mask_tensor, dtype=torch.float32)
                result[mask_tensor == 2] = 0.5  # å¢åŠ è½»å¾®æŸåæƒé‡
                result[mask_tensor == 3] = 0.8  # å¢åŠ ä¸­ç­‰æŸåæƒé‡
                result[mask_tensor == 4] = 1.0  # ä¿æŒä¸¥é‡æŸåæƒé‡
            elif sample_ratio < 0.3:  # æŸåæ ·æœ¬æ¯”ä¾‹è¾ƒä½
                result = torch.zeros_like(mask_tensor, dtype=torch.float32)
                result[mask_tensor == 2] = 0.4  # é€‚åº¦å¢åŠ è½»å¾®æŸåæƒé‡
                result[mask_tensor == 3] = 0.7  # é€‚åº¦å¢åŠ ä¸­ç­‰æŸåæƒé‡
                result[mask_tensor == 4] = 1.0  # ä¿æŒä¸¥é‡æŸåæƒé‡
            elif sample_ratio > 0.7:  # æŸåæ ·æœ¬æ¯”ä¾‹å¾ˆé«˜
                result = torch.zeros_like(mask_tensor, dtype=torch.float32)
                result[mask_tensor == 2] = 0.2  # é™ä½è½»å¾®æŸåæƒé‡
                result[mask_tensor == 3] = 0.5  # é™ä½ä¸­ç­‰æŸåæƒé‡
                result[mask_tensor == 4] = 0.8  # é™ä½ä¸¥é‡æŸåæƒé‡
            else:  # æŸåæ ·æœ¬æ¯”ä¾‹é€‚ä¸­
                result = torch.zeros_like(mask_tensor, dtype=torch.float32)
                result[mask_tensor == 2] = 0.3  # æ ‡å‡†è½»å¾®æŸåæƒé‡
                result[mask_tensor == 3] = 0.6  # æ ‡å‡†ä¸­ç­‰æŸåæƒé‡
                result[mask_tensor == 4] = 1.0  # æ ‡å‡†ä¸¥é‡æŸåæƒé‡
        return result
    else:
        # é»˜è®¤è¡Œä¸º
        return (mask_tensor >= 2).float()

class XView2SegmentationDataset(Dataset):
    def __init__(self, images_dir, masks_dir, sim_feature_dict=None, transform=None, mask_transform=None, 
                 damage_sample_txts=None, damage_prob=0.7, is_training=True, damage_level='all',
                 show_warnings=True, skip_problematic_samples=False):
        """
        xView2æ•°æ®é›†åŠ è½½å™¨
        æ•°æ®é›†ç‰¹æ€§ï¼š
        - åŒ…å«ç¾å‰(pre-disaster)å’Œç¾å(post-disaster)å›¾åƒ
        - æ©ç ç”¨äºå®šä½å’ŒæŸä¼¤è¯„ä¼°ä»»åŠ¡
        - æ©ç æ˜¯å•é€šé“PNGå›¾åƒï¼Œå€¼å«ä¹‰ï¼š
            0: èƒŒæ™¯
            1: æœªæŸå
            2: æŸå
        
        Args:
            show_warnings: æ˜¯å¦æ˜¾ç¤ºæ•°æ®è´¨é‡è­¦å‘Š
            skip_problematic_samples: æ˜¯å¦è·³è¿‡æœ‰é—®é¢˜çš„æ ·æœ¬ï¼ˆç¾åå›¾åƒæ— æŸååŒºåŸŸï¼‰
        """
        self.images_dir = images_dir
        self.masks_dir = masks_dir
        self.sim_feature_dict = sim_feature_dict
        self.transform = transform
        self.mask_transform = mask_transform
        self.is_training = is_training
        self.damage_level = damage_level
        self.show_warnings = show_warnings
        self.skip_problematic_samples = skip_problematic_samples

        # æ™ºèƒ½æ–‡ä»¶ç­›é€‰ï¼šæ ¹æ®æ–‡ä»¶ååˆ¤æ–­æ˜¯å¦ä¸ºç¾åå›¾åƒ
        self.image_files = []
        problematic_files = []
        
        for f in os.listdir(images_dir):
            if f.lower().endswith(('.png', '.jpg', '.jpeg')):
                mask_name = os.path.splitext(f)[0] + "_target.png"
                mask_path = os.path.join(masks_dir, mask_name)
                if os.path.exists(mask_path):
                    # æ£€æŸ¥æ©ç å†…å®¹ï¼Œç¡®ä¿ç¾åå›¾åƒç¡®å®åŒ…å«æŸååŒºåŸŸ
                    try:
                        mask = Image.open(mask_path)
                        if mask.mode != 'L':
                            mask = mask.convert('L')
                        mask_np = np.array(mask)
                        has_damage = (mask_np >= 2).sum() > 0
                        
                        # å¦‚æœæ˜¯ç¾åå›¾åƒä½†æ²¡æœ‰æŸååŒºåŸŸï¼Œè®°å½•è­¦å‘Š
                        if 'post_disaster' in f and not has_damage:
                            if self.show_warnings:
                                print(f"è­¦å‘Šï¼šç¾åå›¾åƒ {f} æœªæ£€æµ‹åˆ°æŸååŒºåŸŸ")
                            problematic_files.append(f)
                            # å¦‚æœé€‰æ‹©è·³è¿‡æœ‰é—®é¢˜çš„æ ·æœ¬ï¼Œåˆ™ä¸æ·»åŠ åˆ°æ–‡ä»¶åˆ—è¡¨
                            if not self.skip_problematic_samples:
                                self.image_files.append(f)
                        else:
                            self.image_files.append(f)
                    except Exception as e:
                        if self.show_warnings:
                            print(f"è­¦å‘Šï¼šæ— æ³•è¯»å–æ©ç  {mask_path}: {e}")
                        continue
        
        if self.skip_problematic_samples and problematic_files:
            print(f"å·²è·³è¿‡ {len(problematic_files)} ä¸ªæœ‰é—®é¢˜çš„æ ·æœ¬")

        self.use_weighted_sampling = False
        if damage_sample_txts is not None:
            # damage_sample_txts: (has_damage_txt, no_damage_txt)
            has_damage_txt, no_damage_txt = damage_sample_txts
            with open(has_damage_txt) as f:
                self.has_damage = [line.strip() for line in f if line.strip()]
            with open(no_damage_txt) as f:
                self.no_damage = [line.strip() for line in f if line.strip()]
            self.use_weighted_sampling = True
            self.damage_prob = damage_prob
            self.length = len(self.has_damage) + len(self.no_damage)
        else:
            self.length = len(self.image_files)

    def __len__(self):
        return self.length

    def __getitem__(self, idx):
        try:
            if self.use_weighted_sampling:
                # å¢å¼ºé‡‡æ ·ï¼šä¿è¯85%æ¦‚ç‡é‡‡æ ·æœ‰æŸååŒºåŸŸ
                found_damage = False
                for _ in range(10):
                    if random.random() < 0.85 and len(self.has_damage) > 0:
                        mask_name = random.choice(self.has_damage)
                    else:
                        mask_name = random.choice(self.no_damage)
                    img_name = mask_name.replace('_target', '')
                    img_path = os.path.join(self.images_dir, img_name)
                    mask_path = os.path.join(self.masks_dir, mask_name)
                    if os.path.exists(img_path) and os.path.exists(mask_path):
                        mask = Image.open(mask_path)
                        if mask.mode != 'L':
                            mask = mask.convert('L')
                        mask = mask.resize((256, 256), resample=Image.Resampling.NEAREST)
                        mask_np = np.array(mask)
                        if (mask_np >= 2).sum() > 0:
                            found_damage = True
                            break
                if not found_damage:
                    # fallbackåˆ°æ— æŸåæ ·æœ¬
                    mask_name = random.choice(self.no_damage)
                    img_name = mask_name.replace('_target', '')
                    img_path = os.path.join(self.images_dir, img_name)
                    mask_path = os.path.join(self.masks_dir, mask_name)
                    mask = Image.open(mask_path)
                    if mask.mode != 'L':
                        mask = mask.convert('L')
                    mask = mask.resize((256, 256), resample=Image.Resampling.NEAREST)
                    mask_np = np.array(mask)
            else:
                img_name = self.image_files[idx]
                mask_name = os.path.splitext(img_name)[0] + "_target.png"
                img_path = os.path.join(self.images_dir, img_name)
                mask_path = os.path.join(self.masks_dir, mask_name)
                mask = Image.open(mask_path)
                if mask.mode != 'L':
                    mask = mask.convert('L')
                mask = mask.resize((256, 256), resample=Image.Resampling.NEAREST)
                mask_np = np.array(mask)  # uint8, 0/1/2/3/4
                mask_bin = (mask_np >= 2).astype(np.float32)  # 2/3/4ä¸ºæŸå
                mask_tensor = torch.from_numpy(mask_bin).unsqueeze(0)  # [1, H, W]
            image = Image.open(img_path).convert('RGB')
            if mask.mode != 'L':
                mask = mask.convert('L')
            mask_tensor = T.ToTensor()(mask)
            # ä½¿ç”¨å¢å¼ºçš„æ©ç å¤„ç†å‡½æ•°ï¼Œæ”¯æŒå¤šç§æŸåçº§åˆ«å¤„ç†æ–¹å¼
            damage_level = getattr(self, 'damage_level', 'all')  # é»˜è®¤ä½¿ç”¨'all'æ¨¡å¼
            mask_tensor = process_xview2_mask(mask_tensor, damage_level)
            
            # åªåœ¨ç¬¬ä¸€ä¸ªæ ·æœ¬æ—¶æ˜¾ç¤ºå¤„ç†ä¿¡æ¯ï¼Œé¿å…åˆ·å±
            if idx == 0:
                unique_values = torch.unique(mask_tensor)
                print(f"[æ©ç å¤„ç†] ä½¿ç”¨damage_level='{damage_level}', å¤„ç†åå”¯ä¸€å€¼: {unique_values.tolist()}")
            if self.transform:
                if 'albumentations' in str(type(self.transform)):
                    augmented = self.transform(image=image, mask=mask)
                    image = augmented['image']
                    mask = augmented['mask']
                else:
                    image = self.transform(image)
            if self.use_weighted_sampling and idx == 0:
                print(f"é‡‡æ ·æ©ç : {mask_name}, æŸååƒç´ æ•°: {(mask_np == 2).sum()}")
            if idx == 0 and not self.use_weighted_sampling:
                print(f"\nç¬¬ä¸€ä¸ªæ ·æœ¬è°ƒè¯•ä¿¡æ¯:")
                print(f"å›¾åƒè·¯å¾„: {img_path}")
                print(f"æ©ç è·¯å¾„: {mask_path}")
                print(f"åŸå§‹æ©ç å€¼èŒƒå›´: min={mask_np.min()}, max={mask_np.max()}")
                print(f"åŸå§‹æ©ç å”¯ä¸€å€¼: {np.unique(mask_np)}")
                print(f"å¤„ç†åæ©ç å½¢çŠ¶: {mask_tensor.shape}")
                print(f"å¤„ç†åæ©ç å”¯ä¸€å€¼: {torch.unique(mask_tensor)}")
            # åŠ è½½simç‰¹å¾
            sim_feat_tensor = torch.zeros(11)
            str_feats = ["", ""]
            if hasattr(self, 'sim_feature_dict') and self.sim_feature_dict is not None:
                key = os.path.basename(img_path)
                if key in self.sim_feature_dict:
                    sim_feats = self.sim_feature_dict[key]
                    sim_feat_tensor = torch.tensor(sim_feats[:11], dtype=torch.float32)
                    str_feats = sim_feats[11:]
            # è‡ªåŠ¨è·³è¿‡å…¨ä¸º0çš„æ©ç ï¼ˆæ— æŸååƒç´ ï¼‰
            return image, mask_tensor, sim_feat_tensor, str_feats
        except Exception as e:
            print(f"[è­¦å‘Š] åŠ è½½æ ·æœ¬ {idx} æ—¶å‡ºé”™: {e}, è‡ªåŠ¨è·³è¿‡ï¼Œå°è¯•ä¸‹ä¸€ä¸ªæ ·æœ¬ã€‚")
            new_idx = (idx + 1) % len(self)
            return self.__getitem__(new_idx)

class PatchSegmentationDataset(Dataset):
    """
    ç”¨äºåŠ è½½npyæ ¼å¼çš„patchå›¾åƒå’Œæ©ç ã€‚
    """
    def __init__(self, images_dir, masks_dir, transform=None):
        self.images_dir = images_dir
        self.masks_dir = masks_dir
        self.transform = transform
        self.image_files = sorted([f for f in os.listdir(images_dir) if f.endswith('.npy')])
        self.mask_files = sorted([f for f in os.listdir(masks_dir) if f.endswith('.npy')])
        assert len(self.image_files) == len(self.mask_files), "å›¾åƒå’Œæ©ç æ•°é‡ä¸ä¸€è‡´"
    
    def __len__(self):
        return len(self.image_files)
    
    def __getitem__(self, idx):
        img_path = os.path.join(self.images_dir, self.image_files[idx])
        mask_path = os.path.join(self.masks_dir, self.mask_files[idx])
        image = np.load(img_path)  # shape: (C, H, W) or (H, W, C)
        mask = np.load(mask_path)  # shape: (H, W)
        # ä¿è¯imageä¸ºfloat32, maskä¸ºfloat32
        if image.dtype != np.float32:
            image = image.astype(np.float32)
        if mask.dtype != np.float32:
            mask = mask.astype(np.float32)
        # å¦‚æœimageæ˜¯(H, W, C)ï¼Œè½¬ä¸º(C, H, W)
        if image.ndim == 3 and image.shape[0] != 1 and image.shape[0] != 3:
            image = np.transpose(image, (2, 0, 1))
        image_tensor = torch.from_numpy(image)
        # ç¡®ä¿maskç»´åº¦ä¸€è‡´
        mask_tensor = torch.from_numpy(mask)
        if mask_tensor.dim() == 2:
            mask_tensor = mask_tensor.unsqueeze(0)  # [H, W] -> [1, H, W]
        elif mask_tensor.dim() == 1:
            mask_tensor = mask_tensor.unsqueeze(0).unsqueeze(0)  # [H] -> [1, 1, H]
        if self.transform:
            image_tensor = self.transform(image_tensor)
        return image_tensor, mask_tensor


def get_patch_dataloaders(data_root="data/patch_dataset", batch_size=4, num_workers=8):
    """
    è·å–patchåˆ†å‰²ä»»åŠ¡çš„æ•°æ®åŠ è½½å™¨
    è¿”å›: (train_loader, val_loader)
    """
    train_images_dir = os.path.join(data_root, "train/images")
    train_masks_dir = os.path.join(data_root, "train/masks")
    val_images_dir = os.path.join(data_root, "val/images")
    val_masks_dir = os.path.join(data_root, "val/masks")

    train_dataset = PatchSegmentationDataset(train_images_dir, train_masks_dir)
    val_dataset = PatchSegmentationDataset(val_images_dir, val_masks_dir)

    train_loader = DataLoader(
        train_dataset,
        batch_size=batch_size,
        shuffle=True,
        num_workers=num_workers,
        pin_memory=True
    )
    val_loader = DataLoader(
        val_dataset,
        batch_size=batch_size,
        shuffle=False,
        num_workers=num_workers,
        pin_memory=True
    )
    return train_loader, val_loader

class MultiModalPatchSegmentationDataset(Dataset):
    """
    æ”¯æŒå¤šæ¨¡æ€patchåˆ†å‰²ï¼šæ¯ä¸ªpatchåŠ è½½patchã€æ©ç ã€åŸå›¾ä»¿çœŸç‰¹å¾ã€‚
    """
    def __init__(self, images_dir, masks_dir, patch_index_csv, sim_feature_dict, transform=None):
        self.images_dir = images_dir
        self.masks_dir = masks_dir
        self.transform = transform
        self.sim_feature_dict = sim_feature_dict
        # åŠ è½½patch->åŸå›¾ç´¢å¼•
        self.patch2img = {}
        with open(patch_index_csv, 'r') as f:
            next(f)
            for line in f:
                patch, img = line.strip().split(',')
                self.patch2img[patch] = img
        self.image_files = sorted([f for f in os.listdir(images_dir) if f.endswith('.npy')])
        self.mask_files = sorted([f for f in os.listdir(masks_dir) if f.endswith('.npy')])
        assert len(self.image_files) == len(self.mask_files), "å›¾åƒå’Œæ©ç æ•°é‡ä¸ä¸€è‡´"
    
    def __len__(self):
        return len(self.image_files)
    
    def __getitem__(self, idx):
        img_patch_name = self.image_files[idx]
        mask_patch_name = self.mask_files[idx]
        img_patch_path = os.path.join(self.images_dir, img_patch_name)
        mask_patch_path = os.path.join(self.masks_dir, mask_patch_name)
        image = np.load(img_patch_path)
        mask = np.load(mask_patch_path)
        if image.dtype != np.float32:
            image = image.astype(np.float32)
        if mask.dtype != np.float32:
            mask = mask.astype(np.float32)
        if image.ndim == 3 and image.shape[0] != 1 and image.shape[0] != 3:
            image = np.transpose(image, (2, 0, 1))
        image_tensor = torch.from_numpy(image)
        # ç¡®ä¿maskç»´åº¦ä¸€è‡´
        mask_tensor = torch.from_numpy(mask)
        if mask_tensor.dim() == 2:
            mask_tensor = mask_tensor.unsqueeze(0)  # [H, W] -> [1, H, W]
        elif mask_tensor.dim() == 1:
            mask_tensor = mask_tensor.unsqueeze(0).unsqueeze(0)  # [H] -> [1, 1, H]
        # æŸ¥æ‰¾åŸå›¾åå¹¶åŠ è½½ä»¿çœŸç‰¹å¾
        origin_img = self.patch2img.get(img_patch_name, None)
        if origin_img is not None and self.sim_feature_dict is not None:
            # å°è¯•å¤šç§è·¯å¾„æ ¼å¼æ¥åŒ¹é…sim_feature_dictä¸­çš„é”®
            possible_keys = [
                origin_img,  # åŸå§‹æ ¼å¼
                f"combined_dataset/images/train2017/{origin_img}",  # è®­ç»ƒé›†è·¯å¾„
                f"combined_dataset/images/val2017/{origin_img}",    # éªŒè¯é›†è·¯å¾„
                f"combined_dataset/images/test2017/{origin_img}",   # æµ‹è¯•é›†è·¯å¾„
                origin_img.replace('/', '\\'),  # Windowsè·¯å¾„æ ¼å¼
                f"combined_dataset\\images\\train2017\\{origin_img}",
                f"combined_dataset\\images\\val2017\\{origin_img}",
                f"combined_dataset\\images\\test2017\\{origin_img}"
            ]
            
            sim_feats = None
            for key in possible_keys:
                if key in self.sim_feature_dict:
                    sim_feats_tuple = self.sim_feature_dict[key]
                    # åªå–æ•°å€¼å‹ç‰¹å¾ï¼Œå¿½ç•¥å­—ç¬¦ä¸²ç‰¹å¾
                    sim_feats = sim_feats_tuple[0] if isinstance(sim_feats_tuple, tuple) else sim_feats_tuple
                    break
            
            if sim_feats is None:
                # å¦‚æœæ‰¾ä¸åˆ°åŒ¹é…çš„é”®ï¼Œä½¿ç”¨é›¶å‘é‡
                sim_feats = np.zeros(11, dtype=np.float32)
        
            # å¯¹sim_featsè¿›è¡Œå½’ä¸€åŒ–å¤„ç†ï¼Œé˜²æ­¢æ•°å€¼è¿‡å¤§
            sim_feats = np.array(sim_feats, dtype=np.float32)
            if np.std(sim_feats) > 0:
                sim_feats = (sim_feats - np.mean(sim_feats)) / np.std(sim_feats)
        
            sim_feats_tensor = torch.tensor(sim_feats, dtype=torch.float32)
        else:
            sim_feats = np.zeros(11, dtype=np.float32)
            # å¯¹sim_featsè¿›è¡Œå½’ä¸€åŒ–å¤„ç†ï¼Œé˜²æ­¢æ•°å€¼è¿‡å¤§
            sim_feats = np.array(sim_feats, dtype=np.float32)
            if np.std(sim_feats) > 0:
                sim_feats = (sim_feats - np.mean(sim_feats)) / np.std(sim_feats)
        
            sim_feats_tensor = torch.tensor(sim_feats, dtype=torch.float32)
        if self.transform:
            image_tensor = self.transform(image_tensor)
        return image_tensor, mask_tensor, sim_feats_tensor


# æ•°æ®å¢å¼ºåŒ…è£…ç±»
class AugmentedDataset(Dataset):
    def __init__(self, dataset, augment_fn):
        self.dataset = dataset
        self.augment_fn = augment_fn
        
    def __len__(self):
        return len(self.dataset)
        
    def __getitem__(self, idx):
        img, mask, sim_feat = self.dataset[idx]
        
        # æ£€æŸ¥è¾“å…¥æ•°æ®
        if torch.isnan(img).any() or torch.isinf(img).any():
            print(f"[è­¦å‘Š] AugmentedDataset: è¾“å…¥å›¾åƒåŒ…å«NaN/Inf!")
            return self.__getitem__((idx + 1) % len(self))  # å°è¯•ä¸‹ä¸€ä¸ªæ ·æœ¬
        
        # æ£€æŸ¥å›¾åƒæ•°æ®ç±»å‹å’ŒèŒƒå›´
        if img.dtype == torch.float32:
            # å¦‚æœå·²ç»æ˜¯float32ï¼Œéœ€è¦åå½’ä¸€åŒ–åˆ°0-255èŒƒå›´
            if img.min() >= -3 and img.max() <= 3:
                # åå½’ä¸€åŒ–ï¼šä»ImageNetæ ‡å‡†åŒ–èŒƒå›´è½¬æ¢å›0-255
                img = img * torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1) + torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1)
                img = img * 255.0
                img = torch.clamp(img, 0, 255)
        
        # è½¬æ¢ä¸ºNumPyæ•°ç»„ (H, W, C) - ä¿æŒfloatç±»å‹ç”¨äºå½’ä¸€åŒ–
        img = img.permute(1, 2, 0).numpy()
        if img.dtype != np.float32:
            img = img.astype(np.float32)
        
        # ç¡®ä¿maskç»´åº¦ä¸€è‡´ - ä¿®å¤ç»´åº¦é—®é¢˜
        if mask.dim() == 3:
            mask = mask.squeeze(0)  # ä» [1, H, W] è½¬ä¸º [H, W]
        elif mask.dim() == 1:
            mask = mask.unsqueeze(0)  # ä» [H] è½¬ä¸º [1, H]
        mask = mask.numpy()
        
        # æ£€æŸ¥è½¬æ¢åçš„æ•°æ®
        if np.isnan(img).any() or np.isinf(img).any():
            print(f"[è­¦å‘Š] AugmentedDataset: è½¬æ¢åå›¾åƒåŒ…å«NaN/Inf!")
            return self.__getitem__((idx + 1) % len(self))  # å°è¯•ä¸‹ä¸€ä¸ªæ ·æœ¬
        
        # åº”ç”¨å¢å¼º
        try:
            img, mask = self.augment_fn(img, mask)
        except Exception as e:
            print(f"[è­¦å‘Š] AugmentedDataset: æ•°æ®å¢å¼ºå¤±è´¥: {e}")
            return self.__getitem__((idx + 1) % len(self))  # å°è¯•ä¸‹ä¸€ä¸ªæ ·æœ¬
        
        # ç¡®ä¿è¿”å›çš„maskæ˜¯3D tensor [1, H, W]
        if isinstance(mask, np.ndarray):
            mask = torch.from_numpy(mask)
        if mask.dim() == 2:
            mask = mask.unsqueeze(0)  # [H, W] -> [1, H, W]
        
        # è°ƒè¯•ï¼šæ£€æŸ¥å¢å¼ºåçš„æ•°æ®
        if torch.isnan(img).any() or torch.isinf(img).any():
            print(f"[è­¦å‘Š] AugmentedDataset: å¢å¼ºåå›¾åƒåŒ…å«NaN/Inf!")
            print(f"img shape: {img.shape}, dtype: {img.dtype}")
            print(f"img range: [{img.min().item():.4f}, {img.max().item():.4f}]")
            # è¿”å›ä¸€ä¸ªå®‰å…¨çš„æ›¿ä»£å›¾åƒ
            img = torch.zeros_like(img)
        
        return img, mask, sim_feat

def get_multimodal_patch_dataloaders(data_root="data/patch_dataset", 
                                    sim_feature_csv="data/sim_features.csv", 
                                    batch_size=4, 
                                    num_workers=4,
                                    damage_boost=5,
                                    normal_ratio=0.05,
                                    preload_to_memory=False,
                                    preload_to_gpu=False,
                                    device='cuda',
                                    pin_memory=True,
                                    persistent_workers=True,
                                    prefetch_factor=8,
                                    drop_last=True):
    """
    è·å–å¤šæ¨¡æ€patchæ•°æ®åŠ è½½å™¨ï¼Œæ”¯æŒå†…å­˜å’ŒGPUé¢„åŠ è½½
    
    Args:
        data_root: æ•°æ®æ ¹ç›®å½•
        sim_feature_csv: ä»¿çœŸç‰¹å¾CSVæ–‡ä»¶è·¯å¾„
        batch_size: æ‰¹æ¬¡å¤§å°
        num_workers: æ•°æ®åŠ è½½è¿›ç¨‹æ•°
        damage_boost: æŸåæ ·æœ¬å¢å¼ºå€æ•°
        normal_ratio: æ­£å¸¸æ ·æœ¬æ¯”ä¾‹
        preload_to_memory: æ˜¯å¦é¢„åŠ è½½åˆ°å†…å­˜
        preload_to_gpu: æ˜¯å¦é¢„åŠ è½½åˆ°GPUæ˜¾å­˜
        device: è®¾å¤‡ç±»å‹
    """
    
    sim_feature_dict = load_sim_features(sim_feature_csv)
    
    # åˆ›å»ºåŸºç¡€æ•°æ®é›†
    train_dataset = MultiModalPatchSegmentationDataset(
        os.path.join(data_root, "train/images"),
        os.path.join(data_root, "train/masks"),
        os.path.join(data_root, "patch_index_train.csv"),
        sim_feature_dict,
        transform=None  # ç¨ååº”ç”¨å¢å¼º
    )
    
    val_dataset = MultiModalPatchSegmentationDataset(
        os.path.join(data_root, "val/images"),
        os.path.join(data_root, "val/masks"),
        os.path.join(data_root, "patch_index_val.csv"),
        sim_feature_dict,
        transform=None
    )
    
    # åº”ç”¨è¿‡é‡‡æ · - å¯ç”¨æ•°æ®å¢å¼º
    train_aug = AdvancedAugmentation(is_training=True)
    val_aug = AdvancedAugmentation(is_training=False)  # éªŒè¯é›†ä¹Ÿéœ€è¦å½’ä¸€åŒ–
    
    train_dataset = DamageAwareDataset(train_dataset, damage_boost=damage_boost, normal_ratio=normal_ratio)
    
    # å¯ç”¨å¢å¼º
    train_dataset = AugmentedDataset(train_dataset, train_aug)
    val_dataset = AugmentedDataset(val_dataset, val_aug)
    
    # æ ¹æ®é¢„åŠ è½½é€‰é¡¹å¤„ç†æ•°æ®é›†
    # if preload_to_gpu:
    #     print("ğŸš€ å¯ç”¨GPUæ˜¾å­˜é¢„åŠ è½½...")
    #     train_dataset = GPUPreloadedDataset(train_dataset, device=device, batch_size=batch_size)
    #     val_dataset = GPUPreloadedDataset(val_dataset, device=device, batch_size=batch_size)
    #     # GPUé¢„åŠ è½½åï¼Œä¸éœ€è¦DataLoaderçš„pin_memoryå’Œnum_workers
    #     train_loader = DataLoader(
    #         train_dataset,
    #         batch_size=batch_size,
    #         shuffle=True,
    #         num_workers=4,  # GPUé¢„åŠ è½½ä¸éœ€è¦å¤šè¿›ç¨‹
    #         pin_memory=False,  # æ•°æ®å·²åœ¨GPU
    #         drop_last=True,
    #         collate_fn=custom_collate_fn
    #     )
    #     val_loader = DataLoader(
    #         val_dataset,
    #         batch_size=batch_size,
    #         shuffle=False,
    #         num_workers=4,
    #         pin_memory=False,
    #         collate_fn=custom_collate_fn
    #     )
    # elif preload_to_memory:
    #     print("ğŸ’¾ å¯ç”¨å†…å­˜é¢„åŠ è½½...")
    #     train_dataset = MemoryCachedDataset(train_dataset, device='cpu')
    #     val_dataset = MemoryCachedDataset(val_dataset, device='cpu')
    #     # å†…å­˜é¢„åŠ è½½åï¼Œä»ç„¶éœ€è¦pin_memoryæ¥å¿«é€Ÿä¼ è¾“åˆ°GPU
    #     train_loader = DataLoader(
    #         train_dataset,
    #         batch_size=batch_size,
    #         shuffle=True,
    #         num_workers=num_workers,
    #         pin_memory=pin_memory,
    #         persistent_workers=persistent_workers,
    #         prefetch_factor=prefetch_factor,
    #         drop_last=drop_last,
    #         collate_fn=custom_collate_fn
    #     )
    #     val_loader = DataLoader(
    #         val_dataset,
    #         batch_size=batch_size,
    #         shuffle=False,
    #         num_workers=num_workers,
    #         pin_memory=pin_memory,
    #         persistent_workers=persistent_workers,
    #         prefetch_factor=prefetch_factor,
    #         drop_last=drop_last,
    #         collate_fn=custom_collate_fn
    #     )
    # else:
    print("ğŸ“ ä½¿ç”¨æ ‡å‡†æ•°æ®åŠ è½½...")
    # æ ‡å‡†æ•°æ®åŠ è½½
    train_loader = DataLoader(
        train_dataset,
        batch_size=batch_size,
        shuffle=True,
        num_workers=num_workers,
        pin_memory=True,
        drop_last=True,
        collate_fn=optimized_collate  # ä½¿ç”¨ä»data_loaderå¯¼å…¥çš„ä¼˜åŒ–collateå‡½æ•°
    )
    val_loader = DataLoader(
        val_dataset,
        batch_size=batch_size,
        shuffle=False,
        num_workers=num_workers,
        pin_memory=True,
        collate_fn=optimized_collate  # ä½¿ç”¨ä»data_loaderå¯¼å…¥çš„ä¼˜åŒ–collateå‡½æ•°
    )
    return train_loader, val_loader

# å¤šæ¨¡æ€åˆ†å‰²æ¨¡å‹ - å¢å¼ºç‰ˆæœ¬
class EnhancedDeepLab(nn.Module):
    def __init__(self, in_channels=3, num_classes=1, sim_feat_dim=11):
        super().__init__()
        # ä½¿ç”¨æ›´å¼ºå¤§çš„ç¼–ç å™¨
        self.deeplab = smp.DeepLabV3Plus(
            encoder_name="resnext101_32x8d",  # æ›´å¼ºçš„é¢„è®­ç»ƒç¼–ç å™¨
            encoder_weights="imagenet",       # ä½¿ç”¨é¢„è®­ç»ƒæƒé‡
            in_channels=in_channels,
            classes=num_classes,
            activation=None
        )
        
        # æ”¹è¿›çš„ç‰¹å¾èåˆæ¨¡å—
        self.sim_fusion = nn.Sequential(
            nn.Linear(sim_feat_dim, 256),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(256, 2048),  # ä¿®æ”¹ä¸º2048ä»¥åŒ¹é…encoderè¾“å‡ºç»´åº¦
            nn.ReLU(),
            nn.Dropout(0.3)
        )
        
        # æ³¨æ„åŠ›é—¨æ§æœºåˆ¶
        self.attention_gate = nn.Sequential(
            nn.Conv2d(2048 + 2048, 512, kernel_size=1),  # ä¿®æ”¹è¾“å…¥é€šé“ä¸º2048+2048=4096
            nn.ReLU(),
            nn.Conv2d(512, 2048, kernel_size=1),  # ä¿®æ”¹è¾“å‡ºé€šé“ä¸º2048ä»¥åŒ¹é…xçš„ç»´åº¦
            nn.Sigmoid()
        )
        
        # åˆå§‹åŒ–æƒé‡
        self._init_weights()
    
    def _init_weights(self):
        for m in self.sim_fusion.modules():
            if isinstance(m, nn.Linear):
                nn.init.xavier_uniform_(m.weight)
                if m.bias is not None:
                    nn.init.zeros_(m.bias)
        
        for m in self.attention_gate.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')
                if m.bias is not None:
                    nn.init.zeros_(m.bias)
    
    def forward(self, img, sim_feat):
        # è¾“å…¥æ£€æŸ¥
        if torch.isnan(img).any() or torch.isinf(img).any():
            print(f"[è­¦å‘Š] æ¨¡å‹è¾“å…¥å›¾åƒåŒ…å«NaN/Inf!")
            img = torch.nan_to_num(img, nan=0.0, posinf=1.0, neginf=-1.0)
        
        if torch.isnan(sim_feat).any() or torch.isinf(sim_feat).any():
            print(f"[è­¦å‘Š] æ¨¡å‹è¾“å…¥simç‰¹å¾åŒ…å«NaN/Inf!")
            sim_feat = torch.nan_to_num(sim_feat, nan=0.0, posinf=1.0, neginf=-1.0)
        
        # æå–å›¾åƒç‰¹å¾
        features = self.deeplab.encoder(img)
        x = features[-1]
        B, C, H, W = x.shape
        
        # å¤„ç†ä»¿çœŸç‰¹å¾
        sim_proj = self.sim_fusion(sim_feat)
        sim_proj = sim_proj.view(B, -1, 1, 1)
        sim_proj = sim_proj.expand(-1, -1, H, W)
        
        # æ³¨æ„åŠ›èåˆ
        combined = torch.cat([x, sim_proj], dim=1)
        attention = self.attention_gate(combined)
        # ç¡®ä¿sim_projç»´åº¦ä¸xåŒ¹é…
        sim_proj = F.interpolate(sim_proj, size=x.shape[2:], mode='nearest')
        fused = x * attention + sim_proj * (1 - attention)
        
        # æ£€æŸ¥èåˆåçš„ç‰¹å¾
        if torch.isnan(fused).any() or torch.isinf(fused).any():
            print(f"[è­¦å‘Š] èåˆç‰¹å¾åŒ…å«NaN/Infï¼Œä½¿ç”¨åŸå§‹ç‰¹å¾!")
            fused = x
        
        # è§£ç å™¨
        features = list(features)
        features[-1] = fused
        out = self.deeplab.decoder(features)
        out = self.deeplab.segmentation_head(out)
        
        # æ£€æŸ¥æœ€ç»ˆè¾“å‡º
        if torch.isnan(out).any() or torch.isinf(out).any():
            print(f"[è­¦å‘Š] æ¨¡å‹è¾“å‡ºåŒ…å«NaN/Infï¼Œè¿”å›é›¶å¼ é‡!")
            out = torch.zeros_like(out)
        
        return out

class MultiClassDeepLab(nn.Module):
    """
    æ”¯æŒå¤šç±»åˆ«åˆ†ç±»çš„DeepLabæ¨¡å‹
    è¾“å‡º5ä¸ªç±»åˆ«ï¼šèƒŒæ™¯(0)ã€æœªæŸå(1)ã€è½»å¾®æŸå(2)ã€ä¸­ç­‰æŸå(3)ã€ä¸¥é‡æŸå(4)
    """
    def __init__(self, in_channels=3, num_classes=5, sim_feat_dim=11):
        super().__init__()
        # ä½¿ç”¨æ›´å¼ºå¤§çš„ç¼–ç å™¨
        self.deeplab = smp.DeepLabV3Plus(
            encoder_name="resnext101_32x8d",  # æ›´å¼ºçš„é¢„è®­ç»ƒç¼–ç å™¨
            encoder_weights="imagenet",       # ä½¿ç”¨é¢„è®­ç»ƒæƒé‡
            in_channels=in_channels,
            classes=num_classes,
            activation=None
        )
        
        # æ”¹è¿›çš„ç‰¹å¾èåˆæ¨¡å—
        self.sim_fusion = nn.Sequential(
            nn.Linear(sim_feat_dim, 256),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(256, 2048),  # ä¿®æ”¹ä¸º2048ä»¥åŒ¹é…encoderè¾“å‡ºç»´åº¦
            nn.ReLU(),
            nn.Dropout(0.3)
        )
        
        # æ³¨æ„åŠ›é—¨æ§æœºåˆ¶
        self.attention_gate = nn.Sequential(
            nn.Conv2d(2048 + 2048, 512, kernel_size=1),  # ä¿®æ”¹è¾“å…¥é€šé“ä¸º2048+2048=4096
            nn.ReLU(),
            nn.Conv2d(512, 2048, kernel_size=1),  # ä¿®æ”¹è¾“å‡ºé€šé“ä¸º2048ä»¥åŒ¹é…xçš„ç»´åº¦
            nn.Sigmoid()
        )
        
        # åˆå§‹åŒ–æƒé‡
        self._init_weights()
    
    def _init_weights(self):
        for m in self.sim_fusion.modules():
            if isinstance(m, nn.Linear):
                nn.init.xavier_uniform_(m.weight)
                if m.bias is not None:
                    nn.init.zeros_(m.bias)
        
        for m in self.attention_gate.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')
                if m.bias is not None:
                    nn.init.zeros_(m.bias)
    
    def forward(self, img, sim_feat):
        # è¾“å…¥æ£€æŸ¥
        if torch.isnan(img).any() or torch.isinf(img).any():
            print(f"[è­¦å‘Š] æ¨¡å‹è¾“å…¥å›¾åƒåŒ…å«NaN/Inf!")
            img = torch.nan_to_num(img, nan=0.0, posinf=1.0, neginf=-1.0)
        
        if torch.isnan(sim_feat).any() or torch.isinf(sim_feat).any():
            print(f"[è­¦å‘Š] æ¨¡å‹è¾“å…¥simç‰¹å¾åŒ…å«NaN/Inf!")
            sim_feat = torch.nan_to_num(sim_feat, nan=0.0, posinf=1.0, neginf=-1.0)
        
        # æå–å›¾åƒç‰¹å¾
        features = self.deeplab.encoder(img)
        x = features[-1]
        B, C, H, W = x.shape
        
        # å¤„ç†ä»¿çœŸç‰¹å¾
        sim_proj = self.sim_fusion(sim_feat)
        sim_proj = sim_proj.view(B, -1, 1, 1)
        sim_proj = sim_proj.expand(-1, -1, H, W)
        
        # æ³¨æ„åŠ›èåˆ
        combined = torch.cat([x, sim_proj], dim=1)
        attention = self.attention_gate(combined)
        # ç¡®ä¿sim_projç»´åº¦ä¸xåŒ¹é…
        sim_proj = F.interpolate(sim_proj, size=x.shape[2:], mode='nearest')
        fused = x * attention + sim_proj * (1 - attention)
        
        # æ£€æŸ¥èåˆåçš„ç‰¹å¾
        if torch.isnan(fused).any() or torch.isinf(fused).any():
            print(f"[è­¦å‘Š] èåˆç‰¹å¾åŒ…å«NaN/Infï¼Œä½¿ç”¨åŸå§‹ç‰¹å¾!")
            fused = x
        
        # è§£ç å™¨
        features = list(features)
        features[-1] = fused
        out = self.deeplab.decoder(features)
        out = self.deeplab.segmentation_head(out)
        
        # æ£€æŸ¥æœ€ç»ˆè¾“å‡º
        if torch.isnan(out).any() or torch.isinf(out).any():
            print(f"[è­¦å‘Š] æ¨¡å‹è¾“å‡ºåŒ…å«NaN/Infï¼Œè¿”å›é›¶å¼ é‡!")
            out = torch.zeros_like(out)
        
        return out

# Dice Loss
class DiceLoss(nn.Module):
    def __init__(self, smooth=1e-5):
        super().__init__()
        self.smooth = smooth
    
    def forward(self, inputs, targets):
        # æ•°æ®åˆæ³•æ€§æ£€æŸ¥
        if torch.isnan(inputs).any():
            print("[è­¦å‘Š] inputsä¸­æœ‰nanå€¼")
            inputs = torch.nan_to_num(inputs, nan=0.0)
        if torch.isnan(targets).any():
            print("[è­¦å‘Š] targetsä¸­æœ‰nanå€¼")
            targets = torch.nan_to_num(targets, nan=0.0)
        
        inputs = torch.sigmoid(inputs)
        inputs = inputs.view(-1)
        targets = targets.view(-1)
        
        # ç¡®ä¿è¾“å…¥åœ¨åˆç†èŒƒå›´å†…
        inputs = torch.clamp(inputs, 0, 1)
        targets = torch.clamp(targets, 0, 1)
        
        intersection = (inputs * targets).sum()
        dice = (2. * intersection + self.smooth) / (inputs.sum() + targets.sum() + self.smooth)
        
        # æ£€æŸ¥è®¡ç®—ç»“æœ
        if torch.isnan(dice) or torch.isinf(dice):
            print(f"[è­¦å‘Š] Diceè®¡ç®—å¼‚å¸¸: dice={dice}, intersection={intersection}, inputs_sum={inputs.sum()}, targets_sum={targets.sum()}")
            return torch.tensor(1.0, device=inputs.device, requires_grad=True)
        
        return 1 - dice

# è¯„ä¼°æŒ‡æ ‡
def iou_score(outputs, masks, smooth=1e-5):
    # æ•°æ®åˆæ³•æ€§æ£€æŸ¥
    if torch.isnan(outputs).any():
        outputs = torch.nan_to_num(outputs, nan=0.0)
    if torch.isnan(masks).any():
        masks = torch.nan_to_num(masks, nan=0.0)
    
    preds = (torch.sigmoid(outputs) > 0.5).float()
    masks = masks.float()
    intersection = (preds * masks).sum()
    union = (preds + masks).sum() - intersection
    iou = (intersection + smooth) / (union + smooth)
    
    # æ£€æŸ¥è®¡ç®—ç»“æœ
    if torch.isnan(iou) or torch.isinf(iou):
        return torch.tensor(0.0, device=outputs.device)
    
    return iou

def dice_score(outputs, masks, smooth=1e-5):
    # æ•°æ®åˆæ³•æ€§æ£€æŸ¥
    if torch.isnan(outputs).any():
        outputs = torch.nan_to_num(outputs, nan=0.0)
    if torch.isnan(masks).any():
        masks = torch.nan_to_num(masks, nan=0.0)
    
    outputs = torch.sigmoid(outputs)
    outputs = (outputs > 0.5).float()
    masks = masks.float()
    intersection = (outputs * masks).sum()
    dice = (2. * intersection + smooth) / (outputs.sum() + masks.sum() + smooth)
    
    # æ£€æŸ¥è®¡ç®—ç»“æœ
    if torch.isnan(dice) or torch.isinf(dice):
        return torch.tensor(0.0, device=outputs.device)
    
    return dice

# ç»„åˆæŸå¤±å‡½æ•°ï¼šDice + Focal + Boundary
class HybridLoss(nn.Module):
    def __init__(self, alpha=0.7, gamma=2.0):
        super().__init__()
        self.dice = DiceLoss()
        self.alpha = alpha
        self.gamma = gamma
        
    def focal_loss(self, inputs, targets):
        BCE_loss = F.binary_cross_entropy_with_logits(inputs, targets, reduction='none')
        pt = torch.exp(-BCE_loss)
        F_loss = (1 - pt)**self.gamma * BCE_loss
        return F_loss.mean()
    
    def boundary_loss(self, pred, target):
        # ä¿è¯è¾“å…¥ä¸º4ç»´ [B, 1, H, W]
        if pred.dim() == 3:
            pred = pred.unsqueeze(1)
        if target.dim() == 3:
            target = target.unsqueeze(1)
        # è®¡ç®—è¾¹ç•Œæ¢¯åº¦å·®å¼‚
        pred_grad_x = torch.abs(pred[:, :, :, 1:] - pred[:, :, :, :-1])
        pred_grad_y = torch.abs(pred[:, :, 1:, :] - pred[:, :, :-1, :])
        target_grad_x = torch.abs(target[:, :, :, 1:] - target[:, :, :, :-1])
        target_grad_y = torch.abs(target[:, :, 1:, :] - target[:, :, :-1, :])
        loss_x = F.l1_loss(pred_grad_x, target_grad_x)
        loss_y = F.l1_loss(pred_grad_y, target_grad_y)
        return (loss_x + loss_y) * 0.5
    
    def forward(self, inputs, targets):
        dice = self.dice(inputs, targets)
        focal = self.focal_loss(inputs, targets)
        
        # è¾¹ç•ŒæŸå¤±éœ€è¦sigmoidè¾“å‡º
        with torch.no_grad():
            pred_sigmoid = torch.sigmoid(inputs)
        
        boundary = self.boundary_loss(pred_sigmoid, targets)
        
        # åŠ æƒç»„åˆ
        return self.alpha * dice + (1 - self.alpha) * focal + 0.3 * boundary

# è¾¹ç•Œæ„ŸçŸ¥è®­ç»ƒ (Boundary-Aware Training)
class BoundaryAwareLoss(nn.Module):
    def __init__(self, main_loss_fn, alpha=0.3, beta=0.2, kernel_size=3, use_sobel=True):
        super().__init__()
        self.main_loss = main_loss_fn
        self.alpha = alpha
        self.beta = beta
        self.use_sobel = use_sobel
        self.kernel = torch.ones(1, 1, kernel_size, kernel_size).float() / (kernel_size**2)
        
        # Sobelç®—å­ - æ›´ç²¾ç¡®çš„è¾¹ç•Œæ£€æµ‹
        if use_sobel:
            self.sobel_x = torch.tensor([[-1, 0, 1], [-2, 0, 2], [-1, 0, 1]], 
                                      dtype=torch.float32).view(1, 1, 3, 3)
            self.sobel_y = torch.tensor([[-1, -2, -1], [0, 0, 0], [1, 2, 1]], 
                                      dtype=torch.float32).view(1, 1, 3, 3)
    
    def to(self, device):
        self.kernel = self.kernel.to(device)
        if self.use_sobel:
            self.sobel_x = self.sobel_x.to(device)
            self.sobel_y = self.sobel_y.to(device)
        return super().to(device)
    
    def compute_boundary_sobel(self, mask):
        # ä¿è¯sobelç®—å­å’Œmaskåœ¨åŒä¸€è®¾å¤‡
        sobel_x = self.sobel_x.to(mask.device)
        sobel_y = self.sobel_y.to(mask.device)
        
        # è°ƒè¯•ä¿¡æ¯
        #print(f"[è°ƒè¯•] è¾“å…¥maskå½¢çŠ¶: {mask.shape}")
        
        # å¤„ç†å¤šé€šé“è¾“å…¥ï¼šå¦‚æœmaskæœ‰å¤šä¸ªé€šé“ï¼Œå–å¹³å‡å€¼
        if mask.size(1) > 1:
            mask = mask.mean(dim=1, keepdim=True)
            #print(f"[è°ƒè¯•] å¤šé€šé“å¤„ç†åmaskå½¢çŠ¶: {mask.shape}")
        
        # ç¡®ä¿maskæ˜¯4ç»´å¼ é‡ [B, 1, H, W]
        if mask.dim() == 3:
            mask = mask.unsqueeze(1)
            #print(f"[è°ƒè¯•] 3ç»´è½¬4ç»´åmaskå½¢çŠ¶: {mask.shape}")
        elif mask.dim() == 2:
            mask = mask.unsqueeze(0).unsqueeze(0)
            #print(f"[è°ƒè¯•] 2ç»´è½¬4ç»´åmaskå½¢çŠ¶: {mask.shape}")
        
        # ç¡®ä¿é€šé“æ•°ä¸º1
        if mask.size(1) != 1:
            mask = mask.mean(dim=1, keepdim=True)
            #print(f"[è°ƒè¯•] é€šé“æ•°è°ƒæ•´åmaskå½¢çŠ¶: {mask.shape}")
        
        #print(f"[è°ƒè¯•] æœ€ç»ˆmaskå½¢çŠ¶: {mask.shape}")
        #print(f"[è°ƒè¯•] sobel_xå½¢çŠ¶: {sobel_x.shape}")
        
        grad_x = F.conv2d(mask, sobel_x, padding=1)
        grad_y = F.conv2d(mask, sobel_y, padding=1)
        grad = torch.sqrt(grad_x ** 2 + grad_y ** 2)
        
        #print(f"[è°ƒè¯•] è¾“å‡ºgradå½¢çŠ¶: {grad.shape}")
        return grad
    
    def compute_boundary_conv(self, mask):
        """ä½¿ç”¨å·ç§¯æ ¸è®¡ç®—è¾¹ç•Œ - åŸå§‹æ–¹æ³•"""
        # å¤„ç†å¤šé€šé“è¾“å…¥ï¼šå¦‚æœmaskæœ‰å¤šä¸ªé€šé“ï¼Œå–å¹³å‡å€¼
        if mask.size(1) > 1:
            mask = mask.mean(dim=1, keepdim=True)
        
        # ç¡®ä¿maskæ˜¯4ç»´å¼ é‡ [B, 1, H, W]
        if mask.dim() == 3:
            mask = mask.unsqueeze(1)
        elif mask.dim() == 2:
            mask = mask.unsqueeze(0).unsqueeze(0)
        
        # ç¡®ä¿é€šé“æ•°ä¸º1
        if mask.size(1) != 1:
            mask = mask.mean(dim=1, keepdim=True)
        
        # è®¡ç®—è¾¹ç•Œå›¾
        avg_pooled = F.conv2d(mask, self.kernel, padding=self.kernel.size(-1)//2)
        boundary = torch.abs(mask - avg_pooled)
        return boundary
    
    def compute_boundary_multi_scale(self, mask, scales=[1, 2, 4]):
        """å¤šå°ºåº¦è¾¹ç•Œæ£€æµ‹"""
        boundaries = []
        original_size = mask.shape[-2:]  # ä¿å­˜åŸå§‹å°ºå¯¸
        
        for scale in scales:
            if scale == 1:
                scaled_mask = mask
            else:
                # ä¸‹é‡‡æ · - ä½¿ç”¨æ›´ç²¾ç¡®çš„å°ºå¯¸è®¡ç®—
                h, w = mask.shape[-2:]
                new_h, new_w = h // scale, w // scale
                scaled_mask = F.adaptive_avg_pool2d(mask, (new_h, new_w))
            
            # è®¡ç®—è¾¹ç•Œ
            if self.use_sobel:
                boundary = self.compute_boundary_sobel(scaled_mask)
            else:
                boundary = self.compute_boundary_conv(scaled_mask)
            
            # ç¡®ä¿æ‰€æœ‰è¾¹ç•Œéƒ½ä¸Šé‡‡æ ·åˆ°åŸå§‹å°ºå¯¸
            if boundary.shape[-2:] != original_size:
                boundary = F.interpolate(boundary, size=original_size, 
                                       mode='bilinear', align_corners=True)
            
            boundaries.append(boundary)
        
        # ç¡®ä¿æ‰€æœ‰è¾¹ç•Œå¼ é‡å°ºå¯¸ä¸€è‡´åå†stack
        if len(boundaries) > 0:
            # å¼ºåˆ¶æ‰€æœ‰è¾¹ç•Œå¼ é‡éƒ½è°ƒæ•´åˆ°åŸå§‹å°ºå¯¸
            target_size = original_size
            for i, boundary in enumerate(boundaries):
                if boundary.shape[-2:] != target_size:
                    print(f"è°ƒæ•´è¾¹ç•Œå¼ é‡{i}å°ºå¯¸ä»{boundary.shape}åˆ°ç›®æ ‡å°ºå¯¸{target_size}")
                    boundary = F.interpolate(boundary, size=target_size, 
                                           mode='bilinear', align_corners=True)
                    boundaries[i] = boundary
            
            # éªŒè¯æ‰€æœ‰è¾¹ç•Œå¼ é‡å°ºå¯¸ä¸€è‡´
            first_shape = boundaries[0].shape
            for i, boundary in enumerate(boundaries):
                if boundary.shape != first_shape:
                    print(f"é”™è¯¯ï¼šè¾¹ç•Œå¼ é‡{i}å°ºå¯¸ä»ä¸åŒ¹é…ï¼ŒæœŸæœ›{first_shape}ï¼Œå®é™…{boundary.shape}")
                    # æœ€åä¸€æ¬¡å¼ºåˆ¶è°ƒæ•´
                    boundary = F.interpolate(boundary, size=first_shape[-2:], 
                                           mode='bilinear', align_corners=True)
                    boundaries[i] = boundary
            
            # å¤šå°ºåº¦è¾¹ç•Œèåˆ
            multi_scale_boundary = torch.mean(torch.stack(boundaries), dim=0)
        else:
            # å¦‚æœæ²¡æœ‰è¾¹ç•Œï¼Œè¿”å›é›¶å¼ é‡
            multi_scale_boundary = torch.zeros_like(mask)
        
        return multi_scale_boundary
    
    def compute_boundary(self, mask):
        """ä¸»è¾¹ç•Œè®¡ç®—æ–¹æ³•"""
        if self.use_sobel:
            # ä½¿ç”¨å¤šå°ºåº¦è¾¹ç•Œæ£€æµ‹
            return self.compute_boundary_multi_scale(mask)
        else:
            return self.compute_boundary_conv(mask)
    
    def forward(self, pred, target):
        # ä¸»æŸå¤±
        main_loss = self.main_loss(pred, target)
        
        # é¢„æµ‹è¾¹ç•Œ
        pred_prob = torch.sigmoid(pred)
        pred_boundary = self.compute_boundary(pred_prob)
        
        # çœŸå®è¾¹ç•Œ
        target_boundary = self.compute_boundary(target)
        
        # è¾¹ç•ŒæŸå¤±
        boundary_loss = F.l1_loss(pred_boundary, target_boundary)
        
        # è¾¹ç•ŒIoUæŸå¤±
        pred_boundary_bin = (pred_boundary > 0.1).float()
        target_boundary_bin = (target_boundary > 0.1).float()
        boundary_iou = 1 - iou_score(pred_boundary_bin, target_boundary_bin)
        
        # ç»„åˆæŸå¤±
        total_loss = main_loss + self.alpha * boundary_loss + self.beta * boundary_iou
        
        return total_loss

# è‡ªé€‚åº”éš¾æ ·æœ¬æŒ–æ˜ (Adaptive Hard Example Mining)
class AdaptiveMiner:
    def __init__(self, base_loss_fn, start_epoch=5, max_ratio=0.3, warmup_epochs=10):
        self.base_loss = base_loss_fn
        self.start_epoch = start_epoch
        self.max_ratio = max_ratio
        self.warmup_epochs = warmup_epochs
        self.current_epoch = 0
    
    def update_epoch(self, epoch):
        self.current_epoch = epoch
    
    def __call__(self, pred, target):
        # è®¡ç®—æ¯ä¸ªåƒç´ çš„æŸå¤±
        pixel_loss = F.binary_cross_entropy_with_logits(pred, target, reduction='none')
        
        # å‰å‡ ä¸ªepochä½¿ç”¨æ ‡å‡†æŸå¤±
        if self.current_epoch < self.start_epoch:
            return pixel_loss.mean()
        
        # è®¡ç®—éš¾æ ·æœ¬æ¯”ä¾‹ï¼ˆéšepochçº¿æ€§å¢åŠ ï¼‰
        ratio = min(self.max_ratio, 
                  self.max_ratio * (self.current_epoch - self.start_epoch) / self.warmup_epochs)
        
        # è¯†åˆ«éš¾æ ·æœ¬
        with torch.no_grad():
            # é¢„æµ‹æ¦‚ç‡
            pred_prob = torch.sigmoid(pred)
            # é¢„æµ‹é”™è¯¯åŒºåŸŸ
            incorrect_mask = (pred_prob > 0.5).float() != target
            # è¾¹ç•ŒåŒºåŸŸ
            boundary = self.compute_boundary(target)
            boundary_mask = boundary > 0.1
            
            # éš¾æ ·æœ¬æ©ç 
            hard_mask = incorrect_mask | boundary_mask
            
        # è®¡ç®—éš¾æ ·æœ¬æŸå¤±
        hard_loss = (pixel_loss * hard_mask).sum() / (hard_mask.sum() + 1e-5)
        
        # è®¡ç®—æ˜“æ ·æœ¬æŸå¤±
        easy_loss = (pixel_loss * ~hard_mask).sum() / ((~hard_mask).sum() + 1e-5)
        
        # åŠ æƒç»„åˆ
        return ratio * hard_loss + (1 - ratio) * easy_loss
    
    def compute_boundary(self, mask, kernel_size=3):
        # å¤„ç†å¤šé€šé“è¾“å…¥ï¼šå¦‚æœmaskæœ‰å¤šä¸ªé€šé“ï¼Œå–å¹³å‡å€¼
        if mask.size(1) > 1:
            mask = mask.mean(dim=1, keepdim=True)
        
        # ç¡®ä¿maskæ˜¯4ç»´å¼ é‡ [B, 1, H, W]
        if mask.dim() == 3:
            mask = mask.unsqueeze(1)
        elif mask.dim() == 2:
            mask = mask.unsqueeze(0).unsqueeze(0)
        
        # ç¡®ä¿é€šé“æ•°ä¸º1
        if mask.size(1) != 1:
            mask = mask.mean(dim=1, keepdim=True)
        
        kernel = torch.ones(1, 1, kernel_size, kernel_size).to(mask.device) / (kernel_size**2)
        avg_pooled = F.conv2d(mask, kernel, padding=kernel_size//2)
        boundary = torch.abs(mask - avg_pooled)
        return boundary

# å¢å¼ºçš„æŸå¤±å‡½æ•°ç»„åˆ
class EnhancedLoss(nn.Module):
    def __init__(self, alpha=0.4, beta=0.3, gamma=0.2, delta=0.1):
        super().__init__()
        self.alpha = alpha  # Diceæƒé‡
        self.beta = beta    # Focalæƒé‡
        self.gamma = gamma  # Boundaryæƒé‡
        self.delta = delta  # IoUæƒé‡
        
        self.dice_loss = DiceLoss()
        
    def focal_loss(self, inputs, targets, gamma=2.0):
        BCE_loss = F.binary_cross_entropy_with_logits(inputs, targets, reduction='none')
        pt = torch.exp(-BCE_loss)
        F_loss = (1 - pt)**gamma * BCE_loss
        return F_loss.mean()
    
    def boundary_loss(self, pred, target):
        # è®¡ç®—è¾¹ç•ŒæŸå¤±
        pred_prob = torch.sigmoid(pred)
        
        # å¤„ç†å¤šé€šé“è¾“å…¥ï¼šå¦‚æœpred_probæœ‰å¤šä¸ªé€šé“ï¼Œå–å¹³å‡å€¼
        if pred_prob.size(1) > 1:
            pred_prob = pred_prob.mean(dim=1, keepdim=True)
        
        # å¤„ç†å¤šé€šé“è¾“å…¥ï¼šå¦‚æœtargetæœ‰å¤šä¸ªé€šé“ï¼Œå–å¹³å‡å€¼
        if target.size(1) > 1:
            target = target.mean(dim=1, keepdim=True)
        
        # ç¡®ä¿æ˜¯4ç»´å¼ é‡ [B, 1, H, W]
        if pred_prob.dim() == 3:
            pred_prob = pred_prob.unsqueeze(1)
        elif pred_prob.dim() == 2:
            pred_prob = pred_prob.unsqueeze(0).unsqueeze(0)
            
        if target.dim() == 3:
            target = target.unsqueeze(1)
        elif target.dim() == 2:
            target = target.unsqueeze(0).unsqueeze(0)
        
        # ä½¿ç”¨Sobelç®—å­è®¡ç®—æ¢¯åº¦
        sobel_x = torch.tensor([[-1, 0, 1], [-2, 0, 2], [-1, 0, 1]], dtype=torch.float32).view(1, 1, 3, 3).to(pred.device)
        sobel_y = torch.tensor([[-1, -2, -1], [0, 0, 0], [1, 2, 1]], dtype=torch.float32).view(1, 1, 3, 3).to(pred.device)
        
        pred_grad_x = F.conv2d(pred_prob, sobel_x, padding=1)
        pred_grad_y = F.conv2d(pred_prob, sobel_y, padding=1)
        pred_grad = torch.sqrt(pred_grad_x**2 + pred_grad_y**2)
        
        target_grad_x = F.conv2d(target, sobel_x, padding=1)
        target_grad_y = F.conv2d(target, sobel_y, padding=1)
        target_grad = torch.sqrt(target_grad_x**2 + target_grad_y**2)
        
        return F.l1_loss(pred_grad, target_grad)
    
    def forward(self, inputs, targets):
        dice = self.dice_loss(inputs, targets)
        focal = self.focal_loss(inputs, targets)
        boundary = self.boundary_loss(inputs, targets)
        
        # IoUæŸå¤±
        iou = 1 - iou_score(inputs, targets)
        
        # ç»„åˆæŸå¤±
        total_loss = (self.alpha * dice + 
                     self.beta * focal + 
                     self.gamma * boundary + 
                     self.delta * iou)
        
        return total_loss

class MultiClassLoss(nn.Module):
    """
    å¤šç±»åˆ«åˆ†ç±»æŸå¤±å‡½æ•°
    æ”¯æŒ5ä¸ªç±»åˆ«ï¼šèƒŒæ™¯(0)ã€æœªæŸå(1)ã€è½»å¾®æŸå(2)ã€ä¸­ç­‰æŸå(3)ã€ä¸¥é‡æŸå(4)
    """
    def __init__(self, alpha=0.4, beta=0.3, gamma=0.2, delta=0.1, class_weights=None):
        super().__init__()
        self.alpha = alpha  # äº¤å‰ç†µæŸå¤±æƒé‡
        self.beta = beta    # DiceæŸå¤±æƒé‡
        self.gamma = gamma  # FocalæŸå¤±æƒé‡
        self.delta = delta  # è¾¹ç•ŒæŸå¤±æƒé‡
        
        # ç±»åˆ«æƒé‡ï¼šæ›´å…³æ³¨æŸåç±»åˆ«
        if class_weights is None:
            # èƒŒæ™¯æƒé‡è¾ƒä½ï¼ŒæŸåç±»åˆ«æƒé‡è¾ƒé«˜
            self.class_weights = torch.tensor([0.5, 1.0, 2.0, 3.0, 4.0], dtype=torch.float32)
        else:
            self.class_weights = class_weights
        
        # æŸå¤±å‡½æ•°
        self.ce_loss = nn.CrossEntropyLoss(weight=self.class_weights)
        self.focal_loss = self._focal_loss
        self.dice_loss = self._dice_loss
        self.boundary_loss = self._boundary_loss
    
    def _focal_loss(self, inputs, targets, gamma=2.0, alpha=0.25):
        """Focal Loss for multi-class classification"""
        ce_loss = F.cross_entropy(inputs, targets, reduction='none')
        pt = torch.exp(-ce_loss)
        focal_loss = alpha * (1-pt)**gamma * ce_loss
        return focal_loss.mean()
    
    def _dice_loss(self, inputs, targets, smooth=1e-5):
        """Dice Loss for multi-class classification"""
        # å°†è¾“å…¥è½¬æ¢ä¸ºæ¦‚ç‡åˆ†å¸ƒ
        probs = F.softmax(inputs, dim=1)
        
        # å°†ç›®æ ‡è½¬æ¢ä¸ºone-hotç¼–ç 
        num_classes = inputs.size(1)
        targets_one_hot = F.one_hot(targets, num_classes).float()
        targets_one_hot = targets_one_hot.permute(0, 3, 1, 2)  # [B, C, H, W]
        
        # è®¡ç®—æ¯ä¸ªç±»åˆ«çš„DiceæŸå¤±
        dice_loss = 0
        for i in range(num_classes):
            intersection = (probs[:, i] * targets_one_hot[:, i]).sum()
            union = probs[:, i].sum() + targets_one_hot[:, i].sum()
            dice_loss += 1 - (2 * intersection + smooth) / (union + smooth)
        
        return dice_loss / num_classes
    
    def _boundary_loss(self, pred, target):
        """è¾¹ç•ŒæŸå¤±ï¼šå…³æ³¨æŸååŒºåŸŸçš„è¾¹ç•Œ"""
        # å°†è¾“å…¥è½¬æ¢ä¸ºæ¦‚ç‡åˆ†å¸ƒ
        probs = F.softmax(pred, dim=1)
        
        # åˆ›å»ºæŸååŒºåŸŸçš„maskï¼ˆç±»åˆ«2,3,4ï¼‰
        damage_mask = (target >= 2).float()
        
        # è®¡ç®—è¾¹ç•Œ
        kernel = torch.ones(1, 1, 3, 3, device=pred.device)
        boundary = F.conv2d(damage_mask.unsqueeze(1), kernel, padding=1)
        boundary = ((boundary > 0) & (boundary < 9)).float()
        
        # åœ¨è¾¹ç•ŒåŒºåŸŸè®¡ç®—æŸå¤±
        boundary_loss = 0
        for i in range(2, 5):  # åªè€ƒè™‘æŸåç±»åˆ«
            class_mask = (target == i).float()
            boundary_class = boundary * class_mask
            if boundary_class.sum() > 0:
                prob_class = probs[:, i]
                boundary_loss += F.binary_cross_entropy(prob_class, boundary_class)
        
        return boundary_loss
    
    def forward(self, inputs, targets):
        """
        Args:
            inputs: [B, C, H, W] æ¨¡å‹è¾“å‡º
            targets: [B, H, W] ç›®æ ‡æ ‡ç­¾ (0-4)
        """
        # æ•°æ®åˆæ³•æ€§æ£€æŸ¥
        if torch.isnan(inputs).any():
            print(f"[è­¦å‘Š] æŸå¤±å‡½æ•°è¾“å…¥åŒ…å«NaN!")
            inputs = torch.nan_to_num(inputs, nan=0.0)
        
        if torch.isnan(targets).any():
            print(f"[è­¦å‘Š] æŸå¤±å‡½æ•°ç›®æ ‡åŒ…å«NaN!")
            targets = torch.nan_to_num(targets, nan=0.0)
        
        # ç¡®ä¿ç›®æ ‡åœ¨æœ‰æ•ˆèŒƒå›´å†…
        targets = torch.clamp(targets, 0, 4).long()
        
        # è®¡ç®—å„ç§æŸå¤±
        ce_loss = self.ce_loss(inputs, targets)
        focal_loss = self.focal_loss(inputs, targets)
        dice_loss = self.dice_loss(inputs, targets)
        boundary_loss = self.boundary_loss(inputs, targets)
        
        # ç»„åˆæŸå¤±
        total_loss = (self.alpha * ce_loss + 
                     self.beta * dice_loss + 
                     self.gamma * focal_loss + 
                     self.delta * boundary_loss)
        
        return total_loss

def multi_class_iou_score(outputs, masks, num_classes=5, smooth=1e-5):
    """
    å¤šç±»åˆ«åˆ†ç±»çš„IoUè¯„åˆ†
    Args:
        outputs: [B, C, H, W] æ¨¡å‹è¾“å‡º
        masks: [B, H, W] ç›®æ ‡æ ‡ç­¾
        num_classes: ç±»åˆ«æ•°é‡
        smooth: å¹³æ»‘å› å­
    """
    # æ•°æ®åˆæ³•æ€§æ£€æŸ¥
    if torch.isnan(outputs).any():
        print(f"[è­¦å‘Š] IoUè®¡ç®—è¾“å…¥åŒ…å«NaN!")
        outputs = torch.nan_to_num(outputs, nan=0.0)
    
    if torch.isnan(masks).any():
        print(f"[è­¦å‘Š] IoUè®¡ç®—ç›®æ ‡åŒ…å«NaN!")
        masks = torch.nan_to_num(masks, nan=0.0)
    
    # è·å–é¢„æµ‹ç±»åˆ«
    pred = torch.argmax(outputs, dim=1)
    
    # è®¡ç®—æ¯ä¸ªç±»åˆ«çš„IoU
    iou_scores = []
    for i in range(num_classes):
        pred_mask = (pred == i).float()
        target_mask = (masks == i).float()
        
        intersection = (pred_mask * target_mask).sum()
        union = pred_mask.sum() + target_mask.sum() - intersection
        
        iou = (intersection + smooth) / (union + smooth)
        iou_scores.append(iou.item())
    
    # è¿”å›å¹³å‡IoUå’Œæ¯ä¸ªç±»åˆ«çš„IoU
    mean_iou = sum(iou_scores) / len(iou_scores)
    return mean_iou, iou_scores

def multi_class_dice_score(outputs, masks, num_classes=5, smooth=1e-5):
    """
    å¤šç±»åˆ«åˆ†ç±»çš„Diceè¯„åˆ†
    Args:
        outputs: [B, C, H, W] æ¨¡å‹è¾“å‡º
        masks: [B, H, W] ç›®æ ‡æ ‡ç­¾
        num_classes: ç±»åˆ«æ•°é‡
        smooth: å¹³æ»‘å› å­
    """
    # æ•°æ®åˆæ³•æ€§æ£€æŸ¥
    if torch.isnan(outputs).any():
        print(f"[è­¦å‘Š] Diceè®¡ç®—è¾“å…¥åŒ…å«NaN!")
        outputs = torch.nan_to_num(outputs, nan=0.0)
    
    if torch.isnan(masks).any():
        print(f"[è­¦å‘Š] Diceè®¡ç®—ç›®æ ‡åŒ…å«NaN!")
        masks = torch.nan_to_num(masks, nan=0.0)
    
    # è·å–é¢„æµ‹ç±»åˆ«
    pred = torch.argmax(outputs, dim=1)
    
    # è®¡ç®—æ¯ä¸ªç±»åˆ«çš„Diceåˆ†æ•°
    dice_scores = []
    for i in range(num_classes):
        pred_mask = (pred == i).float()
        target_mask = (masks == i).float()
        
        intersection = (pred_mask * target_mask).sum()
        dice = (2 * intersection + smooth) / (pred_mask.sum() + target_mask.sum() + smooth)
        dice_scores.append(dice.item())
    
    # è¿”å›å¹³å‡Diceå’Œæ¯ä¸ªç±»åˆ«çš„Dice
    mean_dice = sum(dice_scores) / len(dice_scores)
    return mean_dice, dice_scores

def evaluate_multi_class_performance(outputs, masks, num_classes=5):
    """
    å¤šç±»åˆ«åˆ†ç±»æ€§èƒ½è¯„ä¼°
    Args:
        outputs: [B, C, H, W] æ¨¡å‹è¾“å‡º
        masks: [B, H, W] ç›®æ ‡æ ‡ç­¾
        num_classes: ç±»åˆ«æ•°é‡
    Returns:
        dict: åŒ…å«å„ç§è¯„ä¼°æŒ‡æ ‡
    """
    # è·å–é¢„æµ‹ç±»åˆ«
    pred = torch.argmax(outputs, dim=1)
    
    # è®¡ç®—IoUå’ŒDice
    mean_iou, class_ious = multi_class_iou_score(outputs, masks, num_classes)
    mean_dice, class_dices = multi_class_dice_score(outputs, masks, num_classes)
    
    # è®¡ç®—å‡†ç¡®ç‡
    accuracy = (pred == masks).float().mean().item()
    
    # è®¡ç®—æŸåç±»åˆ«çš„æ€§èƒ½ï¼ˆç±»åˆ«2,3,4ï¼‰
    damage_pred = (pred >= 2).float()
    damage_target = (masks >= 2).float()
    damage_iou = ((damage_pred * damage_target).sum() + 1e-5) / (damage_pred.sum() + damage_target.sum() - (damage_pred * damage_target).sum() + 1e-5)
    
    # è®¡ç®—ä¸¥é‡æŸåçš„æ€§èƒ½ï¼ˆç±»åˆ«4ï¼‰
    severe_pred = (pred == 4).float()
    severe_target = (masks == 4).float()
    severe_iou = ((severe_pred * severe_target).sum() + 1e-5) / (severe_pred.sum() + severe_target.sum() - (severe_pred * severe_target).sum() + 1e-5)
    
    return {
        'mean_iou': mean_iou,
        'mean_dice': mean_dice,
        'accuracy': accuracy,
        'damage_iou': damage_iou.item(),
        'severe_damage_iou': severe_iou.item(),
        'class_ious': class_ious,
        'class_dices': class_dices,
        'class_names': ['èƒŒæ™¯', 'æœªæŸå', 'è½»å¾®æŸå', 'ä¸­ç­‰æŸå', 'ä¸¥é‡æŸå']
    }

def main():
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    
    # æ¨¡å‹é€‰æ‹©é…ç½®
    use_multiscale = False  # è®¾ç½®ä¸ºTrueä½¿ç”¨å¤šå°ºåº¦æ¨¡å‹ï¼ŒFalseä½¿ç”¨åŸå§‹æ¨¡å‹
    use_landslide_detector = True  # è®¾ç½®ä¸ºTrueä½¿ç”¨LandslideDetectoræ¨¡å‹
    use_ensemble = True  # è®¾ç½®ä¸ºTrueä½¿ç”¨æ¨¡å‹é›†æˆè®­ç»ƒ
    
    print("="*60)
    print("ğŸš€ æ¨¡å‹é…ç½®")
    print("="*60)
    if use_ensemble:
        print("ğŸ¯ ä½¿ç”¨æ¨¡å‹é›†æˆè®­ç»ƒ (DeepLab + LandslideDetector)")
    elif use_landslide_detector:
        print("ğŸ“Š ä½¿ç”¨ä¼˜åŒ–çš„LandslideDetectoræ¨¡å‹")
    elif use_multiscale:
        print("ğŸ“Š ä½¿ç”¨å¤šå°ºåº¦ç‰¹å¾èåˆæ¨¡å‹ (MultiScaleDeepLab)")
        print("âš ï¸  æ³¨æ„ï¼šå¤šå°ºåº¦æ¨¡å‹ä¸ç°æœ‰æ£€æŸ¥ç‚¹ä¸å…¼å®¹ï¼Œå°†ä»å¤´å¼€å§‹è®­ç»ƒ")
    else:
        print("ğŸ“Š ä½¿ç”¨åŸå§‹å¢å¼ºæ¨¡å‹ (EnhancedDeepLab)")
    print("="*60)
    
    # æ•°æ®åŠ è½½é…ç½®
    print("ğŸš€ æ•°æ®åŠ è½½é…ç½®")
    print("="*60)
    
    # æ ¹æ®å¯ç”¨å†…å­˜é€‰æ‹©é¢„åŠ è½½ç­–ç•¥
    import psutil
    total_memory_gb = psutil.virtual_memory().total / (1024**3)
    gpu_memory_gb = torch.cuda.get_device_properties(0).total_memory / (1024**3) if torch.cuda.is_available() else 0
    
    print(f"ç³»ç»Ÿå†…å­˜: {total_memory_gb:.1f} GB")
    print(f"GPUæ˜¾å­˜: {gpu_memory_gb:.1f} GB")
    
    # === å¼ºåˆ¶åŠ¨æ€åŠ è½½+pin memory ===
    preload_to_memory = False
    preload_to_gpu = False
    print("ğŸ“ [å¼ºåˆ¶] ä½¿ç”¨æ ‡å‡†æ•°æ®åŠ è½½ (åŠ¨æ€åŠ è½½+pin memory)")
    
    # è‡ªåŠ¨é€‰æ‹©é¢„åŠ è½½ç­–ç•¥
    if total_memory_gb >= 32:  # 32GBä»¥ä¸Šå†…å­˜
        print("ğŸ’¾ æ£€æµ‹åˆ°å……è¶³å†…å­˜ï¼Œå¯ç”¨å†…å­˜é¢„åŠ è½½")
        preload_to_memory = True
    elif gpu_memory_gb >= 16:  # 16GBä»¥ä¸Šæ˜¾å­˜
        print("ğŸš€ æ£€æµ‹åˆ°å……è¶³æ˜¾å­˜ï¼Œå¯ç”¨GPUé¢„åŠ è½½")
        preload_to_gpu = True
    else:
        print("ğŸ“ ä½¿ç”¨æ ‡å‡†æ•°æ®åŠ è½½")
    
    # åŠ¨æ€åŠ è½½é…ç½®
    batch_size = 32  # é»˜è®¤batch sizeï¼Œå¯æ ¹æ®GPUæ˜¾å­˜è°ƒæ•´
    num_workers = 12  # ä½¿ç”¨12ä¸ªå·¥ä½œè¿›ç¨‹
    pin_memory = True  # å¯ç”¨pin memory
    
    # æ ¹æ®GPUæ˜¾å­˜è‡ªåŠ¨è°ƒæ•´batch size
    if torch.cuda.is_available():
        total_memory = torch.cuda.get_device_properties(0).total_memory
        free_memory = torch.cuda.memory_reserved(0)
        if total_memory >= 16 * 1024**3:  # 16GBä»¥ä¸Šæ˜¾å­˜
            batch_size =64
        elif total_memory >= 8 * 1024**3:  # 8GBæ˜¾å­˜
            batch_size = 32
        else:  # å°äº8GBæ˜¾å­˜
            batch_size = 16
        print(f"è‡ªåŠ¨è®¾ç½®batch sizeä¸º: {batch_size} (æ ¹æ®GPUæ˜¾å­˜ {total_memory/1024**3:.1f}GB)")
    persistent_workers = True  # ä¿æŒå·¥ä½œè¿›ç¨‹å­˜æ´»
    prefetch_factor = 2  # é¢„å–2ä¸ªbatch
    drop_last = True  # ä¸¢å¼ƒæœ€åä¸€ä¸ªä¸å®Œæ•´çš„batch
    
    print(f"æ‰¹æ¬¡å¤§å°: {batch_size}")
    print(f"å·¥ä½œè¿›ç¨‹æ•°: {num_workers}")
    print("="*60)
    
    train_loader, val_loader = get_multimodal_patch_dataloaders(
        data_root="data/patch_dataset",
        sim_feature_csv="data/sim_features.csv",
        batch_size=batch_size,
        num_workers=num_workers,
        pin_memory=pin_memory,
        persistent_workers=persistent_workers,
        prefetch_factor=prefetch_factor,
        drop_last=drop_last,
        damage_boost=5,
        normal_ratio=0.05,
        preload_to_memory=preload_to_memory,
        preload_to_gpu=preload_to_gpu,
        device=device
    )
    
    # æ ¹æ®é…ç½®é€‰æ‹©æ¨¡å‹
    if use_ensemble:
        # åˆ›å»ºæ¨¡å‹é›†æˆ
        deeplab_model = EnhancedDeepLab(in_channels=3, num_classes=1, sim_feat_dim=11).to(device)
        landslide_model = create_segmentation_landslide_model(
            num_classes=1,
            use_attention=True,
            use_fpn=True,
            use_dynamic_attention=True,
            use_multi_scale=True
        ).to(device)
        
        # åˆ›å»ºåŒæ¨¡å‹é›†æˆ
        ensemble_model = DualModelEnsemble(deeplab_model, landslide_model, fusion_weight=0.5).to(device)
        # ä¸ºäº†å…¼å®¹æ€§ï¼Œå°†ensemble_modelèµ‹å€¼ç»™model
        model = ensemble_model
        print("âœ… åŒæ¨¡å‹é›†æˆåˆ›å»ºæˆåŠŸ (DeepLab + åˆ†å‰²ç‰ˆLandslideDetector)")
        
    elif use_landslide_detector:
        model = create_segmentation_landslide_model(
            num_classes=1,
            use_attention=True,
            use_fpn=True,
            use_dynamic_attention=True,
            use_multi_scale=True
        ).to(device)
        print("âœ… åˆ†å‰²ç‰ˆLandslideDetectoræ¨¡å‹åˆ›å»ºæˆåŠŸ")
        
    elif use_multiscale:
        model = MultiScaleDeepLab(in_channels=3, num_classes=1, sim_feat_dim=11).to(device)
        print("âœ… å¤šå°ºåº¦ç‰¹å¾èåˆæ¨¡å‹åˆ›å»ºæˆåŠŸ")
    else:
        model = EnhancedDeepLab(in_channels=3, num_classes=1, sim_feat_dim=11).to(device)
        print("âœ… åŸå§‹å¢å¼ºæ¨¡å‹åˆ›å»ºæˆåŠŸ")
    
    # ä½¿ç”¨å¢å¼ºçš„æŸå¤±å‡½æ•° - ä¿®å¤ï¼šä½¿ç”¨æ›´ç¨³å®šçš„æŸå¤±å‡½æ•°
    if use_ensemble:
        # å¯¹äºé›†æˆæ¨¡å‹ï¼Œä½¿ç”¨æ›´ç®€å•çš„æŸå¤±å‡½æ•°ä»¥é¿å…æ•°å€¼ä¸ç¨³å®š
        criterion = DiceLoss()
        print("âœ… ä½¿ç”¨DiceLossä»¥ç¡®ä¿æ•°å€¼ç¨³å®šæ€§")
    else:
        base_criterion = HybridLoss()
        criterion = BoundaryAwareLoss(base_criterion, alpha=0.3, beta=0.2)
        
        # åˆ›å»ºè‡ªé€‚åº”éš¾æ ·æœ¬æŒ–æ˜å™¨
        adaptive_miner = AdaptiveMiner(base_criterion, start_epoch=5, max_ratio=0.3, warmup_epochs=10)
        print("âœ… è¾¹ç•Œæ„ŸçŸ¥æŸå¤±å’Œè‡ªé€‚åº”éš¾æ ·æœ¬æŒ–æ˜å·²å¯ç”¨")
    
    # === æ©ç å¤„ç†å’Œåå¤„ç†é…ç½® ===
    print("ğŸ”§ æ©ç å¤„ç†å’Œåå¤„ç†é…ç½®")
    print("="*60)
    
    # æ©ç å¤„ç†é…ç½®
    damage_level = 'all'  # å¯é€‰: 'all', 'light', 'binary', 'multi'
    print(f"ğŸ“Š æ©ç å¤„ç†æ–¹å¼: {damage_level}")
    print("  - all: æ‰€æœ‰æŸåçº§åˆ«(2,3,4)éƒ½æ ‡è®°ä¸º1")
    print("  - light: è½»å¾®æŸå(2)æ ‡è®°ä¸º0.3ï¼Œä¸­ç­‰(3)æ ‡è®°ä¸º0.6ï¼Œä¸¥é‡(4)æ ‡è®°ä¸º1.0")
    print("  - binary: è½»å¾®(2)æ ‡è®°ä¸º0ï¼Œä¸­ç­‰å’Œä¸¥é‡(3,4)æ ‡è®°ä¸º1")
    print("  - multi: è½»å¾®(2)æ ‡è®°ä¸º1ï¼Œä¸­ç­‰(3)æ ‡è®°ä¸º2ï¼Œä¸¥é‡(4)æ ‡è®°ä¸º3")
    
    # åå¤„ç†é…ç½®
    enable_postprocess = True  # æ˜¯å¦å¯ç”¨åå¤„ç†
    postprocess_min_area = 100  # æœ€å°è¿é€šåŸŸé¢ç§¯
    postprocess_merge_distance = 10  # åˆå¹¶è·ç¦»
    print(f"ğŸ”§ åå¤„ç†é…ç½®:")
    print(f"  - å¯ç”¨åå¤„ç†: {enable_postprocess}")
    print(f"  - æœ€å°è¿é€šåŸŸé¢ç§¯: {postprocess_min_area}")
    print(f"  - åˆå¹¶è·ç¦»: {postprocess_merge_distance}")
    print("="*60)
    
    # åˆ†å±‚å­¦ä¹ ç‡é…ç½®
    encoder_params = []
    decoder_params = []
    fusion_params = []
    
    for name, param in model.named_parameters():
        if 'encoder' in name:
            encoder_params.append(param)
        elif 'decoder' in name or 'segmentation_head' in name:
            decoder_params.append(param)
        else:
            fusion_params.append(param)
    
    optimizer = optim.AdamW([
        {'params': encoder_params, 'lr': 1e-5},    # é¢„è®­ç»ƒç¼–ç å™¨ç”¨è¾ƒå°LR
        {'params': decoder_params, 'lr': 1e-4},
        {'params': fusion_params, 'lr': 5e-4}
    ], weight_decay=1e-4)
    
    # ä¸´æ—¶è°ƒåº¦å™¨ï¼Œç¨åæ›´æ–°
    scheduler = optim.lr_scheduler.OneCycleLR(
        optimizer,
        max_lr=[5e-4, 5e-3, 2.5e-3],  # å¯¹åº”encoder/decoder/fusionçš„max_lr
        total_steps=20 * len(train_loader),  # ä¸´æ—¶å€¼
        pct_start=0.3,
        anneal_strategy='linear',
        final_div_factor=10000,
        three_phase=False
    )
    
    # åˆ›å»ºæ··åˆç²¾åº¦è®­ç»ƒå™¨
    print("ğŸš€ åˆå§‹åŒ–CPU-GPUååŒè®­ç»ƒå™¨...")
    hybrid_trainer = HybridPrecisionTrainer(
        model, 
        optimizer,
        criterion,
        device,
        num_cpu_workers=8,
        num_async_workers=8
    )
    print("âœ… CPU-GPUååŒè®­ç»ƒå™¨åˆå§‹åŒ–å®Œæˆ")
    print(f"é…ç½®å‚æ•°: 12ä¸ªCPUå·¥ä½œçº¿ç¨‹, 8ä¸ªå¼‚æ­¥æ•°æ®é¢„å¤„ç†çº¿ç¨‹ ")

    # === æ™ºèƒ½æ¨¡å‹æ¢å¤ç›¸å…³ ===
    checkpoint_path = "models/checkpoint.pth"
    best_model_path = "models/best_multimodal_patch_model.pth"
    
    # æ™ºèƒ½æ¢å¤è®­ç»ƒçŠ¶æ€ - ä¼˜å…ˆä»æœ€ä½³æ¨¡å‹æ–‡ä»¶æ¢å¤
    start_epoch = 1
    best_val_iou = 0.0
    iou_log = []
    
    # é¦–å…ˆå°è¯•ä»æœ€ä½³æ¨¡å‹æ–‡ä»¶æ¢å¤ï¼ˆå¦‚æœå®ƒæ˜¯æ£€æŸ¥ç‚¹æ ¼å¼ï¼‰
    if os.path.exists(best_model_path):
        try:
            print(f"ğŸ” æ£€æŸ¥æœ€ä½³æ¨¡å‹æ–‡ä»¶: {best_model_path}")
            best_model_data = torch.load(best_model_path, map_location=device)
            
            # æ£€æŸ¥æ˜¯å¦æ˜¯æœ€ä½³æ¨¡å‹æ–‡ä»¶åŒ…å«æ£€æŸ¥ç‚¹ä¿¡æ¯
            if isinstance(best_model_data, dict) and 'model_state_dict' in best_model_data:
                print("âœ… å‘ç°æœ€ä½³æ¨¡å‹æ–‡ä»¶åŒ…å«å®Œæ•´è®­ç»ƒçŠ¶æ€")
                print(f"   - Epoch: {best_model_data.get('epoch', 'N/A')}")
                print(f"   - æœ€ä½³IoU: {best_model_data.get('best_val_iou', 'N/A'):.4f}")
                print(f"   - åŒ…å«ä¼˜åŒ–å™¨çŠ¶æ€: {'optimizer_state_dict' in best_model_data}")
                print(f"   - åŒ…å«è°ƒåº¦å™¨çŠ¶æ€: {'scheduler_state_dict' in best_model_data}")
                
                # ä»æœ€ä½³æ¨¡å‹æ–‡ä»¶æ¢å¤
                if use_ensemble:
                    ensemble_model.load_state_dict(best_model_data['model_state_dict'])
                else:
                    model.load_state_dict(best_model_data['model_state_dict'])
                
                if 'optimizer_state_dict' in best_model_data:
                    optimizer.load_state_dict(best_model_data['optimizer_state_dict'])
                if 'scheduler_state_dict' in best_model_data:
                    scheduler.load_state_dict(best_model_data['scheduler_state_dict'])
                if 'scaler_state_dict' in best_model_data:
                    hybrid_trainer.scaler.load_state_dict(best_model_data['scaler_state_dict'])
                
                start_epoch = best_model_data.get('epoch', 0) + 1
                best_val_iou = best_model_data.get('best_val_iou', 0.0)
                iou_log = best_model_data.get('iou_log', [])
                
                print(f"âœ… æˆåŠŸä»æœ€ä½³æ¨¡å‹æ–‡ä»¶æ¢å¤è®­ç»ƒçŠ¶æ€!")
                print(f"  ä»epoch {start_epoch} ç»§ç»­è®­ç»ƒ")
                print(f"  å†å²æœ€ä½³IoU: {best_val_iou:.4f}")
                print(f"  å·²è®­ç»ƒepochæ•°: {len(iou_log)}")
                
                # åŒæ—¶æ›´æ–°æ£€æŸ¥ç‚¹æ–‡ä»¶
                torch.save(best_model_data, checkpoint_path)
                print(f"âœ… å·²åŒæ­¥æ›´æ–°æ£€æŸ¥ç‚¹æ–‡ä»¶: {checkpoint_path}")
                
            else:
                print("âš ï¸ æœ€ä½³æ¨¡å‹æ–‡ä»¶ä¸æ˜¯æ£€æŸ¥ç‚¹æ ¼å¼ï¼Œå°è¯•ä»æ£€æŸ¥ç‚¹æ–‡ä»¶æ¢å¤...")
                raise Exception("æœ€ä½³æ¨¡å‹æ–‡ä»¶ä¸æ˜¯æ£€æŸ¥ç‚¹æ ¼å¼")
                
        except Exception as e:
            print(f"âŒ ä»æœ€ä½³æ¨¡å‹æ–‡ä»¶æ¢å¤å¤±è´¥: {e}")
            print("å°è¯•ä»æ£€æŸ¥ç‚¹æ–‡ä»¶æ¢å¤...")
            
            # å¦‚æœä»æœ€ä½³æ¨¡å‹æ–‡ä»¶æ¢å¤å¤±è´¥ï¼Œå°è¯•ä»æ£€æŸ¥ç‚¹æ–‡ä»¶æ¢å¤
            if os.path.exists(checkpoint_path):
                try:
                    print(f"å‘ç°æ£€æŸ¥ç‚¹æ–‡ä»¶: {checkpoint_path}")
                    print("æ­£åœ¨å°è¯•æ¢å¤è®­ç»ƒçŠ¶æ€...")
                    
                    checkpoint = torch.load(checkpoint_path, map_location=device)
                    
                    # æ£€æŸ¥æ¨¡å‹ç»“æ„å…¼å®¹æ€§
                    if use_ensemble:
                        ensemble_model.load_state_dict(checkpoint['model_state_dict'])
                    else:
                        model.load_state_dict(checkpoint['model_state_dict'])
                    
                    if 'optimizer_state_dict' in checkpoint:
                        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
                    if 'scheduler_state_dict' in checkpoint:
                        scheduler.load_state_dict(checkpoint['scheduler_state_dict'])
                    if 'scaler_state_dict' in checkpoint:
                        hybrid_trainer.scaler.load_state_dict(checkpoint['scaler_state_dict'])
                    
                    start_epoch = checkpoint.get('epoch', 0) + 1
                    best_val_iou = checkpoint.get('best_val_iou', 0.0)
                    iou_log = checkpoint.get('iou_log', [])
                    
                    print(f"âœ… æˆåŠŸä»æ£€æŸ¥ç‚¹æ–‡ä»¶æ¢å¤è®­ç»ƒçŠ¶æ€!")
                    print(f"  ä»epoch {start_epoch} ç»§ç»­è®­ç»ƒ")
                    print(f"  å†å²æœ€ä½³IoU: {best_val_iou:.4f}")
                    print(f"  å·²è®­ç»ƒepochæ•°: {len(iou_log)}")
                    
                except Exception as e2:
                    print(f"âŒ æ¢å¤æ£€æŸ¥ç‚¹å¤±è´¥: {e2}")
                    print("å°†ä»å¤´å¼€å§‹è®­ç»ƒ...")
                    start_epoch = 1
                    best_val_iou = 0.0
                    iou_log = []
            else:
                print("æœªå‘ç°æ£€æŸ¥ç‚¹æ–‡ä»¶ï¼Œå°†ä»å¤´å¼€å§‹è®­ç»ƒ")
    else:
        print("æœªå‘ç°æœ€ä½³æ¨¡å‹æ–‡ä»¶ï¼Œå°è¯•ä»æ£€æŸ¥ç‚¹æ–‡ä»¶æ¢å¤...")
        if os.path.exists(checkpoint_path):
            try:
                print(f"å‘ç°æ£€æŸ¥ç‚¹æ–‡ä»¶: {checkpoint_path}")
                print("æ­£åœ¨å°è¯•æ¢å¤è®­ç»ƒçŠ¶æ€...")
                
                checkpoint = torch.load(checkpoint_path, map_location=device)
                
                # æ£€æŸ¥æ¨¡å‹ç»“æ„å…¼å®¹æ€§
                if use_ensemble:
                    ensemble_model.load_state_dict(checkpoint['model_state_dict'])
                else:
                    model.load_state_dict(checkpoint['model_state_dict'])
                
                if 'optimizer_state_dict' in checkpoint:
                    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
                if 'scheduler_state_dict' in checkpoint:
                    scheduler.load_state_dict(checkpoint['scheduler_state_dict'])
                if 'scaler_state_dict' in checkpoint:
                    hybrid_trainer.scaler.load_state_dict(checkpoint['scaler_state_dict'])
                
                start_epoch = checkpoint.get('epoch', 0) + 1
                best_val_iou = checkpoint.get('best_val_iou', 0.0)
                iou_log = checkpoint.get('iou_log', [])
                
                print(f"âœ… æˆåŠŸä»æ£€æŸ¥ç‚¹æ–‡ä»¶æ¢å¤è®­ç»ƒçŠ¶æ€!")
                print(f"  ä»epoch {start_epoch} ç»§ç»­è®­ç»ƒ")
                print(f"  å†å²æœ€ä½³IoU: {best_val_iou:.4f}")
                print(f"  å·²è®­ç»ƒepochæ•°: {len(iou_log)}")
                
            except Exception as e:
                print(f"âŒ æ¢å¤æ£€æŸ¥ç‚¹å¤±è´¥: {e}")
                print("å°†ä»å¤´å¼€å§‹è®­ç»ƒ...")
                start_epoch = 1
                best_val_iou = 0.0
                iou_log = []
        else:
            print("æœªå‘ç°æ£€æŸ¥ç‚¹æ–‡ä»¶ï¼Œå°†ä»å¤´å¼€å§‹è®­ç»ƒ")

    # === åˆ†é˜¶æ®µè®­ç»ƒé€»è¾‘ ===
    # è®¡ç®—å½“å‰é˜¶æ®µ - ä¿®å¤ï¼šæ ¹æ®å®é™…start_epochè®¡ç®—æ­£ç¡®çš„é˜¶æ®µ
    current_stage = (start_epoch - 1) // 20 + 1
    stage_start_epoch = (current_stage - 1) * 20 + 1
    stage_end_epoch = current_stage * 20
    
    print(f"\nå½“å‰è®­ç»ƒé˜¶æ®µ: {current_stage}")
    print(f"é˜¶æ®µèŒƒå›´: epoch {stage_start_epoch} - {stage_end_epoch}")
    print(f"å®é™…å¼€å§‹epoch: {start_epoch}")
    
    # æ›´æ–°è°ƒåº¦å™¨çš„total_steps
    total_steps = (stage_end_epoch - start_epoch + 1) * len(train_loader)
    scheduler = optim.lr_scheduler.OneCycleLR(
        optimizer,
        max_lr=[5e-4, 5e-3, 2.5e-3],  # å¯¹åº”encoder/decoder/fusionçš„max_lr
        total_steps=total_steps,
        pct_start=0.3,
        anneal_strategy='linear',
        final_div_factor=10000,
        three_phase=False
    )
    
    # æ£€æŸ¥æ˜¯å¦éœ€è¦è®­ç»ƒ - ä¿®å¤ï¼šåªæœ‰å½“start_epochè¶…å‡ºå½“å‰é˜¶æ®µèŒƒå›´æ—¶æ‰è·³è¿‡è®­ç»ƒ
    if start_epoch > stage_end_epoch:
        print(f"\nå½“å‰é˜¶æ®µå·²å®Œæˆï¼ˆä»epoch {start_epoch}æ¢å¤ï¼‰ï¼Œç›´æ¥è¿›è¡Œé‡åŒ–...")
    else:
        # æ­£å¸¸è®­ç»ƒå¾ªç¯ï¼ˆå½“å‰é˜¶æ®µï¼‰
        print(f"å¼€å§‹è®­ç»ƒé˜¶æ®µ {current_stage}ï¼Œä»epoch {start_epoch} åˆ° {stage_end_epoch}")
        
        # æ ¹æ®æ¨¡å‹ç±»å‹é€‰æ‹©è®­ç»ƒæ–¹å¼
        if use_ensemble:
            print("ğŸ¯ ä½¿ç”¨åŒæ¨¡å‹é›†æˆè®­ç»ƒ")
            # ä¸ºé›†æˆæ¨¡å‹åˆ›å»ºä¼˜åŒ–å™¨å’Œè°ƒåº¦å™¨ - ä½¿ç”¨æ›´ä¿å®ˆçš„å­¦ä¹ ç‡å’Œè‡ªé€‚åº”è°ƒæ•´
            ensemble_optimizer = optim.AdamW(ensemble_model.parameters(), lr=1e-6, weight_decay=1e-3)
            ensemble_scheduler = optim.lr_scheduler.OneCycleLR(
                ensemble_optimizer,
                max_lr=5e-5,  # è¿›ä¸€æ­¥é™ä½æœ€å¤§å­¦ä¹ ç‡
                total_steps=total_steps,
                pct_start=0.3,
                anneal_strategy='linear',
                final_div_factor=10000
            )
            ensemble_scaler = GradScaler()
            
            for epoch in range(start_epoch, stage_end_epoch + 1):
                # ä½¿ç”¨åŒæ¨¡å‹é›†æˆè®­ç»ƒ
                train_loss, train_iou = train_dual_model_epoch(
                    ensemble_model, train_loader, ensemble_optimizer, criterion, device, epoch, ensemble_scaler
                )
                
                # éªŒè¯
                val_loss, val_iou = val_dual_model_epoch(
                    ensemble_model, val_loader, criterion, device, ensemble_scaler
                )
                
                # æ›´æ–°å­¦ä¹ ç‡
                ensemble_scheduler.step()
                
                print(f"Epoch {epoch} - Train Loss: {train_loss:.4f}, Train IoU: {train_iou:.4f}")
                print(f"Epoch {epoch} - Val Loss: {val_loss:.4f}, Val IoU: {val_iou:.4f}")
                print(f"Fusion Weight: {ensemble_model.get_fusion_weight():.3f}")
                
                # ä¿å­˜æœ€ä½³æ¨¡å‹ - å¢å¼ºç‰ˆï¼šåŒ…å«å®Œæ•´è®­ç»ƒçŠ¶æ€
                if val_iou > best_val_iou:
                    best_val_iou = val_iou
                    torch.save({
                        'epoch': epoch,
                        'model_state_dict': ensemble_model.state_dict(),
                        'optimizer_state_dict': ensemble_optimizer.state_dict(),
                        'scheduler_state_dict': ensemble_scheduler.state_dict(),
                        'scaler_state_dict': ensemble_scaler.state_dict(),
                        'best_val_iou': best_val_iou,
                        'iou_log': iou_log,
                        'fusion_weight': ensemble_model.get_fusion_weight()
                    }, best_model_path)
                    print(f"âœ… ä¿å­˜æœ€ä½³é›†æˆæ¨¡å‹ï¼ŒIoU: {val_iou:.4f}")
                    print(f"   å·²ä¿å­˜å®Œæ•´è®­ç»ƒçŠ¶æ€åˆ°: {best_model_path}")
                
                iou_log.append(val_iou)
        else:
            # åŸå§‹å•æ¨¡å‹è®­ç»ƒ
            for epoch in range(start_epoch, stage_end_epoch + 1):
                model.train()
                total_loss = 0
                total_iou = 0
                total_dice = 0
                
                # æ›´æ–°è‡ªé€‚åº”éš¾æ ·æœ¬æŒ–æ˜å™¨çš„epoch
                adaptive_miner.update_epoch(epoch)
                
                # æ€§èƒ½ç›‘æ§
                epoch_start_time = time.time()
                gpu_compute_time = 0
                cpu_compute_time = 0
                
                for batch_idx, (images, masks, sim_feats) in enumerate(tqdm(train_loader, desc=f"Epoch {epoch}/{stage_end_epoch} - Training")):
                    # æ•°æ®ç§»åŠ¨åˆ°GPU
                    images = images.to(device, non_blocking=True)
                    masks = masks.to(device, non_blocking=True)
                    sim_feats = sim_feats.to(device, non_blocking=True)
                
                # æ©ç å¤„ç† - æ”¯æŒåŠ¨æ€è°ƒæ•´
                if damage_level != 'all':
                    # è®¡ç®—å½“å‰batchçš„æ ·æœ¬æ¯”ä¾‹
                    current_sample_ratio = None
                    if damage_level == 'adaptive':
                        # è®¡ç®—æŸåæ ·æœ¬æ¯”ä¾‹
                        damage_pixels = (masks >= 2).sum().float()
                        total_pixels = masks.numel()
                        current_sample_ratio = damage_pixels / total_pixels
                        if batch_idx % 100 == 0:  # æ¯100ä¸ªbatchæ‰“å°ä¸€æ¬¡ï¼Œå‡å°‘åˆ·å±
                            print(f"[åŠ¨æ€è°ƒæ•´] Batch {batch_idx} æŸåæ ·æœ¬æ¯”ä¾‹: {current_sample_ratio:.4f}")
                    
                    masks = process_xview2_mask(masks, damage_level, current_sample_ratio)
                
                # ä½¿ç”¨æ··åˆç²¾åº¦è®­ç»ƒå™¨
                gpu_start = time.time()
                
                # å‰å‘ä¼ æ’­
                with autocast('cuda'):
                    outputs = model(images, sim_feats)
                    # ä½¿ç”¨è‡ªé€‚åº”éš¾æ ·æœ¬æŒ–æ˜æŸå¤±
                    loss = adaptive_miner(outputs, masks)
                
                # åå‘ä¼ æ’­
                hybrid_trainer.optimizer.zero_grad()
                hybrid_trainer.scaler.scale(loss).backward()
                hybrid_trainer.scaler.step(hybrid_trainer.optimizer)
                hybrid_trainer.scaler.update()
                
                gpu_compute_time += time.time() - gpu_start
                
                # åå¤„ç†ä¼˜åŒ–IoU
                with torch.no_grad():
                    if enable_postprocess:
                        try:
                            processed_outputs = torch.stack([
                                postprocess(out, min_area=postprocess_min_area, merge_distance=postprocess_merge_distance)
                                for out in outputs.detach().cpu()
                            ]).to(device)
                            batch_iou = iou_score(processed_outputs, masks).item()
                            batch_dice = dice_score(processed_outputs, masks).item()
                        except Exception as e:
                            print(f'[è­¦å‘Š] è®­ç»ƒåå¤„ç†å¤±è´¥: {e}, ä½¿ç”¨åŸå§‹è¾“å‡ºè®¡ç®—IoU')
                            batch_iou = iou_score(outputs, masks).item()
                            batch_dice = dice_score(outputs, masks).item()
                    else:
                        # è·å–CPUè¾…åŠ©è®¡ç®—çš„æŒ‡æ ‡
                        cpu_result = hybrid_trainer.cpu_assistant.get_cpu_result()
                        if cpu_result:
                            task_type, result = cpu_result
                            if task_type == 'compute_metrics':
                                batch_iou = result['iou']
                                batch_dice = result['dice']
                            else:
                                batch_iou = iou_score(outputs, masks).item()
                                batch_dice = dice_score(outputs, masks).item()
                        else:
                            batch_iou = iou_score(outputs, masks).item()
                            batch_dice = dice_score(outputs, masks).item()
                
                # ä¿®å¤ï¼šæ·»åŠ æ›´è¯¦ç»†çš„è°ƒè¯•ä¿¡æ¯
                if batch_idx == 0 and epoch % 10 == 0:  # æ¯10ä¸ªepochæ‰“å°ç¬¬ä¸€ä¸ªbatchçš„ä¿¡æ¯ï¼Œå‡å°‘åˆ·å±
                    print(f"\n[è°ƒè¯•] Epoch {epoch} è®­ç»ƒç¬¬ä¸€ä¸ªbatch:")
                    print(f"  outputs: min={outputs.min().item():.4f}, max={outputs.max().item():.4f}, mean={outputs.mean().item():.4f}")
                    print(f"  loss: {loss:.4f}, IoU: {batch_iou:.4f}, Dice: {batch_dice:.4f}")
                    print(f"  æ©ç å¤„ç†æ–¹å¼: {damage_level}, åå¤„ç†å¯ç”¨: {enable_postprocess}")
                
                if torch.isnan(outputs).any() or torch.isinf(outputs).any():
                    print(f"[è­¦å‘Š] è®­ç»ƒ outputså­˜åœ¨NaN/Inf! batch_idx={batch_idx}")
                    continue  # è·³è¿‡è¿™ä¸ªbatch
                
                # è°ƒæ•´è¾“å‡ºç»´åº¦
                if outputs.shape[-2:] != masks.shape[-2:]:
                    outputs = F.interpolate(outputs, size=masks.shape[-2:], mode='bilinear', align_corners=False)
                outputs = outputs.squeeze(1) if outputs.shape[1] == 1 else outputs
                masks = masks.squeeze(1) if masks.shape[1] == 1 else masks
                
                total_loss += loss
                total_iou += batch_iou
                total_dice += batch_dice
                
                # æ¯100ä¸ªbatchæ‰“å°æ€§èƒ½ç»Ÿè®¡ - å·²ç§»é™¤ä»¥å‡å°‘åˆ·å±
            
            # OneCycleLRåœ¨æ¯ä¸ªbatchåstep
            scheduler.step()
            current_lr = optimizer.param_groups[0]['lr']
            
            epoch_time = time.time() - epoch_start_time
            avg_loss = total_loss / len(train_loader)
            avg_iou = total_iou / len(train_loader)
            avg_dice = total_dice / len(train_loader)
            
            print(f"Epoch {epoch} Train Loss: {avg_loss:.4f} IoU: {avg_iou:.4f} Dice: {avg_dice:.4f}")
            print(f"Epochæ—¶é—´: {epoch_time:.2f}s, å­¦ä¹ ç‡: {current_lr:.2e}")
            
            # æ‰“å°è¯¦ç»†çš„æ€§èƒ½ç»Ÿè®¡
            stats = hybrid_trainer.get_performance_stats()
            if stats:
                print(f"ğŸ“Š æ€§èƒ½ç»Ÿè®¡:")
                print(f"  GPUåˆ©ç”¨ç‡: {stats['gpu_utilization']:.1f}%")
                print(f"  å¹³å‡GPUæ—¶é—´: {stats['avg_gpu_time']:.3f}s")
                print(f"  å¹³å‡CPUæ—¶é—´: {stats['avg_cpu_time']:.3f}s")
                print(f"  æ€»æ‰¹æ¬¡æ•°: {stats['total_batches']}")
                print(f"  TF32åŠ é€Ÿ: å·²å¯ç”¨")
            print(f"Epoch {epoch} Train Loss: {avg_loss:.4f} IoU: {avg_iou:.4f} Dice: {avg_dice:.4f}")
            
            # éªŒè¯é˜¶æ®µ
            model.eval()
            val_loss = 0
            val_iou = 0
            val_dice = 0
            
            with torch.no_grad():
                for batch_idx, (images, masks, sim_feats) in enumerate(tqdm(val_loader, desc=f"Epoch {epoch}/{stage_end_epoch} - Validation")):
                    images = images.to(device, non_blocking=True)
                    masks = masks.to(device, non_blocking=True)
                    sim_feats = sim_feats.to(device, non_blocking=True)
                    
                    # æ©ç å¤„ç†
                    if damage_level != 'all':
                        masks = process_xview2_mask(masks, damage_level)
                    
                    with autocast('cuda'):
                        outputs = model(images, sim_feats)
                    
                    # ä¿®å¤ï¼šæ·»åŠ æ›´è¯¦ç»†çš„éªŒè¯è°ƒè¯•ä¿¡æ¯
                    if batch_idx == 0 and epoch % 10 == 0:  # æ¯10ä¸ªepochæ‰“å°ç¬¬ä¸€ä¸ªbatchçš„ä¿¡æ¯ï¼Œå‡å°‘åˆ·å±
                        print(f"\n[è°ƒè¯•] Epoch {epoch} éªŒè¯ç¬¬ä¸€ä¸ªbatch:")
                        print(f"  æ©ç å”¯ä¸€å€¼: {torch.unique(masks)}")
                        print(f"  outputs: min={outputs.min().item():.4f}, max={outputs.max().item():.4f}, mean={outputs.mean().item():.4f}")
                        print(f"  æ©ç å¤„ç†æ–¹å¼: {damage_level}, åå¤„ç†å¯ç”¨: {enable_postprocess}")
                    
                    if torch.isnan(outputs).any() or torch.isinf(outputs).any():
                        print(f"[è­¦å‘Š] éªŒè¯ outputså­˜åœ¨NaN/Inf! batch_idx={batch_idx}")
                        continue  # è·³è¿‡è¿™ä¸ªbatch
                    
                    if outputs.shape[-2:] != masks.shape[-2:]:
                        outputs = F.interpolate(outputs, size=masks.shape[-2:], mode='bilinear', align_corners=False)
                    outputs = outputs.squeeze(1) if outputs.shape[1] == 1 else outputs
                    masks = masks.squeeze(1) if masks.shape[1] == 1 else masks
                    
                    # åå¤„ç†ä¼˜åŒ–IoU
                    if enable_postprocess:
                        try:
                            processed_outputs = torch.stack([
                                postprocess(out, min_area=postprocess_min_area, merge_distance=postprocess_merge_distance)
                                for out in outputs.detach().cpu()
                            ]).to(device)
                            batch_iou = iou_score(processed_outputs, masks).item()
                            batch_dice = dice_score(processed_outputs, masks).item()
                        except Exception as e:
                            print(f'[è­¦å‘Š] éªŒè¯åå¤„ç†å¤±è´¥: {e}, ä½¿ç”¨åŸå§‹è¾“å‡ºè®¡ç®—IoU')
                            batch_iou = iou_score(outputs, masks).item()
                            batch_dice = dice_score(outputs, masks).item()
                    else:
                        # ä½¿ç”¨CPUè¾…åŠ©è®¡ç®—éªŒè¯æŒ‡æ ‡
                        hybrid_trainer.cpu_assistant.submit_cpu_task('compute_metrics', (outputs, masks))
                        cpu_result = hybrid_trainer.cpu_assistant.get_cpu_result()
                        
                        if cpu_result:
                            task_type, result = cpu_result
                            if task_type == 'compute_metrics':
                                batch_iou = result['iou']
                                batch_dice = result['dice']
                            else:
                                batch_iou = iou_score(outputs, masks).item()
                                batch_dice = dice_score(outputs, masks).item()
                        else:
                            batch_iou = iou_score(outputs, masks).item()
                            batch_dice = dice_score(outputs, masks).item()
                    
                    loss = hybrid_trainer.criterion(outputs, masks)
                    if torch.isnan(loss) or torch.isinf(loss):
                        print(f"[è°ƒè¯•] éªŒè¯ lossä¸ºNaN/Inf! batch_idx={batch_idx}")
                        continue
                    
                    val_loss += loss.item()
                    val_iou += batch_iou
                    val_dice += batch_dice
            avg_val_loss = val_loss / len(val_loader)
            avg_val_iou = val_iou / len(val_loader)
            avg_val_dice = val_dice / len(val_loader)
            print(f"Epoch {epoch} Val Loss: {avg_val_loss:.4f} IoU: {avg_val_iou:.4f} Dice: {avg_val_dice:.4f}")
            iou_log.append(avg_val_iou)
            
            # ä¿å­˜æœ€ä½³æ¨¡å‹é€»è¾‘ - å¢å¼ºç‰ˆï¼šä¿å­˜å®Œæ•´è®­ç»ƒçŠ¶æ€
            if avg_val_iou > best_val_iou:
                best_val_iou = avg_val_iou
                
                # ä¿å­˜å®Œæ•´è®­ç»ƒçŠ¶æ€åˆ°æœ€ä½³æ¨¡å‹æ–‡ä»¶
                if use_ensemble:
                    best_model_checkpoint = {
                        'epoch': epoch,
                        'model_state_dict': ensemble_model.state_dict(),
                        'optimizer_state_dict': optimizer.state_dict(),
                        'scheduler_state_dict': scheduler.state_dict(),
                        'scaler_state_dict': hybrid_trainer.scaler.state_dict(),
                        'best_val_iou': best_val_iou,
                        'iou_log': iou_log
                    }
                else:
                    best_model_checkpoint = {
                        'epoch': epoch,
                        'model_state_dict': model.state_dict(),
                        'optimizer_state_dict': optimizer.state_dict(),
                        'scheduler_state_dict': scheduler.state_dict(),
                        'scaler_state_dict': hybrid_trainer.scaler.state_dict(),
                        'best_val_iou': best_val_iou,
                        'iou_log': iou_log
                    }
                
                torch.save(best_model_checkpoint, best_model_path)
                print(f"[ä¿å­˜] æ–°æœ€ä½³æ¨¡å‹ï¼ŒVal IoU: {best_val_iou:.4f} (å†å²æœ€ä½³)")
                print(f"   å·²ä¿å­˜å®Œæ•´è®­ç»ƒçŠ¶æ€åˆ°: {best_model_path}")
            else:
                print(f"[æœªä¿å­˜] å½“å‰IoU {avg_val_iou:.4f} < å†å²æœ€ä½³ {best_val_iou:.4f}")
            
            # æ›´æ–°å­¦ä¹ ç‡è°ƒåº¦å™¨
            scheduler.step()
            current_lr = optimizer.param_groups[0]['lr']
            print(f"å½“å‰å­¦ä¹ ç‡: {current_lr:.2e}")
            
            # ä¿å­˜æ£€æŸ¥ç‚¹ï¼ˆåŒ…æ‹¬æ¨¡å‹ã€ä¼˜åŒ–å™¨ã€scalerç­‰çŠ¶æ€ï¼‰
            checkpoint = {
                'epoch': epoch,
                'model_state_dict': model.state_dict(),
                'optimizer_state_dict': optimizer.state_dict(),
                'scheduler_state_dict': scheduler.state_dict(),
                'scaler_state_dict': hybrid_trainer.scaler.state_dict(),
                'best_val_iou': best_val_iou,
                'iou_log': iou_log
            }
            torch.save(checkpoint, checkpoint_path)
            print(f"æ£€æŸ¥ç‚¹å·²ä¿å­˜: {checkpoint_path}")
        
        # ä¿å­˜IoUå†å²
        with open("iou_history.csv", "w") as f:
            f.write("epoch,iou\n")
            for i, iou in enumerate(iou_log):
                f.write(f"{i+1},{iou:.6f}\n")
    
    # === åˆ†é˜¶æ®µé‡åŒ–é€»è¾‘ ===
    # æ£€æŸ¥å½“å‰é˜¶æ®µæ˜¯å¦å®Œæˆ
    if start_epoch > stage_end_epoch or epoch == stage_end_epoch:
        # å…³é—­æ··åˆç²¾åº¦è®­ç»ƒå™¨
        print("ğŸ”„ å…³é—­CPU-GPUååŒè®­ç»ƒå™¨...")
        hybrid_trainer.shutdown()
        print("âœ… CPU-GPUååŒè®­ç»ƒå™¨å·²å…³é—­")
        
        print(f"\n" + "="*50)
        print(f"é˜¶æ®µ {current_stage} è®­ç»ƒå®Œæˆï¼å¼€å§‹è‡ªåŠ¨é‡åŒ–æ¨¡å‹...")
        print("="*50)
        try:
            import subprocess
            import sys
            # ç¡®ä¿æœ€ä½³æ¨¡å‹å­˜åœ¨
            if not os.path.exists(best_model_path):
                print(f"è­¦å‘Šï¼šæœ€ä½³æ¨¡å‹æ–‡ä»¶ {best_model_path} ä¸å­˜åœ¨ï¼Œè·³è¿‡é‡åŒ–")
                return
            # è°ƒç”¨é‡åŒ–è„šæœ¬
            quantize_script = os.path.join("inference", "quantize_model.py")
            if os.path.exists(quantize_script):
                print("å¼€å§‹é‡åŒ–æ¨¡å‹...")
                cmd = [
                    sys.executable, quantize_script,
                    "--model_path", best_model_path,
                    "--quant_path", f"models/quantized_seg_model_stage{current_stage}.pt",
                    "--data_root", "data/combined_dataset"
                ]
                result = subprocess.run(cmd, capture_output=True, text=True)
                if result.returncode == 0:
                    print("âœ… æ¨¡å‹é‡åŒ–æˆåŠŸï¼")
                    print("é‡åŒ–è¾“å‡º:")
                    print(result.stdout)
                    # æ£€æŸ¥é‡åŒ–æ¨¡å‹æ–‡ä»¶
                    quantized_model_path = f"models/quantized_seg_model_stage{current_stage}.pt"
                    if os.path.exists(quantized_model_path):
                        file_size = os.path.getsize(quantized_model_path) / (1024 * 1024)  # MB
                        print(f"é‡åŒ–æ¨¡å‹æ–‡ä»¶å¤§å°: {file_size:.2f} MB")
                        # å¯¹æ¯”åŸå§‹æ¨¡å‹å¤§å°
                        original_size = os.path.getsize(best_model_path) / (1024 * 1024)  # MB
                        compression_ratio = original_size / file_size
                        print(f"åŸå§‹æ¨¡å‹å¤§å°: {original_size:.2f} MB")
                        print(f"å‹ç¼©æ¯”: {compression_ratio:.2f}x")
                    else:
                        print("âŒ é‡åŒ–æ¨¡å‹æ–‡ä»¶æœªç”Ÿæˆ")
                else:
                    print("âŒ æ¨¡å‹é‡åŒ–å¤±è´¥ï¼")
                    print("é”™è¯¯è¾“å‡º:")
                    print(result.stderr)
            else:
                print(f"âŒ é‡åŒ–è„šæœ¬ {quantize_script} ä¸å­˜åœ¨")
        except Exception as e:
            print(f"âŒ é‡åŒ–è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
        print(f"\n" + "="*50)
        print(f"é˜¶æ®µ {current_stage} è®­ç»ƒå’Œé‡åŒ–æµç¨‹å®Œæˆï¼")
        print("="*50)
        # æç¤ºä¸‹ä¸€é˜¶æ®µ
        next_stage = current_stage + 1
        next_stage_start = next_stage * 20 - 19
        next_stage_end = next_stage * 20
        print(f"\nä¸‹ä¸€é˜¶æ®µ: é˜¶æ®µ {next_stage} (epoch {next_stage_start}-{next_stage_end})")
        print("é‡æ–°è¿è¡Œè„šæœ¬ç»§ç»­ä¸‹ä¸€é˜¶æ®µè®­ç»ƒ...")
    else:
        print(f"\nå½“å‰é˜¶æ®µ {current_stage} æœªå®Œæˆï¼Œè¯·ç»§ç»­è®­ç»ƒ...")

# ====== è®­ç»ƒä¸éªŒè¯æµç¨‹ä¼˜åŒ– ======
def train_multi_class_epoch(model, train_loader, optimizer, criterion, device, epoch, scaler):
    """
    å¤šç±»åˆ«åˆ†ç±»è®­ç»ƒå‡½æ•°
    """
    model.train()
    total_loss = 0
    total_accuracy = 0
    total_damage_iou = 0
    num_batches = 0
    
    for batch_idx, (images, masks, sim_feats) in enumerate(train_loader):
        images = images.to(device, non_blocking=True)
        masks = masks.to(device, non_blocking=True)
        sim_feats = sim_feats.to(device, non_blocking=True)
        
        optimizer.zero_grad()
        
        with torch.cuda.amp.autocast():
            outputs = model(images, sim_feats)
            loss = criterion(outputs, masks)
        
        scaler.scale(loss).backward()
        scaler.step(optimizer)
        scaler.update()
        
        # è®¡ç®—æ€§èƒ½æŒ‡æ ‡
        with torch.no_grad():
            performance = evaluate_multi_class_performance(outputs, masks)
            total_loss += loss.item()
            total_accuracy += performance['accuracy']
            total_damage_iou += performance['damage_iou']
            num_batches += 1
        
        if batch_idx % 10 == 0:
            print(f'Epoch {epoch}, Batch {batch_idx}/{len(train_loader)}, '
                  f'Loss: {loss.item():.4f}, '
                  f'Accuracy: {performance["accuracy"]:.4f}, '
                  f'Damage IoU: {performance["damage_iou"]:.4f}')
    
    avg_loss = total_loss / num_batches
    avg_accuracy = total_accuracy / num_batches
    avg_damage_iou = total_damage_iou / num_batches
    
    return avg_loss, avg_accuracy, avg_damage_iou

def val_multi_class_epoch(model, val_loader, criterion, device, scaler):
    """
    å¤šç±»åˆ«åˆ†ç±»éªŒè¯å‡½æ•°
    """
    model.eval()
    total_loss = 0
    total_accuracy = 0
    total_damage_iou = 0
    total_severe_iou = 0
    class_ious = [0] * 5
    num_batches = 0
    
    with torch.no_grad():
        for batch_idx, (images, masks, sim_feats) in enumerate(val_loader):
            images = images.to(device, non_blocking=True)
            masks = masks.to(device, non_blocking=True)
            sim_feats = sim_feats.to(device, non_blocking=True)
            
            with torch.cuda.amp.autocast():
                outputs = model(images, sim_feats)
                loss = criterion(outputs, masks)
            
            # è®¡ç®—æ€§èƒ½æŒ‡æ ‡
            performance = evaluate_multi_class_performance(outputs, masks)
            total_loss += loss.item()
            total_accuracy += performance['accuracy']
            total_damage_iou += performance['damage_iou']
            total_severe_iou += performance['severe_damage_iou']
            
            # ç´¯ç§¯æ¯ä¸ªç±»åˆ«çš„IoU
            for i, iou in enumerate(performance['class_ious']):
                class_ious[i] += iou
            
            num_batches += 1
    
    avg_loss = total_loss / num_batches
    avg_accuracy = total_accuracy / num_batches
    avg_damage_iou = total_damage_iou / num_batches
    avg_severe_iou = total_severe_iou / num_batches
    avg_class_ious = [iou / num_batches for iou in class_ious]
    
    return avg_loss, avg_accuracy, avg_damage_iou, avg_severe_iou, avg_class_ious

def train_epoch(model, train_loader, optimizer, criterion, device, epoch, scaler):
    model.train()
    total_loss = 0
    total_iou = 0
    for batch_idx, (images, masks, sim_feats) in enumerate(train_loader):
        images = images.to(device, non_blocking=True)
        masks = masks.to(device, non_blocking=True)
        sim_feats = sim_feats.to(device, non_blocking=True)
        optimizer.zero_grad()
        with autocast(device_type='cuda', dtype=torch.float16):
            outputs = model(images, sim_feats)
            if outputs.shape[-2:] != masks.shape[-2:]:
                outputs = F.interpolate(outputs, size=masks.shape[-2:], mode='bilinear', align_corners=False)
            loss = criterion(outputs, masks)
        scaler.scale(loss).backward()
        scaler.unscale_(optimizer)
        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
        scaler.step(optimizer)
        scaler.update()
        # åå¤„ç†ä¼˜åŒ–IoU - ä½¿ç”¨ç®€åŒ–ç‰ˆæœ¬
        with torch.no_grad():
            # ä½¿ç”¨ç®€åŒ–çš„åå¤„ç†å‡½æ•°ï¼Œæé«˜è®­ç»ƒé€Ÿåº¦
            processed_outputs = torch.stack([
                simple_postprocess(out) for out in outputs.detach().cpu()
            ]).to(device)
            iou = iou_score(processed_outputs, masks)
        total_loss += loss.item()
        total_iou += iou.item()
        if batch_idx % 50 == 0:
            print(f'Epoch {epoch} Batch {batch_idx}/{len(train_loader)} Loss: {loss.item():.4f} IoU: {iou.item():.4f}')
    avg_loss = total_loss / len(train_loader)
    avg_iou = total_iou / len(train_loader)
    return avg_loss, avg_iou

def val_epoch(model, val_loader, criterion, device, scaler):
    model.eval()
    total_loss = 0
    total_iou = 0
    with torch.no_grad():
        for batch_idx, (images, masks, sim_feats) in enumerate(val_loader):
            images = images.to(device, non_blocking=True)
            masks = masks.to(device, non_blocking=True)
            sim_feats = sim_feats.to(device, non_blocking=True)
            with autocast(device_type='cuda', dtype=torch.float16):
                outputs = model(images, sim_feats)
                if outputs.shape[-2:] != masks.shape[-2:]:
                    outputs = F.interpolate(outputs, size=masks.shape[-2:], mode='bilinear', align_corners=False)
                loss = criterion(outputs, masks)
            # éªŒè¯æ—¶ä½¿ç”¨ç®€åŒ–åå¤„ç†
            processed_outputs = torch.stack([
                simple_postprocess(out) for out in outputs.detach().cpu()
            ]).to(device)
            iou = iou_score(processed_outputs, masks)
            total_loss += loss.item()
            total_iou += iou.item()
    avg_loss = total_loss / len(val_loader)
    avg_iou = total_iou / len(val_loader)
    return avg_loss, avg_iou

# ====== æ¨¡å‹é›†æˆç»“æ„ ======
class ModelEnsemble(nn.Module):
    def __init__(self, model_paths, device):
        super().__init__()
        self.models = []
        for path in model_paths:
            model = EnhancedDeepLab().to(device)
            model.load_state_dict(torch.load(path, map_location=device))
            model.eval()
            self.models.append(model)
    def forward(self, x, sim_feat):
        outputs = []
        for model in self.models:
            with torch.no_grad():
                output = model(x, sim_feat)
                outputs.append(torch.sigmoid(output))
        weights = torch.linspace(0.5, 1.5, len(outputs)).to(x.device)
        weights = F.softmax(weights, dim=0)
        ensemble_output = torch.zeros_like(outputs[0])
        for i, out in enumerate(outputs):
            ensemble_output += weights[i] * out
        return ensemble_output

# å¤šå°ºåº¦ç‰¹å¾èåˆæ¨¡å‹ - ä¿®å¤ç‰ˆæœ¬
class MultiScaleDeepLab(nn.Module):
    def __init__(self, in_channels=3, num_classes=1, sim_feat_dim=11):
        super().__init__()
        # ä½¿ç”¨ç°æœ‰çš„EnhancedDeepLabä½œä¸ºåŸºç¡€
        self.base_model = smp.DeepLabV3Plus(
            encoder_name="resnext101_32x8d",
            encoder_weights="imagenet",
            in_channels=in_channels,
            classes=num_classes,
            activation=None
        )
        
        # å¤šå°ºåº¦åˆ†æ”¯ - å¢å¼ºç‰ˆæœ¬
        self.scale_branches = nn.ModuleList([
            # ä¸‹é‡‡æ ·åˆ†æ”¯ (1/4åˆ†è¾¨ç‡)
            nn.Sequential(
                nn.Conv2d(in_channels, 64, kernel_size=3, stride=2, padding=1),
                nn.BatchNorm2d(64),
                nn.ReLU(inplace=True),
                nn.Conv2d(64, 128, kernel_size=3, stride=2, padding=1),
                nn.BatchNorm2d(128),
                nn.ReLU(inplace=True)
            ),
            # åŸå§‹åˆ†è¾¨ç‡åˆ†æ”¯
            nn.Sequential(
                nn.Conv2d(in_channels, 64, kernel_size=3, padding=1),
                nn.BatchNorm2d(64),
                nn.ReLU(inplace=True)
            ),
            # ä¸Šé‡‡æ ·åˆ†æ”¯ (2xåˆ†è¾¨ç‡)
            nn.Sequential(
                nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True),
                nn.Conv2d(in_channels, 64, kernel_size=3, padding=1),
                nn.BatchNorm2d(64),
                nn.ReLU(inplace=True)
            )
        ])
        
        # ä»¿çœŸç‰¹å¾èåˆæ¨¡å— - å¢å¼ºç‰ˆæœ¬
        self.sim_fusion = nn.Sequential(
            nn.Linear(sim_feat_dim, 512),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(512, 1024),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(1024, 2048),
            nn.ReLU(),
            nn.Dropout(0.3)
        )
        
        # æ³¨æ„åŠ›é—¨æ§æœºåˆ¶ - é‡æ–°æ·»åŠ 
        self.attention_gate = nn.Sequential(
            nn.Conv2d(2048 + 128 + 64 + 64 + 2048, 1024, kernel_size=1),  # åŸºç¡€ç‰¹å¾ + 3ä¸ªå°ºåº¦ç‰¹å¾ + simç‰¹å¾
            nn.BatchNorm2d(1024),
            nn.ReLU(),
            nn.Conv2d(1024, 512, kernel_size=1),
            nn.BatchNorm2d(512),
            nn.ReLU(),
            nn.Conv2d(512, 2048, kernel_size=1),
            nn.Sigmoid()
        )
        
        # ç‰¹å¾èåˆå·ç§¯ - å¢å¼ºç‰ˆæœ¬
        self.fusion_conv = nn.Sequential(
            nn.Conv2d(2048 + 128 + 64 + 64, 1024, kernel_size=1),
            nn.BatchNorm2d(1024),
            nn.ReLU(inplace=True),
            nn.Conv2d(1024, 2048, kernel_size=3, padding=1),
            nn.BatchNorm2d(2048),
            nn.ReLU(inplace=True)
        )
        
        # ç©ºé—´æ³¨æ„åŠ›æ¨¡å—
        self.spatial_attention = nn.Sequential(
            nn.Conv2d(2048, 512, kernel_size=1),
            nn.ReLU(),
            nn.Conv2d(512, 1, kernel_size=1),
            nn.Sigmoid()
        )
        
        # é€šé“æ³¨æ„åŠ›æ¨¡å—
        self.channel_attention = nn.Sequential(
            nn.AdaptiveAvgPool2d(1),
            nn.Conv2d(2048, 512, kernel_size=1),
            nn.ReLU(),
            nn.Conv2d(512, 2048, kernel_size=1),
            nn.Sigmoid()
        )
        
        # åˆå§‹åŒ–æƒé‡
        self._init_weights()
    
    def _init_weights(self):
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')
                if m.bias is not None:
                    nn.init.zeros_(m.bias)
            elif isinstance(m, nn.Linear):
                nn.init.xavier_uniform_(m.weight)
                if m.bias is not None:
                    nn.init.zeros_(m.bias)
            elif isinstance(m, nn.BatchNorm2d):
                nn.init.ones_(m.weight)
                nn.init.zeros_(m.bias)
    
    def forward(self, img, sim_feat):
        # è¾“å…¥æ£€æŸ¥
        if torch.isnan(img).any() or torch.isinf(img).any():
            print(f"[è­¦å‘Š] æ¨¡å‹è¾“å…¥å›¾åƒåŒ…å«NaN/Inf!")
            img = torch.nan_to_num(img, nan=0.0, posinf=1.0, neginf=-1.0)
        
        if torch.isnan(sim_feat).any() or torch.isinf(sim_feat).any():
            print(f"[è­¦å‘Š] æ¨¡å‹è¾“å…¥simç‰¹å¾åŒ…å«NaN/Inf!")
            sim_feat = torch.nan_to_num(sim_feat, nan=0.0, posinf=1.0, neginf=-1.0)
        
        # åŸºç¡€æ¨¡å‹ç‰¹å¾æå–
        base_features = self.base_model.encoder(img)
        base_feature = base_features[-1]  # [B, 2048, H/32, W/32]
        B, C, H, W = base_feature.shape
        
        # å¤šå°ºåº¦ç‰¹å¾æå–
        scale_features = []
        for branch in self.scale_branches:
            scale_feat = branch(img)
            # è°ƒæ•´åˆ°ä¸åŸºç¡€ç‰¹å¾ç›¸åŒçš„å°ºå¯¸
            if scale_feat.shape[-2:] != base_feature.shape[-2:]:
                scale_feat = F.interpolate(scale_feat, size=base_feature.shape[-2:], 
                                          mode='bilinear', align_corners=True)
            scale_features.append(scale_feat)
        
        # ä»¿çœŸç‰¹å¾èåˆ
        sim_proj = self.sim_fusion(sim_feat)  # [B, 2048]
        sim_proj = sim_proj.view(B, -1, 1, 1)  # [B, 2048, 1, 1]
        sim_proj = sim_proj.expand(-1, -1, H, W)  # [B, 2048, H, W]
        
        # ç‰¹å¾æ‹¼æ¥
        combined_features = torch.cat([base_feature] + scale_features, dim=1)  # [B, 2048+128+64+64, H, W]
        
        # æ³¨æ„åŠ›èåˆ - é‡æ–°æ·»åŠ 
        attention_input = torch.cat([combined_features, sim_proj], dim=1)
        attention = self.attention_gate(attention_input)
        attended_features = base_feature * attention + sim_proj * (1 - attention)
        
        # æœ€ç»ˆç‰¹å¾èåˆ
        fused_features = self.fusion_conv(combined_features)
        
        # ç©ºé—´æ³¨æ„åŠ›
        spatial_weights = self.spatial_attention(fused_features)
        fused_features = fused_features * spatial_weights
        
        # é€šé“æ³¨æ„åŠ›
        channel_weights = self.channel_attention(fused_features)
        fused_features = fused_features * channel_weights
        
        # æ£€æŸ¥èåˆåçš„ç‰¹å¾
        if torch.isnan(fused_features).any() or torch.isinf(fused_features).any():
            print(f"[è­¦å‘Š] èåˆç‰¹å¾åŒ…å«NaN/Infï¼Œä½¿ç”¨åŸå§‹ç‰¹å¾!")
            fused_features = base_feature
        
        # é€šè¿‡è§£ç å™¨
        features = list(base_features)
        features[-1] = fused_features
        out = self.base_model.decoder(features)
        out = self.base_model.segmentation_head(out)
        
        # æ£€æŸ¥æœ€ç»ˆè¾“å‡º
        if torch.isnan(out).any() or torch.isinf(out).any():
            print(f"[è­¦å‘Š] æ¨¡å‹è¾“å‡ºåŒ…å«NaN/Infï¼Œè¿”å›é›¶å¼ é‡!")
            out = torch.zeros_like(out)
        
        return out

# æ·»åŠ IoU-basedéš¾æ ·æœ¬è¯†åˆ«
def identify_hard_samples(self, pred, target):
    pred_prob = torch.sigmoid(pred)
    pred_binary = (pred_prob > 0.5).float()
    
    # è®¡ç®—æ¯ä¸ªåƒç´ çš„IoUè´¡çŒ®
    intersection = pred_binary * target
    union = pred_binary + target - intersection
    
    # ä½IoUåŒºåŸŸä½œä¸ºéš¾æ ·æœ¬
    pixel_iou = intersection / (union + 1e-8)
    hard_mask = pixel_iou < 0.5
    
    return hard_mask

# é›†æˆè®­ç»ƒç­–ç•¥ (Ensemble Training Strategy) - æ”¹è¿›ç‰ˆæœ¬

# é›†æˆè®­ç»ƒç­–ç•¥ (Ensemble Training Strategy) - æ”¹è¿›ç‰ˆæœ¬
class EnsembleTrainer:
    def __init__(self, models, train_loader, val_loader, device, 
                 use_enhanced_loss=True, use_mixed_precision=True):
        self.models = models
        self.train_loader = train_loader
        self.val_loader = val_loader
        self.device = device
        self.use_enhanced_loss = use_enhanced_loss
        self.use_mixed_precision = use_mixed_precision
        
        # å°†æ¨¡å‹ç§»åŠ¨åˆ°è®¾å¤‡
        for model in self.models:
            model.to(device)
        
        # ä¼˜åŒ–å™¨
        self.optimizers = []
        for model in self.models:
            # åˆ†å±‚å­¦ä¹ ç‡
            encoder_params = []
            decoder_params = []
            fusion_params = []
            
            for name, param in model.named_parameters():
                if 'encoder' in name:
                    encoder_params.append(param)
                elif 'decoder' in name or 'segmentation_head' in name:
                    decoder_params.append(param)
                else:
                    fusion_params.append(param)
            
            optimizer = optim.AdamW([
                {'params': encoder_params, 'lr': 1e-5},
                {'params': decoder_params, 'lr': 1e-4},
                {'params': fusion_params, 'lr': 5e-4}
            ], weight_decay=1e-4)
            self.optimizers.append(optimizer)
        
        # æŸå¤±å‡½æ•°
        if use_enhanced_loss:
            base_criterion = HybridLoss()
            self.criterion = BoundaryAwareLoss(base_criterion, alpha=0.3, beta=0.2)
        else:
            self.criterion = nn.BCEWithLogitsLoss()
        
        # å­¦ä¹ ç‡è°ƒåº¦å™¨
        self.schedulers = []
        for optimizer in self.optimizers:
            scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(
                optimizer, T_0=10, T_mult=2, eta_min=1e-6
            )
            self.schedulers.append(scheduler)
        
        # æ··åˆç²¾åº¦è®­ç»ƒ
        if use_mixed_precision:
            self.scalers = [GradScaler() for _ in range(len(self.models))]
        else:
            self.scalers = [None] * len(self.models)
        
        # æ—©åœæœºåˆ¶
        self.best_ensemble_iou = 0.0
        self.patience = 15
        self.patience_counter = 0
        
        # æ¨¡å‹ä¿å­˜è·¯å¾„
        self.save_dir = "models/ensemble"
        os.makedirs(self.save_dir, exist_ok=True)
    
    def train_epoch(self, epoch):
        """è®­ç»ƒä¸€ä¸ªepoch"""
        total_losses = [0.0] * len(self.models)
        total_ious = [0.0] * len(self.models)
        
        # è®¾ç½®æ¨¡å‹ä¸ºè®­ç»ƒæ¨¡å¼
        for model in self.models:
            model.train()
        
        # å¾ªç¯è®­ç»ƒæ¯ä¸ªæ¨¡å‹
        for batch_idx, (images, masks, sim_feats) in enumerate(tqdm(self.train_loader, 
                                                                   desc=f"Ensemble Epoch {epoch}")):
            images = images.to(self.device, non_blocking=True)
            masks = masks.to(self.device, non_blocking=True)
            sim_feats = sim_feats.to(self.device, non_blocking=True)
            
            # è®­ç»ƒæ¯ä¸ªæ¨¡å‹
            for i, (model, optimizer, scaler) in enumerate(zip(self.models, self.optimizers, self.scalers)):
                optimizer.zero_grad()
                
                # å‰å‘ä¼ æ’­
                if self.use_mixed_precision and scaler is not None:
                    with autocast('cuda'):
                        outputs = model(images, sim_feats)
                        
                        # è°ƒæ•´å°ºå¯¸
                        if outputs.shape[-2:] != masks.shape[-2:]:
                            outputs = F.interpolate(outputs, size=masks.shape[-2:], 
                                                  mode='bilinear', align_corners=False)
                        
                        # è®¡ç®—æŸå¤±
                        loss = self.criterion(outputs, masks)
                    
                    # åå‘ä¼ æ’­
                    scaler.scale(loss).backward()
                    scaler.step(optimizer)
                    scaler.update()
                else:
                    outputs = model(images, sim_feats)
                    
                    # è°ƒæ•´å°ºå¯¸
                    if outputs.shape[-2:] != masks.shape[-2:]:
                        outputs = F.interpolate(outputs, size=masks.shape[-2:], 
                                              mode='bilinear', align_corners=False)
                    
                    # è®¡ç®—æŸå¤±
                    loss = self.criterion(outputs, masks)
                    
                    # åå‘ä¼ æ’­
                    loss.backward()
                    optimizer.step()
                
                # è®¡ç®—IoU
                with torch.no_grad():
                    pred_prob = torch.sigmoid(outputs)
                    iou = iou_score(pred_prob, masks).item()
                
                total_losses[i] += loss.item()
                total_ious[i] += iou
        
        # è®¡ç®—å¹³å‡æŸå¤±å’ŒIoU
        avg_losses = [loss / len(self.train_loader) for loss in total_losses]
        avg_ious = [iou / len(self.train_loader) for iou in total_ious]
        
        # æ›´æ–°å­¦ä¹ ç‡è°ƒåº¦å™¨
        for scheduler in self.schedulers:
            scheduler.step()
        
        return avg_losses, avg_ious
    
    def validate_ensemble(self):
        """éªŒè¯é›†æˆæ¨¡å‹"""
        ensemble_iou = 0.0
        ensemble_dice = 0.0
        
        # è®¾ç½®æ¨¡å‹ä¸ºè¯„ä¼°æ¨¡å¼
        for model in self.models:
            model.eval()
        
        with torch.no_grad():
            for images, masks, sim_feats in tqdm(self.val_loader, desc="Ensemble Validation"):
                images = images.to(self.device, non_blocking=True)
                masks = masks.to(self.device, non_blocking=True)
                sim_feats = sim_feats.to(self.device, non_blocking=True)
                
                # æ¨¡å‹é¢„æµ‹
                predictions = []
                for model in self.models:
                    outputs = model(images, sim_feats)
                    if outputs.shape[-2:] != masks.shape[-2:]:
                        outputs = F.interpolate(outputs, size=masks.shape[-2:], 
                                              mode='bilinear', align_corners=False)
                    predictions.append(torch.sigmoid(outputs))
                
                # é›†æˆé¢„æµ‹ - åŠ æƒå¹³å‡
                weights = torch.linspace(0.8, 1.2, len(predictions)).to(self.device)
                weights = F.softmax(weights, dim=0)
                
                ensemble_pred = torch.zeros_like(predictions[0])
                for i, pred in enumerate(predictions):
                    ensemble_pred += weights[i] * pred
                
                # è®¡ç®—æŒ‡æ ‡
                iou = iou_score(ensemble_pred, masks).item()
                dice = dice_score(ensemble_pred, masks).item()
                
                ensemble_iou += iou
                ensemble_dice += dice
        
        ensemble_iou /= len(self.val_loader)
        ensemble_dice /= len(self.val_loader)
        
        return ensemble_iou, ensemble_dice
    
    def save_models(self, epoch, ensemble_iou):
        """ä¿å­˜æ¨¡å‹"""
        for i, model in enumerate(self.models):
            model_path = os.path.join(self.save_dir, f"ensemble_model_{i}_epoch_{epoch}.pth")
            torch.save({
                'epoch': epoch,
                'model_state_dict': model.state_dict(),
                'optimizer_state_dict': self.optimizers[i].state_dict(),
                'scheduler_state_dict': self.schedulers[i].state_dict(),
                'ensemble_iou': ensemble_iou
            }, model_path)
        
        print(f"âœ… é›†æˆæ¨¡å‹å·²ä¿å­˜ï¼ŒEnsemble IoU: {ensemble_iou:.4f}")
    
    def train(self, epochs=50):
        """å®Œæ•´è®­ç»ƒæµç¨‹"""
        print("ğŸš€ å¼€å§‹é›†æˆè®­ç»ƒ...")
        print(f"æ¨¡å‹æ•°é‡: {len(self.models)}")
        print(f"ä½¿ç”¨å¢å¼ºæŸå¤±: {self.use_enhanced_loss}")
        print(f"ä½¿ç”¨æ··åˆç²¾åº¦: {self.use_mixed_precision}")
        
        for epoch in range(1, epochs + 1):
            # è®­ç»ƒé˜¶æ®µ
            avg_losses, avg_ious = self.train_epoch(epoch)
            
            # éªŒè¯é˜¶æ®µ
            ensemble_iou, ensemble_dice = self.validate_ensemble()
            
            # æ‰“å°è®­ç»ƒä¿¡æ¯
            print(f"\nEpoch {epoch}/{epochs}")
            print(f"è®­ç»ƒæŸå¤±: {[f'{loss:.4f}' for loss in avg_losses]}")
            print(f"è®­ç»ƒIoU: {[f'{iou:.4f}' for iou in avg_ious]}")
            print(f"é›†æˆéªŒè¯ IoU: {ensemble_iou:.4f}, Dice: {ensemble_dice:.4f}")
            
            # æ—©åœæ£€æŸ¥
            if ensemble_iou > self.best_ensemble_iou:
                self.best_ensemble_iou = ensemble_iou
                self.patience_counter = 0
                # ä¿å­˜æœ€ä½³æ¨¡å‹
                self.save_models(epoch, ensemble_iou)
            else:
                self.patience_counter += 1
                if self.patience_counter >= self.patience:
                    print(f"æ—©åœè§¦å‘ï¼{self.patience} ä¸ªepochæ²¡æœ‰æ”¹å–„")
                    break
        
        print(f"ğŸ‰ é›†æˆè®­ç»ƒå®Œæˆï¼æœ€ä½³Ensemble IoU: {self.best_ensemble_iou:.4f}")
        return self.models

def ensemble_training(models, train_loader, val_loader, device, epochs=50):
    """ç®€åŒ–çš„é›†æˆè®­ç»ƒæ¥å£"""
    trainer = EnsembleTrainer(models, train_loader, val_loader, device)
    return trainer.train(epochs)

def validate_ensemble(models, val_loader, device):
    """ç®€åŒ–çš„é›†æˆéªŒè¯æ¥å£"""
    ensemble_iou = 0
    for images, masks, sim_feats in val_loader:
        images = images.to(device)
        masks = masks.to(device)
        sim_feats = sim_feats.to(device)
        
        # æ¨¡å‹é¢„æµ‹
        predictions = []
        for model in models:
            with torch.no_grad():
                outputs = model(images, sim_feats)
                if outputs.shape[-2:] != masks.shape[-2:]:
                    outputs = F.interpolate(outputs, size=masks.shape[-2:], 
                                          mode='bilinear', align_corners=False)
                predictions.append(torch.sigmoid(outputs))
        
        # é›†æˆé¢„æµ‹
        ensemble_pred = torch.mean(torch.stack(predictions), dim=0)
        
        # è®¡ç®—IoU
        iou = iou_score(ensemble_pred, masks)
        ensemble_iou += iou.item()
    
    return ensemble_iou / len(val_loader)

# ä¸ºäº†å‘åå…¼å®¹æ€§ï¼Œæ·»åŠ DeepLabWithSimFeatureä½œä¸ºEnhancedDeepLabçš„åˆ«å
DeepLabWithSimFeature = EnhancedDeepLab

class DualModelEnsemble(nn.Module):
    """åŒæ¨¡å‹é›†æˆï¼šDeepLab + åˆ†å‰²ç‰ˆLandslideDetector"""
    def __init__(self, deeplab_model, landslide_model, fusion_weight=0.5):
        super().__init__()
        self.deeplab_model = deeplab_model
        self.landslide_model = landslide_model
        self.fusion_weight = fusion_weight
        
        # å¯å­¦ä¹ çš„èåˆæƒé‡
        self.learnable_weight = nn.Parameter(torch.tensor([fusion_weight]))
        
    def forward(self, img, sim_feat=None):
        # DeepLabå‰å‘ä¼ æ’­
        deeplab_output = self.deeplab_model(img, sim_feat)
        
        # LandslideDetectorå‰å‘ä¼ æ’­ï¼ˆä¸éœ€è¦sim_featï¼‰
        landslide_output = self.landslide_model(img)
        
        # æ£€æŸ¥è¾“å‡ºæ˜¯å¦åŒ…å«NaN
        if torch.isnan(deeplab_output).any() or torch.isinf(deeplab_output).any():
            print("[è­¦å‘Š] DeepLabè¾“å‡ºåŒ…å«NaN/Infï¼Œä½¿ç”¨é›¶å¼ é‡")
            deeplab_output = torch.zeros_like(deeplab_output)
        
        if torch.isnan(landslide_output).any() or torch.isinf(landslide_output).any():
            print("[è­¦å‘Š] LandslideDetectorè¾“å‡ºåŒ…å«NaN/Infï¼Œä½¿ç”¨é›¶å¼ é‡")
            landslide_output = torch.zeros_like(landslide_output)
        
        # ç¡®ä¿è¾“å‡ºå°ºå¯¸ä¸€è‡´
        if deeplab_output.shape != landslide_output.shape:
            landslide_output = F.interpolate(landslide_output, size=deeplab_output.shape[2:], mode='bilinear', align_corners=False)
        
        # åŠ æƒèåˆ
        weight = torch.sigmoid(self.learnable_weight)  # ç¡®ä¿æƒé‡åœ¨[0,1]èŒƒå›´å†…
        
        # æ£€æŸ¥æƒé‡æ˜¯å¦åŒ…å«NaN
        if torch.isnan(weight) or torch.isinf(weight):
            print("[è­¦å‘Š] èåˆæƒé‡åŒ…å«NaN/Infï¼Œä½¿ç”¨é»˜è®¤æƒé‡0.5")
            weight = torch.tensor(0.5, device=img.device)
        
        ensemble_output = weight * deeplab_output + (1 - weight) * landslide_output
        
        # æœ€ç»ˆæ£€æŸ¥
        if torch.isnan(ensemble_output).any() or torch.isinf(ensemble_output).any():
            print("[è­¦å‘Š] èåˆè¾“å‡ºåŒ…å«NaN/Infï¼Œä½¿ç”¨é›¶å¼ é‡")
            ensemble_output = torch.zeros_like(ensemble_output)
        
        return ensemble_output
    
    def get_fusion_weight(self):
        """è·å–å½“å‰èåˆæƒé‡"""
        return torch.sigmoid(self.learnable_weight).item()

def train_dual_model_epoch(ensemble_model, train_loader, optimizer, criterion, device, epoch, scaler):
    """è®­ç»ƒåŒæ¨¡å‹é›†æˆçš„ä¸€ä¸ªepoch"""
    ensemble_model.train()
    total_loss = 0
    total_iou = 0
    num_batches = 0
    
    progress_bar = tqdm(train_loader, desc=f'Epoch {epoch+1} [Train]')
    
    for batch_idx, (images, masks, sim_feats) in enumerate(progress_bar):
        images = images.to(device)
        masks = masks.to(device)
        sim_feats = sim_feats.to(device)
        
        optimizer.zero_grad()
        
        with autocast(device_type='cuda'):
            # å‰å‘ä¼ æ’­
            outputs = ensemble_model(images, sim_feats)
            
                    # è®¡ç®—æŸå¤±
        loss = criterion(outputs, masks)
        
        # æ£€æŸ¥æŸå¤±æ˜¯å¦åŒ…å«NaN
        if torch.isnan(loss) or torch.isinf(loss):
            print(f"[è­¦å‘Š] æŸå¤±åŒ…å«NaN/Infï¼Œè·³è¿‡batch {batch_idx}")
            continue
        
        # åå‘ä¼ æ’­
        scaler.scale(loss).backward()
        
        # æ£€æŸ¥æ¢¯åº¦æ˜¯å¦åŒ…å«NaN/Infï¼ˆåœ¨unscaleä¹‹å‰ï¼‰
        grad_norm = 0
        has_nan_grad = False
        has_inf_grad = False
        
        # ä½¿ç”¨scaleræ£€æŸ¥æ¢¯åº¦
        scaler.unscale_(optimizer)
        
        for param in ensemble_model.parameters():
            if param.grad is not None:
                if torch.isnan(param.grad).any():
                    has_nan_grad = True
                if torch.isinf(param.grad).any():
                    has_inf_grad = True
                grad_norm += param.grad.data.norm(2).item() ** 2
        
        grad_norm = grad_norm ** (1. / 2)
        
        if has_nan_grad or has_inf_grad:
            print(f"[è­¦å‘Š] æ¢¯åº¦åŒ…å«NaN/Infï¼Œè¿›è¡Œæ¢¯åº¦ä¿®å¤")
            # å°†NaN/Infæ¢¯åº¦æ›¿æ¢ä¸º0
            for param in ensemble_model.parameters():
                if param.grad is not None:
                    param.grad.data = torch.where(
                        torch.isnan(param.grad.data) | torch.isinf(param.grad.data),
                        torch.zeros_like(param.grad.data),
                        param.grad.data
                    )
        
        # æ›´ä¸¥æ ¼çš„æ¢¯åº¦è£å‰ªå’Œè‡ªé€‚åº”å­¦ä¹ ç‡è°ƒæ•´
        if grad_norm > 10:
            print(f"[è­¦å‘Š] æ¢¯åº¦èŒƒæ•°è¿‡å¤§ ({grad_norm:.2f})ï¼Œè¿›è¡Œæ¢¯åº¦è£å‰ªå¹¶é™ä½å­¦ä¹ ç‡")
            torch.nn.utils.clip_grad_norm_(ensemble_model.parameters(), max_norm=5.0)
            # ä¸´æ—¶é™ä½å­¦ä¹ ç‡
            for param_group in optimizer.param_groups:
                param_group['lr'] *= 0.5
        elif grad_norm > 5:
            print(f"[è­¦å‘Š] æ¢¯åº¦èŒƒæ•°è¾ƒå¤§ ({grad_norm:.2f})ï¼Œè¿›è¡Œæ¢¯åº¦è£å‰ª")
            torch.nn.utils.clip_grad_norm_(ensemble_model.parameters(), max_norm=2.0)
        else:
            torch.nn.utils.clip_grad_norm_(ensemble_model.parameters(), max_norm=1.0)
        
        scaler.step(optimizer)
        scaler.update()
        
        # æ£€æŸ¥æƒé‡æ˜¯å¦å˜ä¸ºNaN/Inf
        has_nan_weight = False
        has_inf_weight = False
        for param in ensemble_model.parameters():
            if torch.isnan(param).any():
                has_nan_weight = True
            if torch.isinf(param).any():
                has_inf_weight = True
        
        if has_nan_weight or has_inf_weight:
            print(f"[è­¦å‘Š] æƒé‡åŒ…å«NaN/Infï¼Œè¿›è¡Œæƒé‡ä¿®å¤")
            # å°†NaN/Infæƒé‡æ›¿æ¢ä¸ºå°çš„éšæœºå€¼
            for param in ensemble_model.parameters():
                if torch.isnan(param).any() or torch.isinf(param).any():
                    param.data = torch.where(
                        torch.isnan(param.data) | torch.isinf(param.data),
                        torch.randn_like(param.data) * 0.01,
                        param.data
                    )
        
        # è®¡ç®—IoU
        with torch.no_grad():
            pred_masks = torch.sigmoid(outputs) > 0.5
            iou = iou_score(pred_masks, masks)
        
        total_loss += loss.item()
        total_iou += iou.item()
        num_batches += 1
        
        # æ›´æ–°è¿›åº¦æ¡
        progress_bar.set_postfix({
            'Loss': f'{loss.item():.4f}',
            'IoU': f'{iou.item():.4f}',
            'Fusion Weight': f'{ensemble_model.get_fusion_weight():.3f}'
        })
    
    if num_batches == 0:
        return 0.0, 0.0
    
    avg_loss = total_loss / num_batches
    avg_iou = total_iou / num_batches
    
    return avg_loss, avg_iou

def val_dual_model_epoch(ensemble_model, val_loader, criterion, device, scaler):
    """éªŒè¯åŒæ¨¡å‹é›†æˆ"""
    ensemble_model.eval()
    total_loss = 0
    total_iou = 0
    num_batches = 0
    
    with torch.no_grad():
        for images, masks, sim_feats in tqdm(val_loader, desc='Validation'):
            images = images.to(device)
            masks = masks.to(device)
            sim_feats = sim_feats.to(device)
            
            with autocast(device_type='cuda'):
                outputs = ensemble_model(images, sim_feats)
                loss = criterion(outputs, masks)
            
            # è®¡ç®—IoU
            pred_masks = torch.sigmoid(outputs) > 0.5
            iou = iou_score(pred_masks, masks)
            
            total_loss += loss.item()
            total_iou += iou.item()
            num_batches += 1
    
    avg_loss = total_loss / num_batches
    avg_iou = total_iou / num_batches
    
    return avg_loss, avg_iou

if __name__ == "__main__":
    os.makedirs("models", exist_ok=True)
    main()
