#!/usr/bin/env python3
"""
æµ‹è¯•æ¨¡å‹çš„è”åˆæ¨ç†èƒ½åŠ›
éªŒè¯å›¾åƒå’Œsim_featsçš„è”åˆå­¦ä¹ æ•ˆæœ
"""

import torch
import numpy as np
from train_model import DeepLabWithSimFeature, get_multimodal_patch_dataloaders

def test_joint_learning_capability():
    """æµ‹è¯•è”åˆå­¦ä¹ èƒ½åŠ›"""
    print("=== æµ‹è¯•è”åˆå­¦ä¹ èƒ½åŠ› ===")
    
    model = DeepLabWithSimFeature(in_channels=3, num_classes=1, sim_feat_dim=11)
    
    # åˆ›å»ºæ•°æ®åŠ è½½å™¨
    train_loader, _ = get_multimodal_patch_dataloaders(
        data_root="data/patch_dataset",
        sim_feature_csv="data/sim_features.csv",
        batch_size=4,
        num_workers=0,
        damage_boost=1
    )
    
    # è·å–ä¸€ä¸ªbatchçš„æ•°æ®
    for images, masks, sim_feats in train_loader:
        print(f"è¾“å…¥æ•°æ®:")
        print(f"  å›¾åƒå½¢çŠ¶: {images.shape}")
        print(f"  å›¾åƒèŒƒå›´: [{images.min().item():.4f}, {images.max().item():.4f}]")
        print(f"  sim_featså½¢çŠ¶: {sim_feats.shape}")
        print(f"  sim_featsèŒƒå›´: [{sim_feats.min().item():.4f}, {sim_feats.max().item():.4f}]")
        
        # æµ‹è¯•æ¨¡å‹å‰å‘ä¼ æ’­
        model.train()
        outputs = model(images, sim_feats)
        
        print(f"æ¨¡å‹è¾“å‡º:")
        print(f"  è¾“å‡ºå½¢çŠ¶: {outputs.shape}")
        print(f"  è¾“å‡ºèŒƒå›´: [{outputs.min().item():.4f}, {outputs.max().item():.4f}]")
        
        if torch.isnan(outputs).any():
            print("âŒ è¾“å‡ºåŒ…å«NaN")
            return False
        else:
            print("âœ… è”åˆæ¨ç†æ­£å¸¸")
            break
    
    return True

def test_feature_ablation():
    """æµ‹è¯•ç‰¹å¾æ¶ˆèå®éªŒ"""
    print("\n=== ç‰¹å¾æ¶ˆèå®éªŒ ===")
    
    model = DeepLabWithSimFeature(in_channels=3, num_classes=1, sim_feat_dim=11)
    
    # åˆ›å»ºæµ‹è¯•æ•°æ®
    batch_size = 2
    images = torch.randn(batch_size, 3, 64, 64)
    
    # æµ‹è¯•ä¸åŒsim_featsè¾“å…¥
    test_cases = [
        ("é›¶å‘é‡ï¼ˆæ— sim_featsï¼‰", torch.zeros(batch_size, 11)),
        ("éšæœºsim_feats", torch.randn(batch_size, 11)),
        ("å½’ä¸€åŒ–sim_feats", torch.randn(batch_size, 11) * 0.1),
    ]
    
    results = {}
    
    for case_name, sim_feats in test_cases:
        print(f"\næµ‹è¯•: {case_name}")
        
        model.train()
        outputs = model(images, sim_feats)
        
        # è®¡ç®—è¾“å‡ºç»Ÿè®¡
        output_mean = outputs.mean().item()
        output_std = outputs.std().item()
        output_range = outputs.max().item() - outputs.min().item()
        
        results[case_name] = {
            'mean': output_mean,
            'std': output_std,
            'range': output_range
        }
        
        print(f"  è¾“å‡ºå‡å€¼: {output_mean:.4f}")
        print(f"  è¾“å‡ºæ ‡å‡†å·®: {output_std:.4f}")
        print(f"  è¾“å‡ºèŒƒå›´: {output_range:.4f}")
    
    # åˆ†æç»“æœ
    print(f"\næ¶ˆèå®éªŒç»“æœåˆ†æ:")
    zero_case = results["é›¶å‘é‡ï¼ˆæ— sim_featsï¼‰"]
    random_case = results["éšæœºsim_feats"]
    
    mean_diff = abs(random_case['mean'] - zero_case['mean'])
    std_diff = abs(random_case['std'] - zero_case['std'])
    
    print(f"  æœ‰æ— sim_featsçš„å‡å€¼å·®å¼‚: {mean_diff:.4f}")
    print(f"  æœ‰æ— sim_featsçš„æ ‡å‡†å·®å·®å¼‚: {std_diff:.4f}")
    
    if mean_diff > 0.01 or std_diff > 0.01:
        print("âœ… sim_featså¯¹æ¨¡å‹è¾“å‡ºæœ‰æ˜¾è‘—å½±å“")
        return True
    else:
        print("âŒ sim_featså¯¹æ¨¡å‹è¾“å‡ºå½±å“å¾ˆå°")
        return False

def test_gradient_flow():
    """æµ‹è¯•æ¢¯åº¦æµåŠ¨"""
    print("\n=== æµ‹è¯•æ¢¯åº¦æµåŠ¨ ===")
    
    model = DeepLabWithSimFeature(in_channels=3, num_classes=1, sim_feat_dim=11)
    
    # åˆ›å»ºæ•°æ®
    batch_size = 2
    images = torch.randn(batch_size, 3, 64, 64, requires_grad=True)
    sim_feats = torch.randn(batch_size, 11, requires_grad=True)
    targets = torch.randn(batch_size, 1, 64, 64)
    
    # å‰å‘ä¼ æ’­
    outputs = model(images, sim_feats)
    loss = torch.nn.functional.mse_loss(outputs, targets)
    
    # åå‘ä¼ æ’­
    loss.backward()
    
    # æ£€æŸ¥æ¢¯åº¦
    print(f"å›¾åƒæ¢¯åº¦ç»Ÿè®¡:")
    print(f"  æ¢¯åº¦èŒƒå›´: [{images.grad.min().item():.6f}, {images.grad.max().item():.6f}]")
    print(f"  æ¢¯åº¦å‡å€¼: {images.grad.mean().item():.6f}")
    print(f"  æ¢¯åº¦æ ‡å‡†å·®: {images.grad.std().item():.6f}")
    
    print(f"sim_featsæ¢¯åº¦ç»Ÿè®¡:")
    print(f"  æ¢¯åº¦èŒƒå›´: [{sim_feats.grad.min().item():.6f}, {sim_feats.grad.max().item():.6f}]")
    print(f"  æ¢¯åº¦å‡å€¼: {sim_feats.grad.mean().item():.6f}")
    print(f"  æ¢¯åº¦æ ‡å‡†å·®: {sim_feats.grad.std().item():.6f}")
    
    # æ£€æŸ¥sim_fcå±‚çš„æ¢¯åº¦
    sim_fc_grad_norm = 0
    for param in model.sim_fc.parameters():
        if param.grad is not None:
            sim_fc_grad_norm += param.grad.norm().item() ** 2
    sim_fc_grad_norm = sim_fc_grad_norm ** 0.5
    
    print(f"sim_fcå±‚æ¢¯åº¦èŒƒæ•°: {sim_fc_grad_norm:.6f}")
    
    if sim_fc_grad_norm > 1e-6:
        print("âœ… sim_featsæ¢¯åº¦æ­£å¸¸æµåŠ¨")
        return True
    else:
        print("âŒ sim_featsæ¢¯åº¦æµåŠ¨å¼‚å¸¸")
        return False

def test_real_training_step():
    """æµ‹è¯•çœŸå®è®­ç»ƒæ­¥éª¤"""
    print("\n=== æµ‹è¯•çœŸå®è®­ç»ƒæ­¥éª¤ ===")
    
    model = DeepLabWithSimFeature(in_channels=3, num_classes=1, sim_feat_dim=11)
    optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)
    criterion = torch.nn.BCEWithLogitsLoss()
    
    # è·å–çœŸå®æ•°æ®
    train_loader, _ = get_multimodal_patch_dataloaders(
        data_root="data/patch_dataset",
        sim_feature_csv="data/sim_features.csv",
        batch_size=4,
        num_workers=0,
        damage_boost=1
    )
    
    for images, masks, sim_feats in train_loader:
        print(f"è®­ç»ƒæ•°æ®:")
        print(f"  å›¾åƒå½¢çŠ¶: {images.shape}")
        print(f"  æ©ç å½¢çŠ¶: {masks.shape}")
        print(f"  sim_featså½¢çŠ¶: {sim_feats.shape}")
        
        # å‰å‘ä¼ æ’­
        optimizer.zero_grad()
        outputs = model(images, sim_feats)
        
        # è°ƒæ•´æ©ç ç»´åº¦ä»¥åŒ¹é…è¾“å‡º
        if masks.dim() == 3:
            masks = masks.unsqueeze(1)  # [B, H, W] -> [B, 1, H, W]
        
        loss = criterion(outputs, masks)
        
        print(f"  æŸå¤±å€¼: {loss.item():.6f}")
        
        # åå‘ä¼ æ’­
        loss.backward()
        
        # æ£€æŸ¥æ¢¯åº¦
        total_grad_norm = 0
        for param in model.parameters():
            if param.grad is not None:
                total_grad_norm += param.grad.norm().item() ** 2
        total_grad_norm = total_grad_norm ** 0.5
        
        print(f"  æ€»æ¢¯åº¦èŒƒæ•°: {total_grad_norm:.6f}")
        
        # æ›´æ–°å‚æ•°
        optimizer.step()
        
        print(f"âœ… è®­ç»ƒæ­¥éª¤å®Œæˆ")
        break
    
    return True

if __name__ == "__main__":
    print("å¼€å§‹æµ‹è¯•è”åˆæ¨ç†èƒ½åŠ›...")
    print("=" * 50)
    
    # æ‰§è¡Œå„é¡¹æµ‹è¯•
    test1 = test_joint_learning_capability()
    test2 = test_feature_ablation()
    test3 = test_gradient_flow()
    test4 = test_real_training_step()
    
    print("\n" + "=" * 50)
    print("æµ‹è¯•ç»“æœæ€»ç»“:")
    print(f"  è”åˆå­¦ä¹ èƒ½åŠ›: {'âœ… é€šè¿‡' if test1 else 'âŒ å¤±è´¥'}")
    print(f"  ç‰¹å¾æ¶ˆèå®éªŒ: {'âœ… é€šè¿‡' if test2 else 'âŒ å¤±è´¥'}")
    print(f"  æ¢¯åº¦æµåŠ¨æµ‹è¯•: {'âœ… é€šè¿‡' if test3 else 'âŒ å¤±è´¥'}")
    print(f"  çœŸå®è®­ç»ƒæµ‹è¯•: {'âœ… é€šè¿‡' if test4 else 'âŒ å¤±è´¥'}")
    
    if all([test1, test2, test3, test4]):
        print("\nğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼æ¨¡å‹å…·å¤‡å®Œæ•´çš„è”åˆæ¨ç†èƒ½åŠ›ï¼")
    else:
        print("\nâš ï¸ éƒ¨åˆ†æµ‹è¯•å¤±è´¥ï¼Œéœ€è¦è¿›ä¸€æ­¥è°ƒè¯•") 