#!/usr/bin/env python3
"""
å¢å¼ºç‰ˆCPU-GPUååŒä¼˜åŒ–æµ‹è¯•
"""

import torch
import time
import numpy as np
import threading
import queue

def test_cpu_intensive_task():
    """æµ‹è¯•CPUå¯†é›†å‹ä»»åŠ¡"""
    print("ğŸ§ª æµ‹è¯•CPUå¯†é›†å‹ä»»åŠ¡...")
    
    def cpu_worker(task_queue, result_queue, worker_id):
        while True:
            try:
                task = task_queue.get(timeout=1)
                if task is None:
                    break
                
                # æ‰§è¡ŒCPUå¯†é›†å‹è®¡ç®—
                start_time = time.time()
                
                # æ¨¡æ‹Ÿå¤æ‚çš„CPUè®¡ç®—
                data = task['data']
                result = 0
                for i in range(1000):  # æ¨¡æ‹Ÿ1000æ¬¡è®¡ç®—
                    result += np.sin(i) * np.cos(i) * data
                
                compute_time = time.time() - start_time
                result_queue.put({
                    'worker_id': worker_id,
                    'result': result,
                    'compute_time': compute_time
                })
                
            except queue.Empty:
                continue
            except Exception as e:
                print(f"CPUå·¥ä½œçº¿ç¨‹ {worker_id} é”™è¯¯: {e}")
    
    # åˆ›å»ºä»»åŠ¡é˜Ÿåˆ—
    task_queue = queue.Queue()
    result_queue = queue.Queue()
    
    # å¯åŠ¨CPUå·¥ä½œçº¿ç¨‹
    num_workers = 4
    workers = []
    for i in range(num_workers):
        worker = threading.Thread(target=cpu_worker, args=(task_queue, result_queue, i))
        worker.daemon = True
        worker.start()
        workers.append(worker)
    
    # æäº¤ä»»åŠ¡
    num_tasks = 20
    for i in range(num_tasks):
        task_queue.put({'data': i * 0.1})
    
    # ç­‰å¾…ç»“æœ
    results = []
    start_time = time.time()
    
    for _ in range(num_tasks):
        result = result_queue.get()
        results.append(result)
    
    total_time = time.time() - start_time
    
    # å…³é—­å·¥ä½œçº¿ç¨‹
    for _ in range(num_workers):
        task_queue.put(None)
    
    for worker in workers:
        worker.join()
    
    print(f"âœ… CPUå¯†é›†å‹ä»»åŠ¡å®Œæˆ: {total_time:.3f}ç§’")
    print(f"  ä»»åŠ¡æ•°: {num_tasks}, å·¥ä½œçº¿ç¨‹: {num_workers}")
    print(f"  å¹³å‡ä»»åŠ¡æ—¶é—´: {sum(r['compute_time'] for r in results) / len(results):.3f}ç§’")
    
    return total_time

def test_gpu_training_with_cpu_assist():
    """æµ‹è¯•GPUè®­ç»ƒä¸CPUè¾…åŠ©"""
    print("\nğŸ§ª æµ‹è¯•GPUè®­ç»ƒä¸CPUè¾…åŠ©...")
    
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    
    # åˆ›å»ºæµ‹è¯•æ¨¡å‹
    class TestModel(torch.nn.Module):
        def __init__(self):
            super().__init__()
            self.conv1 = torch.nn.Conv2d(3, 16, 3, padding=1)
            self.relu = torch.nn.ReLU()
            self.conv2 = torch.nn.Conv2d(16, 1, 1)
        
        def forward(self, img, sim_feat):
            x = self.conv1(img)
            x = self.relu(x)
            x = self.conv2(x)
            return x
    
    model = TestModel().to(device)
    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)
    
    # åˆ›å»ºæ•°æ®
    batch_size = 8
    images = torch.randn(batch_size, 3, 64, 64).to(device)
    masks = torch.randint(0, 2, (batch_size, 1, 64, 64)).float().to(device)
    sim_feats = torch.randn(batch_size, 11).to(device)
    
    # æµ‹è¯•æ ‡å‡†è®­ç»ƒ
    print("  æµ‹è¯•æ ‡å‡†è®­ç»ƒ...")
    scaler = torch.amp.GradScaler()
    
    torch.cuda.synchronize()
    start_time = time.time()
    
    for i in range(10):
        optimizer.zero_grad()
        with torch.amp.autocast('cuda'):
            outputs = model(images, sim_feats)
            loss = torch.nn.functional.binary_cross_entropy_with_logits(outputs, masks)
        scaler.scale(loss).backward()
        scaler.step(optimizer)
        scaler.update()
    
    torch.cuda.synchronize()
    std_time = time.time() - start_time
    
    # æµ‹è¯•CPUè¾…åŠ©è®­ç»ƒ
    print("  æµ‹è¯•CPUè¾…åŠ©è®­ç»ƒ...")
    
    def cpu_metrics_worker(outputs, masks):
        """CPUæŒ‡æ ‡è®¡ç®—å·¥ä½œçº¿ç¨‹"""
        with torch.no_grad():
            outputs_cpu = outputs.cpu().detach()
            masks_cpu = masks.cpu().detach()
            
            # è®¡ç®—å„ç§æŒ‡æ ‡
            preds = (torch.sigmoid(outputs_cpu) > 0.5).float()
            
            # IoUè®¡ç®—
            intersection = (preds * masks_cpu).sum()
            union = (preds + masks_cpu).sum() - intersection
            iou = (intersection + 1e-5) / (union + 1e-5)
            
            # Diceè®¡ç®—
            dice = (2. * intersection + 1e-5) / (preds.sum() + masks_cpu.sum() + 1e-5)
            
            # å‡†ç¡®ç‡è®¡ç®—
            accuracy = (preds == masks_cpu).float().mean()
            
            # æ¨¡æ‹Ÿé¢å¤–è®¡ç®—
            time.sleep(0.001)
            
            return {'iou': iou.item(), 'dice': dice.item(), 'accuracy': accuracy.item()}
    
    torch.cuda.synchronize()
    start_time = time.time()
    
    for i in range(10):
        optimizer.zero_grad()
        with torch.amp.autocast('cuda'):
            outputs = model(images, sim_feats)
            loss = torch.nn.functional.binary_cross_entropy_with_logits(outputs, masks)
        
        # å¯åŠ¨CPUæŒ‡æ ‡è®¡ç®—çº¿ç¨‹
        cpu_thread = threading.Thread(target=cpu_metrics_worker, args=(outputs, masks))
        cpu_thread.start()
        
        # GPUåå‘ä¼ æ’­
        scaler.scale(loss).backward()
        scaler.step(optimizer)
        scaler.update()
        
        # ç­‰å¾…CPUè®¡ç®—å®Œæˆ
        cpu_thread.join()
    
    torch.cuda.synchronize()
    hybrid_time = time.time() - start_time
    
    print(f"âœ… GPUè®­ç»ƒæµ‹è¯•å®Œæˆ:")
    print(f"  æ ‡å‡†è®­ç»ƒæ—¶é—´: {std_time:.3f}ç§’")
    print(f"  CPUè¾…åŠ©è®­ç»ƒæ—¶é—´: {hybrid_time:.3f}ç§’")
    print(f"  æ€§èƒ½æå‡: {std_time/hybrid_time:.2f}x")
    
    return std_time, hybrid_time

def test_memory_bandwidth():
    """æµ‹è¯•å†…å­˜å¸¦å®½ä¼˜åŒ–"""
    print("\nğŸ§ª æµ‹è¯•å†…å­˜å¸¦å®½ä¼˜åŒ–...")
    
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    
    # æµ‹è¯•æ•°æ®å¤§å°
    sizes = [1024, 2048, 4096, 8192]
    
    for size in sizes:
        print(f"  æµ‹è¯•æ•°æ®å¤§å°: {size}x{size}")
        
        # åˆ›å»ºæµ‹è¯•æ•°æ®
        data = torch.randn(size, size).to(device)
        
        # æµ‹è¯•GPUè®¡ç®—
        torch.cuda.synchronize()
        start_time = time.time()
        
        for _ in range(10):
            result = torch.mm(data, data)
        
        torch.cuda.synchronize()
        gpu_time = time.time() - start_time
        
        # æµ‹è¯•CPUè®¡ç®—
        data_cpu = data.cpu()
        start_time = time.time()
        
        for _ in range(10):
            result = torch.mm(data_cpu, data_cpu)
        
        cpu_time = time.time() - start_time
        
        print(f"    GPUæ—¶é—´: {gpu_time:.3f}ç§’, CPUæ—¶é—´: {cpu_time:.3f}ç§’")
        print(f"    åŠ é€Ÿæ¯”: {cpu_time/gpu_time:.2f}x")

def main():
    print("="*60)
    print("ğŸš€ å¢å¼ºç‰ˆCPU-GPUååŒä¼˜åŒ–æµ‹è¯•")
    print("="*60)
    
    # æµ‹è¯•CPUå¯†é›†å‹ä»»åŠ¡
    cpu_time = test_cpu_intensive_task()
    
    # æµ‹è¯•GPUè®­ç»ƒä¸CPUè¾…åŠ©
    gpu_std_time, gpu_hybrid_time = test_gpu_training_with_cpu_assist()
    
    # æµ‹è¯•å†…å­˜å¸¦å®½
    test_memory_bandwidth()
    
    # æ€»ç»“
    print(f"\nğŸ“Š æµ‹è¯•æ€»ç»“:")
    print(f"  CPUå¯†é›†å‹ä»»åŠ¡: {cpu_time:.3f}ç§’")
    print(f"  GPUæ ‡å‡†è®­ç»ƒ: {gpu_std_time:.3f}ç§’")
    print(f"  GPU+CPUè¾…åŠ©: {gpu_hybrid_time:.3f}ç§’")
    print(f"  è®­ç»ƒæ€§èƒ½æå‡: {gpu_std_time/gpu_hybrid_time:.2f}x")
    
    print("\nğŸ‰ å¢å¼ºç‰ˆCPU-GPUååŒä¼˜åŒ–æµ‹è¯•å®Œæˆï¼")

if __name__ == "__main__":
    main() 