import torch
import torch.nn as nn
import torch.nn.functional as F
from models.starlite_cnn import (
    FPNLayer,
    EnhancedLandslideDetector,
    create_starlite_model
)

def test_fpn_layer():
    """æµ‹è¯•FPNå±‚æ˜¯å¦æ­£ç¡®å¤„ç†ç©ºé—´ç‰¹å¾"""
    print("=== æµ‹è¯•FPNå±‚ ===")
    
    # åˆ›å»ºFPNå±‚
    fpn_layer = FPNLayer(in_channels=64, out_channels=128)
    
    # åˆ›å»ºæ¨¡æ‹Ÿè¾“å…¥ (batch_size, channels, height, width)
    x = torch.randn(2, 64, 32, 32)
    
    # å‰å‘ä¼ æ’­
    output = fpn_layer(x)
    
    print(f"è¾“å…¥å½¢çŠ¶: {x.shape}")
    print(f"è¾“å‡ºå½¢çŠ¶: {output.shape}")
    print(f"è¾“å‡ºé€šé“æ•°: {output.shape[1]}")
    print(f"ç©ºé—´å°ºå¯¸ä¿æŒä¸å˜: {x.shape[2:] == output.shape[2:]}")
    
    # éªŒè¯è¾“å‡º
    assert output.shape[0] == 2, "æ‰¹æ¬¡å¤§å°åº”è¯¥ä¿æŒä¸å˜"
    assert output.shape[1] == 128, "è¾“å‡ºé€šé“æ•°åº”è¯¥æ˜¯128"
    assert output.shape[2:] == x.shape[2:], "ç©ºé—´å°ºå¯¸åº”è¯¥ä¿æŒä¸å˜"
    print("âœ… FPNå±‚æµ‹è¯•é€šè¿‡")

def test_enhanced_model_fpn():
    """æµ‹è¯•å¢å¼ºæ¨¡å‹çš„FPNåŠŸèƒ½"""
    print("\n=== æµ‹è¯•å¢å¼ºæ¨¡å‹FPN ===")
    
    # åˆ›å»ºæ¨¡å‹
    model = create_starlite_model(
        enhanced=True,
        use_attention=True,
        use_fpn=True
    )
    
    # åˆ›å»ºæ¨¡æ‹Ÿè¾“å…¥
    x = torch.randn(2, 3, 224, 224)
    
    # å‰å‘ä¼ æ’­
    with torch.no_grad():
        output = model(x)
    
    print(f"è¾“å…¥å½¢çŠ¶: {x.shape}")
    print(f"è¾“å‡ºå½¢çŠ¶: {output.shape}")
    print(f"æ¨¡å‹å‚æ•°æ•°é‡: {sum(p.numel() for p in model.parameters())}")
    
    # éªŒè¯è¾“å‡º
    assert output.shape[0] == 2, "æ‰¹æ¬¡å¤§å°åº”è¯¥ä¿æŒä¸å˜"
    assert output.shape[1] == 2, "è¾“å‡ºç±»åˆ«æ•°åº”è¯¥æ˜¯2"
    print("âœ… å¢å¼ºæ¨¡å‹FPNæµ‹è¯•é€šè¿‡")

def test_attention_application():
    """æµ‹è¯•æ³¨æ„åŠ›æœºåˆ¶åº”ç”¨ä½ç½®"""
    print("\n=== æµ‹è¯•æ³¨æ„åŠ›æœºåˆ¶ ===")
    
    # åˆ›å»ºæ¨¡å‹
    model = create_starlite_model(
        enhanced=True,
        use_attention=True,
        use_fpn=False  # å…³é—­FPNä»¥ä¾¿ä¸“æ³¨äºæ³¨æ„åŠ›æµ‹è¯•
    )
    
    # æ£€æŸ¥æ³¨æ„åŠ›å±‚
    if hasattr(model, 'model') and hasattr(model.model, 'attention_layers'):
        attention_layers = model.model.attention_layers
        print(f"æ³¨æ„åŠ›å±‚æ•°é‡: {len(attention_layers)}")
        
        for name, layer in attention_layers.items():
            print(f"æ³¨æ„åŠ›å±‚ {name}: {type(layer).__name__}")
    
    # å‰å‘ä¼ æ’­æµ‹è¯•
    x = torch.randn(1, 3, 224, 224)
    with torch.no_grad():
        output = model(x)
    
    print(f"æ³¨æ„åŠ›æ¨¡å‹è¾“å‡ºå½¢çŠ¶: {output.shape}")
    print("âœ… æ³¨æ„åŠ›æœºåˆ¶æµ‹è¯•é€šè¿‡")

def test_quantization_fusion():
    """æµ‹è¯•é‡åŒ–èåˆåŠŸèƒ½"""
    print("\n=== æµ‹è¯•é‡åŒ–èåˆ ===")
    
    # åˆ›å»ºæ¨¡å‹
    model = create_starlite_model(
        enhanced=True,
        use_attention=True,
        use_fpn=True
    )
    
    # æ£€æŸ¥æ¨¡å‹ç»“æ„
    print("èåˆå‰çš„æ¨¡å‹ç»“æ„:")
    print(f"FPNå±‚æ•°é‡: {len(model.model.fpn) if hasattr(model.model, 'fpn') else 0}")
    print(f"æ³¨æ„åŠ›å±‚æ•°é‡: {len(model.model.attention_layers) if hasattr(model.model, 'attention_layers') else 0}")
    
    # å°è¯•èåˆ
    try:
        model.fuse_model()
        print("âœ… é‡åŒ–èåˆæˆåŠŸ")
    except Exception as e:
        print(f"âŒ é‡åŒ–èåˆå¤±è´¥: {e}")
        return False
    
    # èåˆåæµ‹è¯•å‰å‘ä¼ æ’­
    x = torch.randn(1, 3, 224, 224)
    with torch.no_grad():
        output = model(x)
    
    print(f"èåˆåè¾“å‡ºå½¢çŠ¶: {output.shape}")
    print("âœ… é‡åŒ–èåˆæµ‹è¯•é€šè¿‡")
    return True

def test_model_variants():
    """æµ‹è¯•ä¸åŒæ¨¡å‹å˜ä½“"""
    print("\n=== æµ‹è¯•æ¨¡å‹å˜ä½“ ===")
    
    variants = [
        ("åŸå§‹æ¨¡å‹", create_starlite_model(enhanced=False)),
        ("å¢å¼ºæ¨¡å‹(æ³¨æ„åŠ›+FPN)", create_starlite_model(enhanced=True, use_attention=True, use_fpn=True)),
        ("ä»…æ³¨æ„åŠ›", create_starlite_model(enhanced=True, use_attention=True, use_fpn=False)),
        ("ä»…FPN", create_starlite_model(enhanced=True, use_attention=False, use_fpn=True)),
    ]
    
    x = torch.randn(1, 3, 224, 224)
    
    for name, model in variants:
        with torch.no_grad():
            output = model(x)
        param_count = sum(p.numel() for p in model.parameters())
        print(f"{name}: è¾“å‡ºå½¢çŠ¶ {output.shape}, å‚æ•°æ•°é‡ {param_count:,}")

def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("å¼€å§‹æµ‹è¯•FPNä¿®å¤...")
    
    try:
        test_fpn_layer()
        test_enhanced_model_fpn()
        test_attention_application()
        test_quantization_fusion()
        test_model_variants()
        
        print("\nğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼FPNä¿®å¤æˆåŠŸã€‚")
        
    except Exception as e:
        print(f"\nâŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main() 